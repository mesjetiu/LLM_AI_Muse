@article{abeliukHistoriaEvoluacionInteligencia2021,
  title = {{Historia y evoluaci{\'o}n de la inteligencia artificial}},
  author = {Abeliuk, Andr{\'e}s and Guti{\'e}rrez, Claudio},
  year = {2021},
  month = aug,
  journal = {Revista Bits de Ciencia},
  number = {21},
  pages = {14--21},
  url = {https://revistasdex.uchile.cl/index.php/bits/index},
  copyright = {Derechos de autor 2021 Revista Bits de Ciencia},
  language = {es},
  file = {/home/carlos/Zotero/storage/3XUJEG4L/Abeliuk y Gutiérrez - 2021 - Historia y evoluación de la inteligencia artificia.pdf}
}

@misc{AGIEdgerunnersLLMAgentsPapers2024,
  title = {{{AGI-Edgerunners}}/{{LLM-Agents-Papers}}},
  year = {2024},
  month = jan,
  url = {https://github.com/AGI-Edgerunners/LLM-Agents-Papers},
  urldate = {2024-01-02},
  abstract = {A repo lists papers related to LLM based agent},
  howpublished = {AGI-Edgerunners},
  keywords = {agents,large-language-models,llm-agent,paper-list}
}

@article{alan1950a,
  title = {Computing Machinery and Intelligence},
  author = {Turing, Alan Mathison},
  year = {1950},
  journal = {Mind; a quarterly review of psychology and philosophy},
  volume = {49},
  pages = {433--460}
}

@article{Algorave2023,
  title = {{Algorave}},
  year = {2023},
  month = nov,
  journal = {Wikipedia, la enciclopedia libre},
  url = {https://es.wikipedia.org/w/index.php?title=Algorave&oldid=155173649},
  urldate = {2024-02-08},
  abstract = {Una algorave (de algoritmo y rave) es un evento en el que la gente baila con m{\'u}sica generada a partir de algoritmos, a menudo utilizando t{\'e}cnicas de codificaci{\'o}n en vivo.[1]\hspace{0pt} Alex McLean de Slub y Nick Collins acu{\~n}aron la palabra "algorave" en 2011, y el primer evento con ese nombre se organiz{\'o} en Londres, Reino Unido.[2]\hspace{0pt} Desde entonces se ha convertido en un movimiento, con algoraves que tienen lugar en todo el mundo.[3]\hspace{0pt}[4]\hspace{0pt}},
  copyright = {Creative Commons Attribution-ShareAlike License},
  language = {es},
  annotation = {Page Version ID: 155173649},
  file = {/home/carlos/Zotero/storage/G8JCPP7I/Algorave.html}
}

@misc{almazroueiFalconSeriesOpen2023,
  title = {The {{Falcon Series}} of {{Open Language Models}}},
  author = {Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, M{\'e}rouane and Goffinet, {\'E}tienne and Hesslow, Daniel and Launay, Julien and Malartic, Quentin and Mazzotta, Daniele and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},
  year = {2023},
  month = nov,
  number = {arXiv:2311.16867},
  eprint = {2311.16867},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2311.16867},
  url = {http://arxiv.org/abs/2311.16867},
  abstract = {We introduce the Falcon series: 7B, 40B, and 180B parameters causal decoder-only models trained on a diverse high-quality corpora predominantly assembled from web data. The largest model, Falcon-180B, has been trained on over 3.5 trillion tokens of text--the largest openly documented pretraining run. Falcon-180B significantly outperforms models such as PaLM or Chinchilla, and improves upon concurrently developed models such as LLaMA 2 or Inflection-1. It nears the performance of PaLM-2-Large at a reduced pretraining and inference cost, making it, to our knowledge, one of the three best language models in the world along with GPT-4 and PaLM-2-Large. We report detailed evaluations, as well as a deep dive into the methods and custom tooling employed to pretrain Falcon. Notably, we report on our custom distributed training codebase, allowing us to efficiently pretrain these models on up to 4,096 A100s on cloud AWS infrastructure with limited interconnect. We release a 600B tokens extract of our web dataset, as well as the Falcon-7/40/180B models under a permissive license to foster open-science and accelerate the development of an open ecosystem of large language models.},
  archiveprefix = {arxiv},
  language = {English},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/RS363NL2/Almazrouei et al. - 2023 - The Falcon Series of Open Language Models.pdf;/home/carlos/Zotero/storage/TWRD9XPB/2311.html}
}

@article{arizaTwoPioneeringProjects2011,
  title = {Two {{Pioneering Projects}} from the {{Early History}} of {{Computer-Aided Algorithmic Composition}}},
  author = {Ariza, Christopher},
  year = {2011},
  month = sep,
  journal = {MIT Press},
  publisher = {{MIT Press}},
  issn = {0148-9267},
  url = {https://dspace.mit.edu/handle/1721.1/68626},
  abstract = {Lejaren Hiller's 1970 chapter, "Music Composed  with Computers: An Historical Survey" (Hiller  1970) contains numerous descriptions of projects  in the computer generation of musical structures.  By then, just over ten years after the publication  of Hiller's and Leonard Isaacson's seminal book  Experimental Music (Hiller and Isaacson 1959), a  startling number of experiments in generativemusic  with early computers had been completed. Hiller's  early research, compositions, and publications  established him as a leader in the then-emerging  field of computer-aided algorithmic composition  (CAAC). Some researchers, likely inspired by Hiller  and Isaacson's 1956 Illiac Suite string quartet, even  duplicated their instrumentation: in an amusing  footnote, Hiller writes that "it is curious to note  how many computer pieces have been written for  string quartet . . . particularly since string-quartet  performers seem to be among the least receptive to  newer compositional ideas such as computermusic"  (Hiller 1970, p. 70).},
  copyright = {Article is made available in accordance with the publisher's policy and may be subject to US copyright law. Please refer to the publisher's site for terms of use.},
  language = {en\_US},
  annotation = {Accepted: 2012-01-20T20:32:35Z},
  file = {/home/carlos/Zotero/storage/MN9QB5LS/Ariza - 2011 - Two Pioneering Projects from the Early History of .pdf}
}

@misc{arunbijiRAGVsFinetuning,
  title = {{{RAG}} vs {{Finetuning}} vs {{Prompt Engineering}}: {{A}} Pragmatic View on {{LLM}} Implementation},
  shorttitle = {{{RAG}} vs {{Finetuning}} vs {{Prompt Engineering}}},
  author = {Arun Biji, Mathew},
  url = {https://www.linkedin.com/pulse/rag-vs-finetuning-prompt-engineering-pragmatic-view-llm-mathew},
  urldate = {2024-01-21},
  abstract = {The first half of this article tries to give a brief introduction to challenges in practical implementation of LLMs, whereas the second half focuses on solution approaches for addressing these challenges and how they compare to each other. Please skip to the later section if you are already aware of},
  language = {en},
  file = {/home/carlos/Zotero/storage/56G7M9X9/rag-vs-finetuning-prompt-engineering-pragmatic-view-llm-mathew.html}
}

@misc{ArXivOrgEPrint,
  title = {{{arXiv}}.Org e-{{Print}} Archive},
  url = {https://arxiv.org/},
  urldate = {2024-01-29},
  file = {/home/carlos/Zotero/storage/TKFF6YEY/arxiv.org.html}
}

@misc{Audio,
  title = {Audio},
  journal = {Stability AI},
  url = {https://stability.ai/stable-audio},
  urldate = {2023-12-26},
  language = {en-GB},
  file = {/home/carlos/Zotero/storage/2ILU7GRZ/stable-audio.html}
}

@misc{AWSDeepComposer,
  title = {{AWS DeepComposer}},
  journal = {Amazon Web Services, Inc.},
  url = {https://aws.amazon.com/es/deepcomposer/},
  urldate = {2024-02-05},
  abstract = {AWS DeepComposer brinda a los desarrolladores una forma creativa de empezar a usar el aprendizaje autom{\'a}tico. P{\'o}ngase manos a la obra, literalmente, con un teclado musical y las t{\'e}cnicas de aprendizaje autom{\'a}tico m{\'a}s novedosas, dise{\~n}adas para aumentar sus habilidades de ML.},
  language = {es-ES},
  file = {/home/carlos/Zotero/storage/CL4YPVPN/deepcomposer.html}
}

@article{BardChatbot2024,
  title = {Bard (Chatbot)},
  year = {2024},
  month = jan,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Bard_(chatbot)&oldid=1199893524},
  urldate = {2024-01-28},
  abstract = {Bard is a conversational generative artificial intelligence chatbot developed by Google. Initially based on the LaMDA family of large language models (LLMs), it was later upgraded to PaLM and then to Gemini. Bard was developed as a direct response to the meteoric rise of OpenAI's ChatGPT, and was released in a limited capacity in March 2023 to lukewarm responses before expanding to other countries in May. LaMDA was developed and announced in 2021, but was not released to the public out of an abundance of caution. OpenAI's launch of ChatGPT in November 2022 and its subsequent popularity caught Google executives off-guard and sent them into a panic, prompting a sweeping response in the ensuing months. After mobilizing its workforce, the company launched Bard in February 2023, with the chatbot taking center stage during the 2023 Google I/O keynote in May.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  language = {en},
  annotation = {Page Version ID: 1199893524},
  file = {/home/carlos/Zotero/storage/8EX7LFT7/Bard_(chatbot).html}
}

@misc{BeginnerGuideNeural,
  title = {A {{Beginner}}'s {{Guide}} to {{Neural Networks}} and {{Deep Learning}}},
  journal = {Pathmind},
  url = {http://wiki.pathmind.com/neural-network},
  urldate = {2023-12-21},
  abstract = {An introduction to deep artificial neural networks and deep learning.},
  language = {en},
  file = {/home/carlos/Zotero/storage/QUNDLL5X/a_beginners_guide_to_neural_networks_and_deep_learning___pathmind.pdf;/home/carlos/Zotero/storage/A63S2JGX/neural-network.html}
}

@misc{bhavsarPromptEngineeringArithmetic2023,
  title = {Prompt {{Engineering}} for {{Arithmetic Reasoning Problems}}},
  author = {Bhavsar, Kaustubh},
  year = {2023},
  month = nov,
  journal = {Medium},
  url = {https://towardsdatascience.com/prompt-engineering-for-arithmetic-reasoning-problems-28c8bcd5bf0e},
  urldate = {2024-01-05},
  abstract = {Explore various prompt engineering techniques for arithmetic reasoning problems, best practices, and rapid experimentations for{\dots}},
  language = {en},
  file = {/home/carlos/Zotero/storage/JPHKF3VK/prompt-engineering-for-arithmetic-reasoning-problems-28c8bcd5bf0e.html}
}

@article{borsosAudioLMLanguageModeling2023,
  title = {Audiolm: A Language Modeling Approach to Audio Generation},
  author = {Borsos, Zal{\'a}n and Marinier, Rapha{\"e}l and Vincent, Damien and Kharitonov, Eugene and Pietquin, Olivier and Sharifi, Matt and Roblek, Dominik and Teboul, Olivier and Grangier, David and Tagliasacchi, Marco and others},
  year = {2023},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  publisher = {{IEEE}},
  file = {/home/carlos/Zotero/storage/QXGW8ZAJ/Borsos et al. - 2023 - AudioLM a Language Modeling Approach to Audio Generation.pdf}
}

@book{boulangerCsoundBookPerspectives2000,
  title = {The {{Csound Book}}. {{Perspectives}} in {{Software Synthesis}}, {{Sound Design}}, {{Signal Processing}}, and {{Programming}}},
  editor = {Boulanger, Richard},
  year = {2000},
  publisher = {{The MIT Press}},
  address = {{Massachusetts}}
}

@article{bowmanEightThingsKnow2023,
  title = {Eight Things to Know about Large Language Models},
  author = {Bowman, Samuel R},
  year = {2023},
  journal = {arXiv preprint arXiv:2304.00612},
  eprint = {2304.00612},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/RDHLL7B3/Bowman - 2023 - Eight Things to Know about Large Language Models.pdf}
}

@article{bretanUnitSelectionMethodology2016,
  title = {A Unit Selection Methodology for Music Generation Using Deep Neural Networks},
  author = {Bretan, Mason and Weinberg, Gil and Heck, Larry},
  year = {2016},
  journal = {arXiv preprint arXiv:1612.03789},
  eprint = {1612.03789},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/EHW5C3CU/Bretan et al. - 2016 - A Unit Selection Methodology for Music Generation .pdf}
}

@article{brownLanguageModelsAre2020,
  title = {Language Models Are Few-Shot Learners},
  author = {Mann, Ben and Ryder, N and Subbiah, M and Kaplan, J and Dhariwal, P and Neelakantan, A and Shyam, P and Sastry, G and Askell, A and Agarwal, S and others},
  year = {2020},
  journal = {arXiv preprint arXiv:2005.14165},
  eprint = {2005.14165},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/4ZY5M7ZW/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf}
}

@article{butlinConsciousnessArtificialIntelligence2023,
  title = {Consciousness in Artificial Intelligence: Insights from the Science of Consciousness},
  author = {Butlin, Patrick and Long, Robert and Elmoznino, Eric and Bengio, Yoshua and Birch, Jonathan and Constant, Axel and Deane, George and Fleming, Stephen M and Frith, Chris and Ji, Xu and others},
  year = {2023},
  journal = {arXiv preprint arXiv:2308.08708},
  eprint = {2308.08708},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/3YM2L8NV/Butlin et al. - 2023 - Consciousness in Artificial Intelligence Insights.pdf}
}

@misc{caiStructuredPruningAll2022,
  title = {Structured {{Pruning}} Is {{All You Need}} for {{Pruning CNNs}} at {{Initialization}}},
  author = {Cai, Yaohui and Hua, Weizhe and Chen, Hongzheng and Suh, G. Edward and De Sa, Christopher and Zhang, Zhiru},
  year = {2022},
  month = may,
  number = {arXiv:2203.02549},
  eprint = {2203.02549},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.02549},
  url = {http://arxiv.org/abs/2203.02549},
  abstract = {Pruning is a popular technique for reducing the model size and computational cost of convolutional neural networks (CNNs). However, a slow retraining or fine-tuning procedure is often required to recover the accuracy loss caused by pruning. Recently, a new research direction on weight pruning, pruning-at-initialization (PAI), is proposed to directly prune CNNs before training so that fine-tuning or retraining can be avoided. While PAI has shown promising results in reducing the model size, existing approaches rely on fine-grained weight pruning which requires unstructured sparse matrix computation, making it difficult to achieve real speedup in practice unless the sparsity is very high. This work is the first to show that fine-grained weight pruning is in fact not necessary for PAI. Instead, the layerwise compression ratio is the main critical factor to determine the accuracy of a CNN model pruned at initialization. Based on this key observation, we propose PreCropping, a structured hardware-efficient model compression scheme. PreCropping directly compresses the model at the channel level following the layerwise compression ratio. Compared to weight pruning, the proposed scheme is regular and dense in both storage and computation without sacrificing accuracy. In addition, since PreCropping compresses CNNs at initialization, the computational and memory costs of CNNs are reduced for both training and inference on commodity hardware. We empirically demonstrate our approaches on several modern CNN architectures, including ResNet, ShuffleNet, and MobileNet for both CIFAR-10 and ImageNet.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/carlos/Zotero/storage/5ICLM59D/Cai et al. - 2022 - Structured Pruning is All You Need for Pruning CNNs at Initialization.pdf;/home/carlos/Zotero/storage/3PNZ4KJR/2203.html}
}

@misc{calvoRedNeuronalRecurrente2018,
  title = {{Red Neuronal Recurrente - RNN}},
  author = {Calvo, Diego},
  year = {2018},
  month = dec,
  journal = {Diego Calvo},
  url = {https://www.diegocalvo.es/red-neuronal-recurrente/},
  urldate = {2024-01-22},
  abstract = {Definici{\'o}n de Red Neuronal Recurrente Una red neuronal recurrente no tiene una estructura de capas definida, sino que permiten conexiones arbitrarias entre las neuronas, incluso pudiendo crear ciclos, con esto se consigue crear la temporalidad, permitiendo que la red tenga memoria. Las redes neuronales recurrentes son muy potentes para todo lo que tiene que ver [{\dots}]},
  language = {es-ES},
  file = {/home/carlos/Zotero/storage/HMJ2MIVB/red-neuronal-recurrente.html}
}

@article{campbellDeepBlue2002,
  title = {Deep {{Blue}}},
  author = {Campbell, Murray and Hoane, A. Joseph and Hsu, Feng-hsiung},
  year = {2002},
  month = jan,
  journal = {Artificial Intelligence},
  volume = {134},
  number = {1},
  pages = {57--83},
  issn = {0004-3702},
  doi = {10.1016/S0004-3702(01)00129-1},
  url = {https://www.sciencedirect.com/science/article/pii/S0004370201001291},
  abstract = {Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including: {$\bullet$}a single-chip chess search engine,{$\bullet$}a massively parallel system with multiple levels of parallelism,{$\bullet$}a strong emphasis on search extensions,{$\bullet$}a complex evaluation function, and{$\bullet$}effective use of a Grandmaster game database. This paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue.},
  keywords = {Computer chess,Evaluation function,Game tree search,Parallel search,Search extensions,Selective search},
  file = {/home/carlos/Zotero/storage/IAV98XWL/Campbell et al. - 2002 - Deep Blue.pdf;/home/carlos/Zotero/storage/ARPI4JK5/S0004370201001291.html}
}

@inproceedings{caoAllYouNeed2020,
  title = {All You Need Is a Second Look: {{Towards}} Tighter Arbitrary Shape Text Detection},
  booktitle = {{{ICASSP}} 2020-2020 {{IEEE}} International Conference on Acoustics, Speech and Signal Processing ({{ICASSP}})},
  author = {Cao, Meng and Zou, Yuexian},
  year = {2020},
  pages = {2228--2232},
  publisher = {{IEEE}},
  file = {/home/carlos/Zotero/storage/7FSIZ6HW/Cao y Zou - 2020 - All you need is a second look Towards Tighter Arbitrary shape text detection.pdf}
}

@misc{cardereraSimpleStepsAre2023,
  title = {Simple Steps Are All You Need: {{Frank-Wolfe}} and Generalized Self-Concordant Functions},
  shorttitle = {Simple Steps Are All You Need},
  author = {Carderera, Alejandro and Besan{\c c}on, Mathieu and Pokutta, Sebastian},
  year = {2023},
  month = nov,
  number = {arXiv:2105.13913},
  eprint = {2105.13913},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2105.13913},
  url = {http://arxiv.org/abs/2105.13913},
  abstract = {Generalized self-concordance is a key property present in the objective function of many important learning problems. We establish the convergence rate of a simple Frank-Wolfe variant that uses the open-loop step size strategy \${\textbackslash}gamma\_t = 2/(t+2)\$, obtaining a \${\textbackslash}mathcal\{O\}(1/t)\$ convergence rate for this class of functions in terms of primal gap and Frank-Wolfe gap, where \$t\$ is the iteration count. This avoids the use of second-order information or the need to estimate local smoothness parameters of previous work. We also show improved convergence rates for various common cases, e.g., when the feasible region under consideration is uniformly convex or polyhedral.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/carlos/Zotero/storage/K2E4LKXU/Carderera et al. - 2023 - Simple steps are all you need Frank-Wolfe and generalized self-concordant functions.pdf;/home/carlos/Zotero/storage/NFBVR4LZ/2105.html}
}

@misc{ChallengesAssociatedBuilding,
  title = {{The Challenges Associated With Building Products Using Large Language Models (LLMs).}},
  url = {https://www.linkedin.com/pulse/challenges-associated-building-products-using-large-language-das},
  urldate = {2023-10-31},
  abstract = {Hello there. I hope you all are doing well.},
  language = {es},
  file = {/home/carlos/Zotero/storage/A78XI6JS/challenges-associated-building-products-using-large-language-das.html}
}

@inproceedings{chamandFinetuneYourClassifier2022,
  title = {Fine-Tune Your Classifier: {{Finding}} Correlations with Temperature},
  booktitle = {2022 {{IEEE}} International Conference on Image Processing ({{ICIP}})},
  author = {Chamand, Benjamin and {Risser-Maroix}, Olivier and Kurtz, Camille and Joly, Philippe and Lom{\'e}nie, Nicolas},
  year = {2022},
  pages = {2766--2770},
  publisher = {{IEEE}},
  file = {/home/carlos/Zotero/storage/EVJ4JQ22/Chamand et al. - 2022 - Fine-tune your Classifier Finding Correlations Wi.pdf}
}

@misc{chenTeachingLargeLanguage2023,
  title = {Teaching {{Large Language Models}} to {{Self-Debug}}},
  author = {Chen, Xinyun and Lin, Maxwell and Sch{\"a}rli, Nathanael and Zhou, Denny},
  year = {2023},
  month = oct,
  number = {arXiv:2304.05128},
  eprint = {2304.05128},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.05128},
  url = {http://arxiv.org/abs/2304.05128},
  abstract = {Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3\%, and improves the prediction accuracy on problems of the hardest level by 9\%. On TransCoder and MBPP where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12\%. Meanwhile, by leveraging feedback messages and reusing failed predictions, Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/7APGY9B9/Chen et al. - 2023 - Teaching Large Language Models to Self-Debug.pdf;/home/carlos/Zotero/storage/B54ZXYI4/2304.html}
}

@article{ConjuntoMandelbrot2024,
  title = {{Conjunto de Mandelbrot}},
  year = {2024},
  month = jan,
  journal = {Wikipedia, la enciclopedia libre},
  url = {https://es.wikipedia.org/w/index.php?title=Conjunto_de_Mandelbrot&oldid=157536731},
  urldate = {2024-01-28},
  abstract = {El conjunto de Mandelbrot es el m{\'a}s estudiado de los fractales. Se conoce as{\'i} en honor al matem{\'a}tico Beno{\^i}t Mandelbrot (1924-2010), que investig{\'o} sobre {\'e}l en los a{\~n}os setenta. Este conjunto se define en el plano complejo fijando un n{\'u}mero complejo c cualquiera. A partir de c, se construye una sucesi{\'o}n por recursi{\'o}n:                                                \{                                                                                     z                                            0                                                           =                   0                   {$\in$}                                        C                                                                                             (t{\'e}rmino inicial)                                                                                                                                              z                                            n                       +                       1                                                           =                                        z                                            n                                                                 2                                                           +                   c                                                                          (sucesi{\'o}n recursiva)                                                                                                                  \{{\textbackslash}displaystyle \{{\textbackslash}begin\{cases\}z\_\{0\}=0{\textbackslash}in {\textbackslash}mathbb \{C\} \&\{{\textbackslash}text\{(t{\'e}rmino inicial)\}\}{\textbackslash}qquad {\textbackslash}{\textbackslash}z\_\{n+1\}=z\_\{n\}\^\{2\}+c\&\{{\textbackslash}text\{(sucesi{\'o}n recursiva)\}\}{\textbackslash}end\{cases\}\}\}    Si esta sucesi{\'o}n queda acotada, entonces se dice que c pertenece al conjunto de Mandelbrot, y si no, queda excluido del mismo. Por ejemplo, si c = 1 obtenemos la sucesi{\'o}n 0, 1, 2, 5, 26, {\dots}, que diverge. Como no est{\'a} acotada, 1 no es un elemento del conjunto de Mandelbrot. En cambio, si c = {\textendash}1 obtenemos la sucesi{\'o}n 0, {\textendash}1, 0, {\textendash}1, {\dots}, que s{\'i} es acotada y, por tanto, {\textendash}1 s{\'i} pertenece al conjunto de Mandelbrot. A menudo se representa el conjunto mediante el algoritmo de tiempo de escape. En ese caso, los colores de los puntos que no pertenecen al conjunto indican la velocidad con la que diverge (tiende al infinito, en m{\'o}dulo) la sucesi{\'o}n correspondiente a dicho punto. En la imagen de ejemplo, observamos que el rojo oscuro indica que al cabo de pocos c{\'a}lculos se sabe que el punto no est{\'a} en el conjunto mientras que el blanco informa de que se ha tardado mucho m{\'a}s en comprobarlo. Como no se puede calcular un sinf{\'i}n de valores, es preciso poner un l{\'i}mite y decidir que si los p primeros t{\'e}rminos de la sucesi{\'o}n est{\'a}n acotados entonces se considera que el punto pertenece al conjunto. Al aumentar el valor de p se mejora la precisi{\'o}n de la imagen. Por otra parte, se sabe que los puntos cuya distancia al origen es superior a 2, es decir,                                   x                        2                             +                    y                        2                             {$>$}         4                 \{{\textbackslash}displaystyle x\^\{2\}+y\^\{2\}{$>$}4\}    no pertenecen al conjunto. Por lo tanto basta encontrar un solo t{\'e}rmino de la sucesi{\'o}n que verifique                                    {\textbar}                             z                        n                                        {\textbar}                  {$>$}         2                 \{{\textbackslash}displaystyle {\textbar}z\_\{n\}{\textbar}{$>$}2\}    para estar seguro de que c no est{\'a} en el conjunto.\vphantom\}},
  copyright = {Creative Commons Attribution-ShareAlike License},
  language = {es},
  annotation = {Page Version ID: 157536731},
  file = {/home/carlos/Zotero/storage/QDFYZIQZ/Conjunto_de_Mandelbrot.html}
}

@inproceedings{crookConversationalSemanticSearch2018,
  title = {Conversational {{Semantic Search}}: {{Looking BeyondWeb Search}}, {{Q}}\&{{A}} and {{Dialog Systems}}},
  shorttitle = {Conversational {{Semantic Search}}},
  booktitle = {The 11th {{ACM International Conference}} on {{Web Search}} and {{Data Minining}} ({{WSDM}} 2018)},
  author = {Crook, Paul A. and Marin, Alex and Agarwal, Vipul and Anderson, Samantha and Jang, Ohyoung and Lanewala, Aliasgar and Tangirala, Karthik and Zitouni, Imed},
  year = {2018},
  month = feb,
  url = {https://www.microsoft.com/en-us/research/publication/conversational-semantic-search-looking-beyondweb-search-qa-and-dialog-systems/},
  abstract = {User expectations of web search are changing. They are expecting search engines to answer questions, to be more conversational, and to offer means to complete tasks on their behalf. At the same time, to increase the breadth of tasks that personal digital assistants (PDAs), such as Microsoft's Cortana or Amazon's Alexa, are capable of, PDAs [{\dots}]},
  language = {en-US},
  file = {/home/carlos/Zotero/storage/XS5AUBND/Crook et al. - 2018 - Conversational Semantic Search Looking BeyondWeb .pdf}
}

@misc{DeepLearningDL,
  title = {Deep {{Learning}} ({{DL}})},
  journal = {Questions and Answers \hspace{0pt}in MRI},
  url = {http://mriquestions.com/what-is-a-neural-network.html},
  urldate = {2023-12-21},
  abstract = {Deep Learning (DL)},
  language = {en},
  file = {/home/carlos/Zotero/storage/H68M27JI/what-is-a-neural-network.html}
}

@article{departmentofcomputersciencesrminstituteofscienceandtechnologychennaiindia.MusenetMusicGeneration2020a,
  title = {Musenet : {{Music Generation}} Using {{Abstractive}} and {{Generative Methods}}},
  shorttitle = {Musenet},
  author = {{Department of Computer Science, SRM Institute of Science and Technology, Chennai, India.} and Pal*, Abhilash and Saha, Sourav and {Department of Computer Science, SRM Institute of Science and Technology, Chennai, India.} and {Anita} and {Department of Computer Science, SRM Institute of Science and Technology, Chennai, India.}},
  year = {2020},
  month = apr,
  journal = {International Journal of Innovative Technology and Exploring Engineering},
  volume = {9},
  number = {6},
  pages = {784--788},
  issn = {22783075},
  doi = {10.35940/ijitee.F3580.049620},
  url = {https://www.ijitee.org/portfolio-item/F3580049620/},
  abstract = {Humans have been entertained by music for millennia. For ages it has been treated as an art form which requires a lot of imagination, creativity and accumulation of feelings and emotions. Recent trends in the field of Artificial Intelligence have been getting traction and Researchers have been developing and generating rudimentary forms of music through the use of AI. Our goal is to generate novel music, which will be non-repetitive and enjoyable. We aim to utilize a couple of Machine Learning models for the same. Given a seed bar of music, our first Discriminatory network consisting of Support Vector Machines and Neural Nets will choose a note/chord to direct the next bar. Based on this chord or note another network, a Generative Net consisting of Generative Pretrained Transformers(GPT2) and LSTMs will generate the entire bar of music. Our two fold method is novel and our aim is to make the generation method as similar to music composition in reality as possible. This in turn results in better concordant music. Machine generated music will be copyright free and can be generated conditioned on a few parameters for a given use.The paper presents several use cases and while the utilization will be for a niche audience, if a easy to use application can be built, almost anyone will be able to use deep learning to generate concordant music based on their needs.},
  note = {[TLDR] The goal is to generate novel music, which will be non-repetitive and enjoyable, and almost anyone will be able to use deep learning to generate concordant music based on their needs.},
  file = {/home/carlos/Zotero/storage/MUPRNIQZ/Department of Computer Science, SRM Institute of Science and Technology, Chennai, India. et al. - 2020 - Musenet  Music Generation using Abstractive and G.pdf}
}

@article{dhariwalJukeboxGenerativeModel2020,
  title = {Jukebox: {{A}} Generative Model for Music},
  author = {Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
  year = {2020},
  journal = {arXiv preprint arXiv:2005.00341},
  eprint = {2005.00341},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/B294RLLC/Dhariwal et al. - 2020 - Jukebox A Generative Model for Music.pdf}
}

@article{dhuliawalaChainofVerificationReducesHallucination2023,
  title = {Chain-of-Verification Reduces Hallucination in Large Language Models},
  author = {Dhuliawala, Shehzaad and Komeili, Mojtaba and Xu, Jing and Raileanu, Roberta and Li, Xian and Celikyilmaz, Asli and Weston, Jason},
  year = {2023},
  journal = {arXiv preprint arXiv:2309.11495},
  eprint = {2309.11495},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/JLP8R4IU/Dhuliawala et al. - 2023 - Chain-of-Verification Reduces Hallucination in Lar.pdf}
}

@article{douglasLargeLanguageModels2023,
  title = {Large Language Models},
  author = {Douglas, Michael R},
  year = {2023},
  journal = {arXiv preprint arXiv:2307.05782},
  eprint = {2307.05782},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/MTB9SGS5/Douglas - 2023 - Large Language Models.pdf}
}

@article{duAddressingSyntaxBasedSemantic2022,
  title = {Addressing {{Syntax-Based Semantic Complementation}}: {{Incorporating Entity}} and {{Soft Dependency Constraints}} into {{Metonymy Resolution}}},
  shorttitle = {Addressing {{Syntax-Based Semantic Complementation}}},
  author = {Du, Siyuan and Wang, Hao},
  year = {2022},
  month = mar,
  journal = {Future Internet},
  volume = {14},
  number = {3},
  pages = {85},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1999-5903},
  doi = {10.3390/fi14030085},
  url = {https://www.mdpi.com/1999-5903/14/3/85},
  abstract = {State-of-the-art methods for metonymy resolution (MR) consider the sentential context by modeling the entire sentence. However, entity representation, or syntactic structure that are informative may be beneficial for identifying metonymy. Other approaches only using deep neural network fail to capture such information. To leverage both entity and syntax constraints, this paper proposes a robust model EBAGCN for metonymy resolution. First, this work extracts syntactic dependency relations under the guidance of syntactic knowledge. Then the work constructs a neural network to incorporate both entity representation and syntactic structure into better resolution representations. In this way, the proposed model alleviates the impact of noisy information from entire sentences and breaks the limit of performance on the complicated texts. Experiments on the SemEval and ReLocaR dataset show that the proposed model significantly outperforms the state-of-the-art method BERT by more than 4\%. Ablation tests demonstrate that leveraging these two types of constraints benefits fine pre-trained language models in the MR task.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  language = {en},
  keywords = {dependency integration,entity representation,metonymy resolution},
  file = {/home/carlos/Zotero/storage/AP3AK6JY/Du y Wang - 2022 - Addressing Syntax-Based Semantic Complementation .pdf}
}

@article{erkerImaginationAllYou2023,
  title = {Imagination Is All You Need! {{Curved}} Contrastive Learning for Abstract Sequence Modeling Utilized on Long Short-Term Dialogue Planning},
  author = {Erker, Justus-Jonas and Schaffer, Stefan and Spanakis, Gerasimos},
  year = {2022},
  journal = {arXiv preprint arXiv:2211.07591},
  eprint = {2211.07591},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/8M275USC/Erker et al. - 2023 - Imagination is All You Need! Curved Contrastive Learning for Abstract Sequence Modeling Utilized on .pdf}
}

@article{eysenbachDiversityAllYou2018,
  title = {Diversity Is All You Need: {{Learning}} Skills without a Reward Function},
  author = {Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
  year = {2018},
  journal = {arXiv preprint arXiv:1802.06070},
  eprint = {1802.06070},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/FSHXYMD5/Eysenbach et al. - 2018 - Diversity is All You Need Learning Skills without a Reward Function.pdf}
}

@misc{FindWaysDeal,
  title = {{Find ways to deal with the scarcity of labeled data}},
  url = {https://www.linkedin.com/pulse/find-ways-deal-scarcity-labeled-data-amit-asawa},
  urldate = {2023-12-21},
  abstract = {Compared to other available choices such as unsupervised and deep learning algorithms: the whole learning process of supervised machine learning (ML) based algorithms is much simpler; the training \& deployment costs are considerably low too; and most importantly supervised models work perfectly for},
  language = {es},
  file = {/home/carlos/Zotero/storage/K3EN3SDY/find-ways-deal-scarcity-labeled-data-amit-asawa.html}
}

@misc{frackiewiczEvolucionIAComposicion2023,
  title = {{La evoluci{\'o}n de la IA en la composici{\'o}n musical}},
  author = {Fr{\k a}ckiewicz, Marcin},
  year = {2023},
  month = jul,
  journal = {TS2 SPACE},
  url = {https://ts2.space/es/la-evolucion-de-la-ia-en-la-composicion-musical/},
  urldate = {2023-11-03},
  abstract = {La evoluci{\'o}n de la IA en la composici{\'o}n musical TS2 SPACE},
  language = {es-ES},
  file = {/home/carlos/Zotero/storage/Z76TKKBM/la-evolucion-de-la-ia-en-la-composicion-musical.html}
}

@article{funkMusicalSuiteComposed2018a,
  title = {A {{Musical Suite Composed}} by an {{Electronic Brain}}: {{Reexamining}} the {{Illiac Suite}} and the {{Legacy}} of {{Lejaren A}}. {{Hiller Jr}}.},
  shorttitle = {A {{Musical Suite Composed}} by an {{Electronic Brain}}},
  author = {Funk, Tiffany},
  year = {2018},
  journal = {Leonardo Music Journal},
  volume = {28},
  pages = {19--24},
  publisher = {{MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info {\dots}}},
  url = {https://direct.mit.edu/lmj/article-abstract/doi/10.1162/lmj_a_01037/69560},
  file = {/home/carlos/Zotero/storage/JSP23EKX/Funk - 2018 - A Musical Suite Composed by an Electronic Brain R.pdf}
}

@inproceedings{gallegoFocusAllYou2019,
  title = {Focus {{Is All You Need}}: {{Loss Functions For Event-based Vision}}},
  shorttitle = {Focus {{Is All You Need}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Gallego, Guillermo and Gehrig, Mathias and Scaramuzza, Davide},
  year = {2019},
  month = jun,
  eprint = {1904.07235},
  primaryclass = {cs},
  pages = {12272--12281},
  doi = {10.1109/CVPR.2019.01256},
  url = {http://arxiv.org/abs/1904.07235},
  abstract = {Event cameras are novel vision sensors that output pixel-level brightness changes ("events") instead of traditional video frames. These asynchronous sensors offer several advantages over traditional cameras, such as, high temporal resolution, very high dynamic range, and no motion blur. To unlock the potential of such sensors, motion compensation methods have been recently proposed. We present a collection and taxonomy of twenty two objective functions to analyze event alignment in motion compensation approaches (Fig. 1). We call them Focus Loss Functions since they have strong connections with functions used in traditional shape-from-focus applications. The proposed loss functions allow bringing mature computer vision tools to the realm of event cameras. We compare the accuracy and runtime performance of all loss functions on a publicly available dataset, and conclude that the variance, the gradient and the Laplacian magnitudes are among the best loss functions. The applicability of the loss functions is shown on multiple tasks: rotational motion, depth and optical flow estimation. The proposed focus loss functions allow to unlock the outstanding properties of event cameras.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: 29 pages, 19 figures, 4 tables},
  file = {/home/carlos/Zotero/storage/93WVUL84/Gallego et al. - 2019 - Focus Is All You Need Loss Functions For Event-based Vision.pdf;/home/carlos/Zotero/storage/C96MBEX6/1904.html}
}

@inproceedings{ganguliPredictabilitySurpriseLarge2022,
  title = {Predictability and {{Surprise}} in {{Large Generative Models}}},
  booktitle = {2022 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Ganguli, Deep and Hernandez, Danny and Lovitt, Liane and DasSarma, Nova and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Kernion, Jackson and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Elhage, Nelson and Showk, Sheer El and Fort, Stanislav and {Hatfield-Dodds}, Zac and Johnston, Scott and Kravec, Shauna and Nanda, Neel and Ndousse, Kamal and Olsson, Catherine and Amodei, Daniela and Amodei, Dario and Brown, Tom and Kaplan, Jared and McCandlish, Sam and Olah, Chris and Clark, Jack},
  year = {2022},
  month = jun,
  eprint = {2202.07785},
  primaryclass = {cs},
  pages = {1747--1764},
  doi = {10.1145/3531146.3533229},
  url = {http://arxiv.org/abs/2202.07785},
  abstract = {Large-scale pre-training has recently emerged as a technique for creating capable, general purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have an unusual combination of predictable loss on a broad training distribution (as embodied in their "scaling laws"), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, and academics who want to analyze, critique, and potentially develop large generative models.},
  archiveprefix = {arxiv},
  language = {en},
  keywords = {Computer Science - Computers and Society},
  note = {Comment: Updated to reflect the version submitted (and accepted) to ACM FAccT '22. This update incorporates feedback from peer-review and fixes minor typos. See open access FAccT conference version at: https://dl.acm.org/doi/abs/10.1145/3531146.3533229},
  file = {/home/carlos/Zotero/storage/93WQ2BRW/Ganguli et al. - 2022 - Predictability and Surprise in Large Generative Mo.pdf}
}

@misc{GenerationLLMs,
  title = {Generation with {{LLMs}}},
  url = {https://huggingface.co/docs/transformers/main/en/llm_tutorial},
  urldate = {2023-10-31},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {/home/carlos/Zotero/storage/Q83ZWSQC/llm_tutorial.html}
}

@book{gollapudi2016practical,
  title = {Practical Machine Learning},
  author = {Gollapudi, S.},
  year = {2016},
  publisher = {{Packt Publishing}},
  url = {https://books.google.es/books?id=WmsdDAAAQBAJ},
  isbn = {978-1-78439-401-1}
}

@misc{gonzaloAsomandonosVentanaContextual2023,
  type = {{Billet}},
  title = {{Asom{\'a}ndonos a la ventana contextual de la Inteligencia Artificial: dec{\'a}logo de ayuda para la identificaci{\'o}n del uso de ChatGPT en textos acad{\'e}micos}},
  shorttitle = {{Asom{\'a}ndonos a la ventana contextual de la Inteligencia Artificial}},
  author = {Gonzalo, Jover and Carabantes, David and Gonz{\'a}lez Geraldo, Jos{\'e} L.},
  year = {2023},
  month = jun,
  journal = {Aula Magna 2.0},
  url = {https://cuedespyd.hypotheses.org/13299},
  abstract = {Por Gonzalo Jover*, David Carabantes* y Jos{\'e} L. Gonz{\'a}lez Geraldo** *Universidad Complutense de Madrid **Universidad de Castilla La Mancha Palabras clave:~ChatGPT, OpenAI, Inteligencia Artificial ~ Que la Inteligencia Artificial (IA) ha contaminado cada rinc{\'o}n de nuestra sociedad a un ritmo tan espeluznante como atractivo y peligroso es m{\'a}s que evidente a {\dots} Continuar leyendo "Asom{\'a}ndonos a la ventana contextual de la Inteligencia Artificial: dec{\'a}logo de ayuda para la identificaci{\'o}n del uso de ChatGPT en textos acad{\'e}micos"},
  language = {es},
  file = {/home/carlos/Zotero/storage/FZ5IHE5I/13299.html}
}

@book{Goodfellow-et-al-2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {{MIT Press}},
  note = {http://www.deeplearningbook.org}
}

@misc{GPT3RiseFoundation,
  title = {{GPT-3 and the rise of foundation models}},
  url = {https://www.linkedin.com/pulse/gpt-3-rise-foundation-models-joseph-boland},
  urldate = {2023-12-10},
  abstract = {GPT-3 (Generative Pre-Trained Transformer 3) is a large language model with 175 billion parameters, trained using the Common Crawl internet dataset, Wikipedia, and several large digital document collections. Its transformer-based algorithm has demonstrated superior performance in text generation, co},
  language = {es},
  file = {/home/carlos/Zotero/storage/9E6YS6FB/gpt-3-rise-foundation-models-joseph-boland.html}
}

@misc{grahamOneModelAll2022,
  title = {One {{Model}} Is {{All You Need}}: {{Multi-Task Learning Enables Simultaneous Histology Image Segmentation}} and {{Classification}}},
  shorttitle = {One {{Model}} Is {{All You Need}}},
  author = {Graham, Simon and Vu, Quoc Dang and Jahanifar, Mostafa and Raza, Shan E. Ahmed and Minhas, Fayyaz and Snead, David and Rajpoot, Nasir},
  year = {2022},
  month = nov,
  number = {arXiv:2203.00077},
  eprint = {2203.00077},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.00077},
  url = {http://arxiv.org/abs/2203.00077},
  abstract = {The recent surge in performance for image analysis of digitised pathology slides can largely be attributed to the advances in deep learning. Deep models can be used to initially localise various structures in the tissue and hence facilitate the extraction of interpretable features for biomarker discovery. However, these models are typically trained for a single task and therefore scale poorly as we wish to adapt the model for an increasing number of different tasks. Also, supervised deep learning models are very data hungry and therefore rely on large amounts of training data to perform well. In this paper, we present a multi-task learning approach for segmentation and classification of nuclei, glands, lumina and different tissue regions that leverages data from multiple independent data sources. While ensuring that our tasks are aligned by the same tissue type and resolution, we enable meaningful simultaneous prediction with a single network. As a result of feature sharing, we also show that the learned representation can be used to improve the performance of additional tasks via transfer learning, including nuclear classification and signet ring cell detection. As part of this work, we train our developed Cerberus model on a huge amount of data, consisting of over 600K objects for segmentation and 440K patches for classification. We use our approach to process 599 colorectal whole-slide images from TCGA, where we localise 377 million, 900K and 2.1 million nuclei, glands and lumina, respectively and make the results available to the community for downstream analysis.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/home/carlos/Zotero/storage/EFDRE7RU/Graham et al. - 2022 - One Model is All You Need Multi-Task Learning Enables Simultaneous Histology Image Segmentation and.pdf;/home/carlos/Zotero/storage/6E4DBE9Y/2203.html}
}

@mastersthesis{guerraparraMesjetiuTFM_Arte_Sonoro_MEMORIA2020,
  title = {Synthi {{GME Modular Emulator}}. {{Una}} Propuesta de Emulaci{\'o}n Digital Del Sintetizador {{EMS Synthi}} 100 Del {{Gabinete}} de {{M{\'u}sica Electroac{\'u}stica}} de {{Cuenca}}},
  author = {Guerra Parra, Carlos Arturo},
  year = {2020},
  url = {https://github.com/mesjetiu/TFM_Arte_Sonoro_MEMORIA},
  school = {Universidad de Barcelona},
  file = {/home/carlos/Zotero/storage/TRE2DQ8J/TFM_Arte_Sonoro_MEMORIA.html}
}

@misc{gunasekarTextbooksAreAll2023,
  title = {Textbooks {{Are All You Need}}},
  author = {Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and {de Rosa}, Gustavo and Saarikivi, Olli and Salim, Adil and Shah, Shital and Behl, Harkirat Singh and Wang, Xin and Bubeck, S{\'e}bastien and Eldan, Ronen and Kalai, Adam Tauman and Lee, Yin Tat and Li, Yuanzhi},
  year = {2023},
  month = jun,
  number = {arXiv:2306.11644},
  eprint = {2306.11644},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.11644},
  url = {http://arxiv.org/abs/2306.11644},
  abstract = {We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6\% on HumanEval and 55.5\% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45\% on HumanEval.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/carlos/Zotero/storage/IMBD7FR8/Gunasekar et al. - 2023 - Textbooks Are All You Need.pdf;/home/carlos/Zotero/storage/PP9DGAW6/2306.html}
}

@misc{hanPreTrainedModelsPresent2021,
  title = {Pre-{{Trained Models}}: {{Past}}, {{Present}} and {{Future}}},
  shorttitle = {Pre-{{Trained Models}}},
  author = {Han, Xu and Zhang, Zhengyan and Ding, Ning and Gu, Yuxian and Liu, Xiao and Huo, Yuqi and Qiu, Jiezhong and Yao, Yuan and Zhang, Ao and Zhang, Liang and Han, Wentao and Huang, Minlie and Jin, Qin and Lan, Yanyan and Liu, Yang and Liu, Zhiyuan and Lu, Zhiwu and Qiu, Xipeng and Song, Ruihua and Tang, Jie and Wen, Ji-Rong and Yuan, Jinhui and Zhao, Wayne Xin and Zhu, Jun},
  year = {2021},
  month = aug,
  number = {arXiv:2106.07139},
  eprint = {2106.07139},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.07139},
  url = {http://arxiv.org/abs/2106.07139},
  abstract = {Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success and become a milestone in the field of artificial intelligence (AI). Owing to sophisticated pre-training objectives and huge model parameters, large-scale PTMs can effectively capture knowledge from massive labeled and unlabeled data. By storing knowledge into huge parameters and fine-tuning on specific tasks, the rich knowledge implicitly encoded in huge parameters can benefit a variety of downstream tasks, which has been extensively demonstrated via experimental verification and empirical analysis. It is now the consensus of the AI community to adopt PTMs as backbone for downstream tasks rather than learning models from scratch. In this paper, we take a deep look into the history of pre-training, especially its special relation with transfer learning and self-supervised learning, to reveal the crucial position of PTMs in the AI development spectrum. Further, we comprehensively review the latest breakthroughs of PTMs. These breakthroughs are driven by the surge of computational power and the increasing availability of data, towards four important directions: designing effective architectures, utilizing rich contexts, improving computational efficiency, and conducting interpretation and theoretical analysis. Finally, we discuss a series of open problems and research directions of PTMs, and hope our view can inspire and advance the future study of PTMs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/79LBRCTL/Han et al. - 2021 - Pre-Trained Models Past, Present and Future.pdf;/home/carlos/Zotero/storage/CEQNUL7H/2106.html}
}

@article{hernandez-olivanSurveyArtificialIntelligence2022,
  title = {A Survey on Artificial Intelligence for Music Generation: {{Agents}}, Domains and Perspectives},
  author = {{Hernandez-Olivan}, Carlos and {Hernandez-Olivan}, Javier and Beltran, Jose R},
  year = {2022},
  journal = {arXiv preprint arXiv:2210.13944},
  eprint = {2210.13944},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/498MZNDW/Hernandez-Olivan et al. - 2022 - A Survey on Artificial Intelligence for Music Gene.pdf}
}

@misc{HistoriaInteligenciaArtificial,
  title = {{Historia de la Inteligencia Artificial, el Machine Learning y el Deep Learning}},
  url = {https://www.algotive.ai/es-mx/blog/historia-de-la-inteligencia-artificial-el-machine-learning-y-el-deep-learning},
  urldate = {2023-10-30},
  abstract = {Conoce toda la historia de la Inteligencia Artificial (IA), el Machine Learning (ML) y el Deep Learning (DL), de manera breve, sencilla e ilustrativa.},
  language = {es-mx},
  file = {/home/carlos/Zotero/storage/LU4PW86U/historia-de-la-inteligencia-artificial-el-machine-learning-y-el-deep-learning.html}
}

@article{HochreiterVanishingGradient1998,
  title = {The Vanishing Gradient Problem during Learning Recurrent Neural Nets and Problem Solutions},
  author = {Hochreiter, Sepp},
  year = {1998},
  month = apr,
  journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  volume = {6},
  pages = {107--116},
  doi = {10.1142/S0218488598000094}
}

@misc{holtzmanCuriousCaseNeural2020,
  title = {The {{Curious Case}} of {{Neural Text Degeneration}}},
  author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  year = {2020},
  month = feb,
  number = {arXiv:1904.09751},
  eprint = {1904.09751},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1904.09751},
  url = {http://arxiv.org/abs/1904.09751},
  abstract = {Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: Published in ICLR 2020},
  file = {/home/carlos/Zotero/storage/J4H86DF3/Holtzman et al. - 2020 - The Curious Case of Neural Text Degeneration.pdf;/home/carlos/Zotero/storage/6Z9WQDE7/1904.html}
}

@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = {1989},
  month = jan,
  journal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(89)90020-8},
  url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
  abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
  keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
  file = {/home/carlos/Zotero/storage/DIC8KESB/0893608089900208.html}
}

@misc{HowGetBetter2023,
  title = {How to {{Get Better Outputs}} from {{Your Large Language Model}}},
  year = {2023},
  month = jun,
  journal = {NVIDIA Technical Blog},
  url = {https://developer.nvidia.com/blog/how-to-get-better-outputs-from-your-large-language-model/},
  urldate = {2024-01-25},
  abstract = {Large language models (LLMs) have generated excitement worldwide due to their ability to understand and process human language at a scale that is unprecedented. It has transformed the way that we{\dots}},
  language = {en-US},
  file = {/home/carlos/Zotero/storage/ZWDH2CVI/how-to-get-better-outputs-from-your-large-language-model.html}
}

@article{huangAgentCoderMultiAgentbasedCode2023,
  title = {{{AgentCoder}}: {{Multi-agent-based}} Code Generation with Iterative Testing and Optimisation},
  author = {Huang, Dong and Bu, Qingwen and Zhang, Jie M and Luck, Michael and Cui, Heming},
  year = {2023},
  journal = {arXiv preprint arXiv:2312.13010},
  eprint = {2312.13010},
  archiveprefix = {arxiv}
}

@misc{hungTranscriptionAllYou2020,
  title = {Transcription {{Is All You Need}}: {{Learning}} to {{Separate Musical Mixtures}} with {{Score}} as {{Supervision}}},
  shorttitle = {Transcription {{Is All You Need}}},
  author = {Hung, Yun-Ning and Wichern, Gordon and Roux, Jonathan Le},
  year = {2020},
  month = oct,
  number = {arXiv:2010.11904},
  eprint = {2010.11904},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2010.11904},
  url = {http://arxiv.org/abs/2010.11904},
  abstract = {Most music source separation systems require large collections of isolated sources for training, which can be difficult to obtain. In this work, we use musical scores, which are comparatively easy to obtain, as a weak label for training a source separation system. In contrast with previous score-informed separation approaches, our system does not require isolated sources, and score is used only as a training target, not required for inference. Our model consists of a separator that outputs a time-frequency mask for each instrument, and a transcriptor that acts as a critic, providing both temporal and frequency supervision to guide the learning of the separator. A harmonic mask constraint is introduced as another way of leveraging score information during training, and we propose two novel adversarial losses for additional fine-tuning of both the transcriptor and the separator. Results demonstrate that using score information outperforms temporal weak-labels, and adversarial structures lead to further improvements in both separation and transcription performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/carlos/Zotero/storage/H5CEA699/Hung et al. - 2020 - Transcription Is All You Need Learning to Separate Musical Mixtures with Score as Supervision.pdf;/home/carlos/Zotero/storage/QYZ9B53T/2010.html}
}

@misc{IntroducingChatGPT,
  title = {Introducing {{ChatGPT}}},
  url = {https://openai.com/blog/chatgpt},
  urldate = {2024-01-28},
  abstract = {We've trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.},
  language = {en-US},
  file = {/home/carlos/Zotero/storage/DJM6NATU/chatgpt.html}
}

@misc{IntroducingClaude,
  title = {Introducing {{Claude}}},
  url = {https://www.anthropic.com/news/introducing-claude},
  urldate = {2024-01-28},
  abstract = {Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.},
  language = {en},
  file = {/home/carlos/Zotero/storage/XLPRF8QQ/introducing-claude.html}
}

@article{InviernoIA2023,
  title = {{Invierno IA}},
  year = {2023},
  month = sep,
  journal = {Wikipedia, la enciclopedia libre},
  url = {https://es.wikipedia.org/w/index.php?title=Invierno_IA&oldid=154109150},
  urldate = {2024-01-28},
  abstract = {En la historia de la inteligencia artificial, un Invierno IA es un per{\'i}odo de reducci{\'o}n de fondos e inter{\'e}s en la investigaci{\'o}n de inteligencia artificial.[1]\hspace{0pt} El t{\'e}rmino fue acu{\~n}ado por analog{\'i}a a la idea del invierno nuclear.[2]\hspace{0pt} El t{\'e}rmino apareci{\'o} por primera vez en 1984 como el tema central de un debate p{\'u}blico en la conferencia anual de la AAAI. Es una reacci{\'o}n en cadena que comienza con el pesimismo de la comunidad de IA, seguido por el pesimismo en la prensa, seguido de un severo recorte en la financiaci{\'o}n, seguido por el final de la investigaci{\'o}n seria.[2]\hspace{0pt} En la conferencia, Roger Schank y Marvin Minsky-dos de los principales investigadores de la IA que hab{\'i}an sobrevivido el "invierno" de la d{\'e}cada de 1970, advirtieron a la comunidad de negocios que el entusiasmo por la IA hab{\'i}a crecido de forma descontrolada en la d{\'e}cada de 1980 y que, sin duda, la decepci{\'o}n ciertamente seguir{\'i}a. Tres a{\~n}os m{\'a}s tarde, la industria de la IA mil millones de d{\'o}lares comenz{\'o} a derrumbarse.[2]\hspace{0pt} El furor es com{\'u}n en diversas tecnolog{\'i}as emergentes, como lo fue la Mania del Ferrocarril o la Burbuja puntocom. El Invierno IA fue un resultado de ese furor, debido a promesas poco realistas por parte de los desarrolladores, expectativas altas de los usuarios finales y una amplia promoci{\'o}n en los medios.[3]\hspace{0pt}  A pesar de la subida y la ca{\'i}da de la reputaci{\'o}n de la IA, se ha continuado desarrollando nuevas tecnolog{\'i}as y exitosas tecnolog{\'i}as. El investigador Rodney Brooks se quejar{\'i}a en 2002 de que "existe este est{\'u}pido mito de que la IA ha fallado, pero la IA esta a su alrededor cada segundo del d{\'i}a." [4]\hspace{0pt} En el 2005, Ray Kurzweil estaba de acuerdo: "Muchos observadores siguen pensando que el invierno IA fue el final de la historia y que nada desde entonces ha venido del campo IA. Sin embargo, hoy en d{\'i}a miles de aplicaciones de la IA est{\'a}n profundamente arraigados en la infraestructura de todas las industrias."[5]\hspace{0pt} EL entusiasmo y optimismo sobre la IA ha aumentado gradualmente desde su punto m{\'a}s bajo en 1990. A partir de la d{\'e}cada del 2010 la Inteligencia artificial (y especialmente el subcampo del Aprendizaje autom{\'a}tico) empez{\'o} a ganar inter{\'e}s por parte de la comunidad de investigaci{\'o}n, lo que llev{\'o} a un auge dram{\'a}tico en el financiamiento y la inversi{\'o}n del sector.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  language = {es},
  annotation = {Page Version ID: 154109150},
  file = {/home/carlos/Zotero/storage/Z4HAAIN6/Invierno_IA.html}
}

@article{jiangMistral7B2023,
  title = {Mistral {{7B}}},
  author = {Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  year = {2023},
  journal = {arXiv preprint arXiv:2310.06825},
  eprint = {2310.06825},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/HZWHGN5Z/Jiang et al. - 2023 - Mistral 7B.pdf}
}

@article{jonesDoesGPT4Pass2023,
  title = {Does {{GPT-4}} Pass the Turing Test?},
  author = {Jones, Cameron and Bergen, Benjamin},
  year = {2023},
  journal = {arXiv preprint arXiv:2310.20216},
  eprint = {2310.20216},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/BQ9SQ2XE/Jones y Bergen - 2023 - Does GPT-4 Pass the Turing Test.pdf}
}

@misc{jzh2074,
  title = {{{AI}} Types. {{Tipos}} Inteligencia {{Artificial}}.Svg},
  author = {{Jzh2074}},
  year = {2022},
  url = {https://commons.wikimedia.org/wiki/File:AI_Types._Tipos_Inteligencia_Artificial.svg}
}

@article{kaddourChallengesApplicationsLarge2023,
  title = {Challenges and Applications of Large Language Models},
  author = {Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and Bradley, Herbie and Raileanu, Roberta and McHardy, Robert},
  year = {2023},
  journal = {arXiv preprint arXiv:2307.10169},
  eprint = {2307.10169},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/N4TI6Q5V/Kaddour et al. - 2023 - Challenges and Applications of Large Language Mode.pdf}
}

@misc{kainatIntroductionGenerativeAI2023,
  title = {Introduction to {{Generative AI}}},
  author = {Kainat},
  year = {2023},
  month = aug,
  journal = {Medium},
  url = {https://medium.com/@kitkat73275/introduction-to-generative-ai-833c9c467dfa},
  urldate = {2024-01-23},
  abstract = {AI (Artificial Intelligence) is the broad field of creating intelligent machines that can perform tasks that typically require human{\dots}},
  language = {en},
  file = {/home/carlos/Zotero/storage/HANH4MKW/introduction-to-generative-ai-833c9c467dfa.html}
}

@misc{kirkbrideQirkyFoxDot2023,
  title = {Qirky/{{FoxDot}}},
  author = {Kirkbride, Ryan},
  year = {2023},
  month = dec,
  url = {https://github.com/Qirky/FoxDot},
  urldate = {2023-12-09},
  abstract = {Python driven environment for Live Coding}
}

@misc{lewisRetrievalAugmentedGenerationKnowledgeIntensive2021,
  title = {Retrieval-{{Augmented Generation}} for {{Knowledge-Intensive NLP Tasks}}},
  author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
  year = {2021},
  month = apr,
  number = {arXiv:2005.11401},
  eprint = {2005.11401},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2005.11401},
  url = {http://arxiv.org/abs/2005.11401},
  abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: Accepted at NeurIPS 2020},
  file = {/home/carlos/Zotero/storage/WVAFYFB3/Lewis et al. - 2021 - Retrieval-Augmented Generation for Knowledge-Inten.pdf;/home/carlos/Zotero/storage/C495Y3UY/2005.html}
}

@misc{liSelfAlignmentInstructionBacktranslation2023,
  title = {Self-{{Alignment}} with {{Instruction Backtranslation}}},
  author = {Li, Xian and Yu, Ping and Zhou, Chunting and Schick, Timo and Zettlemoyer, Luke and Levy, Omer and Weston, Jason and Lewis, Mike},
  year = {2023},
  month = aug,
  number = {arXiv:2308.06259},
  eprint = {2308.06259},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.06259},
  url = {http://arxiv.org/abs/2308.06259},
  abstract = {We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/JADHWQUA/Li et al. - 2023 - Self-Alignment with Instruction Backtranslation.pdf;/home/carlos/Zotero/storage/4SRRSDIW/2308.html}
}

@article{ListAudioProgramming2023,
  title = {List of Audio Programming Languages},
  year = {2023},
  month = jul,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=List_of_audio_programming_languages&oldid=1167204231},
  urldate = {2023-12-11},
  abstract = {This is a list of notable programming languages optimized for sound production, algorithmic composition, and sound synthesis. ABC notation, a language for notating music using the ASCII character set Bol Processor, a model of formal grammars enriched with polymetric expressions for the representation of time structures ChucK, strongly timed, concurrent, and on-the-fly audio programming language Real-time Cmix, a MUSIC-N synthesis language somewhat similar to Csound Cmajor, a high-performance JIT-compiled C-style language for DSP Common Lisp Music (CLM), a music synthesis and signal processing package in the Music V family Csound, a MUSIC-N synthesis language released under the LGPL with many available unit generators Extempore, a live-coding environment that borrows a core foundation from the Impromptu environment FAUST, Functional Audio Stream, a functional compiled language for efficient real-time audio signal processing GLICOL, a graph-oriented live coding language written in Rust Hierarchical Music Specification Language (HMSL), optimized more for music than synthesis, developed in the 1980s in Forth Impromptu, a Scheme language environment for Mac OS X capable of sound and video synthesis, algorithmic composition, and 2D and 3D graphics programming Ixi lang, a programming language for live coding musical expression. JFugue, a Java and JVM library for programming music that outputs to MIDI and has the ability to convert to formats including ABC Notation, Lilypond, and MusicXML jMusic JSyn Keykit, programming language and portable graphical environment for MIDI music composition Kyma (sound design language) LilyPond, a computer program and file format for music engraving. Max/MSP, a proprietary, modular visual programming language aimed at sound synthesis for music Music Macro Language (MML), often used to produce chiptune music in Japan MUSIC-N, includes versions I, II, III, IV, IV-B, IV-BF, V, 11, and 360 Nyquist OpenMusic Orca (music programming language) Pure Data, a modular visual programming language for signal processing aimed at music creation Reaktor Sonic Pi Structured Audio Orchestra Language (SAOL), part of the MPEG-4 Structured Audio standard SuperCollider SynthEdit, a modular visual programming language for signal processing aimed at creating audio plug-ins},
  copyright = {Creative Commons Attribution-ShareAlike License},
  language = {en},
  annotation = {Page Version ID: 1167204231},
  file = {/home/carlos/Zotero/storage/GFR76MYV/List_of_audio_programming_languages.html}
}

@misc{liStructuredChainofThoughtPrompting2023,
  title = {Structured {{Chain-of-Thought Prompting}} for {{Code Generation}}},
  author = {Li, Jia and Li, Ge and Li, Yongmin and Jin, Zhi},
  year = {2023},
  month = sep,
  number = {arXiv:2305.06599},
  eprint = {2305.06599},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2305.06599},
  abstract = {Large Language Models (LLMs) (e.g., ChatGPT) have shown impressive performance in code generation. LLMs take prompts as inputs, and Chain-of-Thought (CoT) prompting is the state-of-theart prompting technique. CoT prompting asks LLMs first to generate CoTs (i.e., intermediate natural language reasoning steps) and then output the code. However, CoT prompting is designed for natural language generation and has low accuracy in code generation.},
  archiveprefix = {arxiv},
  language = {en},
  keywords = {Computer Science - Computation and Language,Computer Science - Software Engineering},
  file = {/home/carlos/Zotero/storage/KQ5L5V5X/Li et al. - 2023 - Structured Chain-of-Thought Prompting for Code Gen.pdf}
}

@misc{liSyntheticDataGeneration2023,
  title = {Synthetic {{Data Generation}} with {{Large Language Models}} for {{Text Classification}}: {{Potential}} and {{Limitations}}},
  shorttitle = {Synthetic {{Data Generation}} with {{Large Language Models}} for {{Text Classification}}},
  author = {Li, Zhuoyan and Zhu, Hangxiao and Lu, Zhuoran and Yin, Ming},
  year = {2023},
  month = oct,
  number = {arXiv:2310.07849},
  eprint = {2310.07849},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2310.07849},
  abstract = {The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. To better understand factors that moderate the effectiveness of the LLMgenerated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data. We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM for synthetic data generation1.},
  archiveprefix = {arxiv},
  language = {en},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/PRD9A6NF/Li et al. - 2023 - Synthetic Data Generation with Large Language Models for Text Classification Potential and Limitati.pdf}
}

@article{liuLostMiddleHow2023,
  title = {Lost in the Middle: {{How}} Language Models Use Long Contexts},
  author = {Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  year = {2023},
  journal = {arXiv preprint arXiv:2307.03172},
  eprint = {2307.03172},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/45RW8HRL/Liu et al. - 2023 - Lost in the Middle How Language Models Use Long C.pdf}
}

@misc{liuWavJourneyCompositionalAudio2023,
  title = {{{WavJourney}}: {{Compositional Audio Creation}} with {{Large Language Models}}},
  shorttitle = {{{WavJourney}}},
  author = {Liu, Xubo and Zhu, Zhongkai and Liu, Haohe and Yuan, Yi and Cui, Meng and Huang, Qiushi and Liang, Jinhua and Cao, Yin and Kong, Qiuqiang and Plumbley, Mark D. and Wang, Wenwu},
  year = {2023},
  month = nov,
  number = {arXiv:2307.14335},
  eprint = {2307.14335},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2307.14335},
  abstract = {Despite breakthroughs in audio generation models, their capabilities are often confined to domain-specific conditions such as speech transcriptions and audio captions. However, real-world audio creation aims to generate harmonious audio containing various elements such as speech, music, and sound effects with controllable conditions, which is challenging to address using existing audio generation systems. We present WavJourney, a novel framework that leverages Large Language Models (LLMs) to connect various audio models for audio creation. WavJourney allows users to create storytelling audio content with diverse audio elements simply from textual descriptions. Specifically, given a text instruction, WavJourney first prompts LLMs to generate an audio script that serves as a structured semantic representation of audio elements. The audio script is then converted into a computer program, where each line of the program calls a task-specific audio generation model or computational operation function. The computer program is then executed to obtain a compositional and interpretable solution for audio creation. Experimental results suggest that WavJourney is capable of synthesizing realistic audio aligned with textually-described semantic, spatial and temporal conditions, achieving state-of-the-art results on text-toaudio generation benchmarks. Additionally, we introduce a new multi-genre story benchmark. Subjective evaluations demonstrate the potential of WavJourney in crafting engaging storytelling audio content from text. We further demonstrate that WavJourney can facilitate human-machine co-creation in multi-round dialogues. To foster future research, the code and synthesized audio are available at: https://audio-agi.github.io/WavJourney\_demopage/.},
  archiveprefix = {arxiv},
  language = {en},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multimedia,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/carlos/Zotero/storage/FT53GGQ9/Liu et al. - 2023 - WavJourney Compositional Audio Creation with Large Language Models.pdf}
}

@misc{LLMPromptingGuide,
  title = {{{LLM}} Prompting Guide},
  url = {https://huggingface.co/docs/transformers/main/en/tasks/prompting},
  urldate = {2023-10-30},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  language = {English},
  file = {/home/carlos/Zotero/storage/MLZ8EYCS/prompting.html}
}

@article{luMuseCocoGeneratingSymbolic2023,
  title = {{{MuseCoco}}: {{Generating}} Symbolic Music from Text},
  author = {Lu, Peiling and Xu, Xin and Kang, Chenfei and Yu, Botao and Xing, Chengyi and Tan, Xu and Bian, Jiang},
  year = {2023},
  journal = {arXiv preprint arXiv:2306.00110},
  eprint = {2306.00110},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/PW28ZPDB/Lu et al. - 2023 - MuseCoco Generating Symbolic Music from Text.pdf}
}

@article{malachAutoRegressiveNextTokenPredictors2023,
  title = {Auto-Regressive next-Token Predictors Are Universal Learners},
  author = {Malach, Eran},
  year = {2023},
  journal = {arXiv preprint arXiv:2309.06979},
  eprint = {2309.06979},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/3B7X3RYX/Malach - 2023 - Auto-Regressive Next-Token Predictors are Universa.pdf}
}

@article{manickamResearchStudyApplications2017,
  title = {Research Study on Applications of Artificial Neural Networks and E-Learning Personalization},
  author = {Manickam, M.R.M. and Mohanapriya, M. and Kale, Sandip and Uday, M. and Kulkarni, Prashant and Khandagale, Y. and Patil, S.P.},
  year = {2017},
  month = aug,
  journal = {International Journal of Civil Engineering and Technology},
  volume = {8},
  pages = {1422--1432},
  abstract = {The artificial neural network may likely be the complete solution over the most recent decades which have been broadly utilized as a part of a huge variety of applications. This article focuses on vast artificial neural network applications and importance of e-Learning application. To assess the impact of personalized learning in neural network applications. It is a need to adapt in new smart eLearning system for individual learners personalization. Artificial Neural Networks methodology to the development of new neural network model with an appropriate way of problems formulation is presented in this paper. Student's performance prediction using neural system its impact is presented to understand the necessity of neural network in smart e-learning model. The outcome focused on the importance of using neural networks in possible applications and its influence on learner's progress with personalization system.},
  file = {/home/carlos/Zotero/storage/WR3XIC7Q/Manickam et al. - 2017 - Research study on applications of artificial neural networks and e-learning personalization.pdf}
}

@misc{mastropaoloRobustnessCodeGeneration2023,
  title = {On the {{Robustness}} of {{Code Generation Techniques}}: {{An Empirical Study}} on {{GitHub Copilot}}},
  shorttitle = {On the {{Robustness}} of {{Code Generation Techniques}}},
  author = {Mastropaolo, Antonio and Pascarella, Luca and Guglielmi, Emanuela and Ciniselli, Matteo and Scalabrino, Simone and Oliveto, Rocco and Bavota, Gabriele},
  year = {2023},
  month = feb,
  number = {arXiv:2302.00438},
  eprint = {2302.00438},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2302.00438},
  abstract = {Software engineering research has always being concerned with the improvement of code completion approaches, which suggest the next tokens a developer will likely type while coding. The release of GitHub Copilot constitutes a big step forward, also because of its unprecedented ability to automatically generate even entire functions from their natural language description. While the usefulness of Copilot is evident, it is still unclear to what extent it is robust. Specifically, we do not know the extent to which semantic-preserving changes in the natural language description provided to the model have an effect on the generated code function. In this paper we present an empirical study in which we aim at understanding whether different but semantically equivalent natural language descriptions result in the same recommended function. A negative answer would pose questions on the robustness of deep learning (DL)-based code generators since it would imply that developers using different wordings to describe the same code would obtain different recommendations. We asked Copilot to automatically generate 892 Java methods starting from their original Javadoc description. Then, we generated different semantically equivalent descriptions for each method both manually and automatically, and we analyzed the extent to which predictions generated by Copilot changed. Our results show that modifying the description results in different code recommendations in {\textasciitilde}46\% of cases. Also, differences in the semantically equivalent descriptions might impact the correctness of the generated code {\textasciitilde}28\%.},
  archiveprefix = {arxiv},
  language = {en},
  keywords = {Computer Science - Software Engineering},
  file = {/home/carlos/Zotero/storage/HYGTSJJX/Mastropaolo et al. - 2023 - On the Robustness of Code Generation Techniques A.pdf}
}

@misc{mehdiAnnouncingMicrosoftCopilot2023,
  title = {Announcing {{Microsoft Copilot}}, Your Everyday {{AI}} Companion},
  author = {Mehdi, Yusuf},
  year = {2023},
  month = sep,
  journal = {The Official Microsoft Blog},
  url = {https://blogs.microsoft.com/blog/2023/09/21/announcing-microsoft-copilot-your-everyday-ai-companion/},
  urldate = {2024-01-28},
  abstract = {Updated November 15, 2023: To simplify the user experience and make Copilot more accessible to everyone, Bing Chat and Bing Chat Enterprise will now simply become Microsoft Copilot. For more information: https://aka.ms/BingIgnite We are entering a new era of AI, one that is fundamentally changing how we relate to and benefit from technology. With the...},
  language = {en-US},
  file = {/home/carlos/Zotero/storage/ED7X9C4H/announcing-microsoft-copilot-your-everyday-ai-companion.html}
}

@article{michaelisBroadDatasetAll2022,
  title = {A Broad Dataset Is All You Need for One-Shot Object Detection},
  author = {Michaelis, Claudio and Bethge, Matthias and Ecker, Alexander S},
  year = {2020},
  journal = {arXiv preprint arXiv:2011.04267},
  eprint = {2011.04267},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/TTGXP52I/Michaelis et al. - 2022 - A Broad Dataset is All You Need for One-Shot Object Detection.pdf}
}

@book{minsky1969perceptrons,
  title = {Perceptrons: An Introduction to Computational Geometry},
  author = {Minsky, M. and Papert, S.},
  year = {1969},
  publisher = {{MIT Press}},
  url = {https://books.google.es/books?id=Ow1OAQAAIAAJ},
  isbn = {978-0-262-63022-1},
  lccn = {69014379}
}

@book{minskyPerceptronsIntroductionComputational2017,
  title = {Perceptrons: {{An Introduction}} to {{Computational Geometry}}},
  shorttitle = {Perceptrons},
  author = {Minsky, Marvin and Papert, Seymour A.},
  year = {2017},
  month = sep,
  publisher = {{The MIT Press}},
  doi = {10.7551/mitpress/11301.001.0001},
  url = {https://direct.mit.edu/books/book/3132/PerceptronsAn-Introduction-to-Computational},
  abstract = {The first systematic study of parallelism in computation by two pioneers in the field.Reissue of the 1988 Expanded Edition with a new foreword by L{\'e}on BottouIn},
  isbn = {978-0-262-34393-0},
  language = {en},
  file = {/home/carlos/Zotero/storage/VUWTY5QW/PerceptronsAn-Introduction-to-Computational.html}
}

@article{mishkinAllYouNeed2016,
  title = {All You Need Is a Good Init},
  author = {Mishkin, Dmytro and Matas, Jiri},
  year = {2015},
  journal = {arXiv preprint arXiv:1511.06422},
  eprint = {1511.06422},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/Z8PT92DE/Mishkin y Matas - 2016 - All you need is a good init.pdf}
}

@article{ModelacionLenguaje2024,
  title = {{Modelaci{\'o}n del lenguaje}},
  year = {2024},
  month = jan,
  journal = {Wikipedia, la enciclopedia libre},
  url = {https://es.wikipedia.org/w/index.php?title=Modelaci%C3%B3n_del_lenguaje&oldid=157157282},
  urldate = {2024-01-24},
  abstract = {Un modelo estad{\'i}stico de lenguaje asigna una probabilidad a una secuencia de m palabras                         P         (                    w                        1                             ,         {\dots}         ,                    w                        m                             )                 \{{\textbackslash}displaystyle P(w\_\{1\},{\textbackslash}ldots ,w\_\{m\})\}    mediante una distribuci{\'o}n de probabilidad. Contar con una forma de estimar la verosimilitud de diversas frases resulta sumamente {\'u}til en una variedad de aplicaciones dentro del procesamiento del lenguaje natural. Los modelos de lenguaje se emplean en el reconocimiento de voz, la traducci{\'o}n autom{\'a}tica, el etiquetado de discurso, el an{\'a}lisis de texto, el reconocimiento de escritura, la recuperaci{\'o}n de informaci{\'o}n y otras muchas aplicaciones.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  language = {es},
  annotation = {Page Version ID: 157157282},
  file = {/home/carlos/Zotero/storage/LTFQTHQH/Modelación_del_lenguaje.html}
}

@misc{mohanStructureReinforcementLearning2023,
  title = {Structure in {{Reinforcement Learning}}: {{A Survey}} and {{Open Problems}}},
  shorttitle = {Structure in {{Reinforcement Learning}}},
  author = {Mohan, Aditya and Zhang, Amy and Lindauer, Marius},
  year = {2023},
  month = aug,
  number = {arXiv:2306.16021},
  eprint = {2306.16021},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.16021},
  url = {http://arxiv.org/abs/2306.16021},
  abstract = {Reinforcement Learning (RL), bolstered by the expressive capabilities of Deep Neural Networks (DNNs) for function approximation, has demonstrated considerable success in numerous applications. However, its practicality in addressing various real-world scenarios, characterized by diverse and unpredictable dynamics, noisy signals, and large state and action spaces, remains limited. This limitation stems from issues such as poor data efficiency, limited generalization capabilities, a lack of safety guarantees, and the absence of interpretability, among other factors. To overcome these challenges and improve performance across these crucial metrics, one promising avenue is to incorporate additional structural information about the problem into the RL learning process. Various sub-fields of RL have proposed methods for incorporating such inductive biases. We amalgamate these diverse methodologies under a unified framework, shedding light on the role of structure in the learning problem, and classify these methods into distinct patterns of incorporating structure. By leveraging this comprehensive framework, we provide valuable insights into the challenges of structured RL and lay the groundwork for a design pattern perspective on RL research. This novel perspective paves the way for future advancements and aids in developing more effective and efficient RL algorithms that can potentially handle real-world scenarios better.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/carlos/Zotero/storage/GJR7LZLG/Mohan et al. - 2023 - Structure in Reinforcement Learning A Survey and Open Problems.pdf;/home/carlos/Zotero/storage/MR6TEXIV/2306.html}
}

@article{moradidakhelGitHubCopilotAI2023,
  title = {{{GitHub Copilot AI}} Pair Programmer: {{Asset}} or {{Liability}}?},
  shorttitle = {{{GitHub Copilot AI}} Pair Programmer},
  author = {Moradi Dakhel, Arghavan and Majdinasab, Vahid and Nikanjam, Amin and Khomh, Foutse and Desmarais, Michel C. and Jiang, Zhen Ming (Jack)},
  year = {2023},
  month = sep,
  journal = {Journal of Systems and Software},
  volume = {203},
  pages = {111734},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2023.111734},
  url = {https://www.sciencedirect.com/science/article/pii/S0164121223001292},
  abstract = {Automatic program synthesis is a long-lasting dream in software engineering. Recently, a promising Deep Learning (DL) based solution, called Copilot, has been proposed by OpenAI and Microsoft as an industrial product. Although some studies evaluate the correctness of Copilot solutions and report its issues, more empirical evaluations are necessary to understand how developers can benefit from it effectively. In this paper, we study the capabilities of Copilot in two different programming tasks: (i) generating (and reproducing) correct and efficient solutions for fundamental algorithmic problems, and (ii) comparing Copilot's proposed solutions with those of human programmers on a set of programming tasks. For the former, we assess the performance and functionality of Copilot in solving selected fundamental problems in computer science, like sorting and implementing data structures. In the latter, a dataset of programming problems with human-provided solutions is used. The results show that Copilot is capable of providing solutions for almost all fundamental algorithmic problems, however, some solutions are buggy and non-reproducible. Moreover, Copilot has some difficulties in combining multiple methods to generate a solution. Comparing Copilot to humans, our results show that the correct ratio of humans' solutions is greater than Copilot's suggestions, while the buggy solutions generated by Copilot require less effort to be repaired. Based on our findings, if Copilot is used by expert developers in software projects, it can become an asset since its suggestions could be comparable to humans' contributions in terms of quality. However, Copilot can become a liability if it is used by novice developers who may fail to filter its buggy or non-optimal solutions due to a lack of expertise.},
  keywords = {Code completion,GitHub copilot,Language model,Testing},
  file = {/home/carlos/Zotero/storage/UZ645QAQ/Moradi Dakhel et al. - 2023 - GitHub Copilot AI pair programmer Asset or Liabil.pdf;/home/carlos/Zotero/storage/U3TB2FHT/S0164121223001292.html}
}

@article{mukherjeeOrcaProgressiveLearning2023,
  title = {Orca: {{Progressive Learning}} from {{Complex Explanation Traces}} of {{GPT-4}}},
  shorttitle = {Orca},
  author = {Mukherjee, Subhabrata (Subho) and Mitra, Arindam and Jawahar, Ganesh and Agarwal, Sahaj and Palangi, Hamid and Awadallah, Ahmed H.},
  year = {2023},
  month = jun,
  url = {https://www.microsoft.com/en-us/research/publication/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4/},
  abstract = {Recent research has focused on enhancing the capability of smaller models through imitation~learning, drawing on the outputs generated by large foundation models (LFMs). A number of issues impact the quality of these models, ranging from~limited imitation signals~from shallow LFM outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting in [{\dots}]},
  language = {en-US},
  file = {/home/carlos/Zotero/storage/ANQ2XMEH/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4.html}
}

@misc{MusicLM,
  title = {{{MusicLM}}},
  url = {https://google-research.github.io/seanet/musiclm/examples/},
  urldate = {2024-02-05}
}

@article{nielsenNeuralNetworksDeep2015,
  title = {Neural {{Networks}} and {{Deep Learning}}},
  author = {Nielsen, Michael A.},
  year = {2015},
  publisher = {{Determination Press}},
  url = {http://neuralnetworksanddeeplearning.com},
  language = {en},
  file = {/home/carlos/Zotero/storage/TZXGVB5T/index.html}
}

@misc{nishiKentoNishiAwesomeallyouneedpapers2024,
  title = {{{KentoNishi}}/Awesome-All-You-Need-Papers},
  author = {Nishi, Kento},
  year = {2024},
  month = jan,
  url = {https://github.com/KentoNishi/awesome-all-you-need-papers},
  urldate = {2024-01-27},
  abstract = {A list of all "all you need" papers. Updated daily using the arXiv API.},
  keywords = {arxiv,awesome-list,machine-learning,papers}
}

@misc{noeverTuringDeception2022,
  title = {The {{Turing Deception}}},
  author = {Noever, David and Ciolino, Matt},
  year = {2022},
  month = dec,
  number = {arXiv:2212.06721},
  eprint = {2212.06721},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.06721},
  url = {http://arxiv.org/abs/2212.06721},
  abstract = {This research revisits the classic Turing test and compares recent large language models such as ChatGPT for their abilities to reproduce human-level comprehension and compelling text generation. Two task challenges -- summarization, and question answering -- prompt ChatGPT to produce original content (98-99\%) from a single text entry and also sequential questions originally posed by Turing in 1950. We score the original and generated content against the OpenAI GPT-2 Output Detector from 2019, and establish multiple cases where the generated content proves original and undetectable (98\%). The question of a machine fooling a human judge recedes in this work relative to the question of "how would one prove it?" The original contribution of the work presents a metric and simple grammatical set for understanding the writing mechanics of chatbots in evaluating their readability and statistical clarity, engagement, delivery, and overall quality. While Turing's original prose scores at least 14\% below the machine-generated output, the question of whether an algorithm displays hints of Turing's truly original thoughts (the "Lovelace 2.0" test) remains unanswered and potentially unanswerable for now.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/carlos/Zotero/storage/MTI49WUJ/Noever y Ciolino - 2022 - The Turing Deception.pdf;/home/carlos/Zotero/storage/JKLFGCTV/2212.html}
}

@phdthesis{nordgrenPROMPTENGINEERINGITS2023,
  title = {{{PROMPT ENGINEERING AND ITS USABILITY TO IMPROVE MODERN PSYCHOLOGY CHATBOTS}}},
  author = {Nordgren, Isak J and Svensson, L Gustaf E},
  year = {2023},
  abstract = {As advancements in chatbots and Large Language Models (LLMs) such as GPT-3.5 and GPT-4 continue, their applications in diverse fields, including psychology, expand. This study investigates the effectiveness of LLMs optimized through prompt engineering, aiming to enhance their performance in psychological applications. To this end, two distinct versions of a GPT-3.5-based chatbot were developed: a version similar to the base model, and a version equipped with a more extensive system prompt detailing expected behavior.},
  language = {en},
  file = {/home/carlos/Zotero/storage/PXI4SHRK/Nordgren y Svensson - PROMPT ENGINEERING AND ITS USABILITY TO IMPROVE MODERN PSYCHOLOGY CHATBOTS.pdf}
}

@article{NurArtificialNeural2014,
  title = {Artificial Neural Network Weight Optimization: {{A}} Review},
  author = {Nur, Abdirashid and Radzi, Nor and Osman, Ashraf},
  year = {2014},
  month = sep,
  journal = {TELKOMNIKA},
  volume = {12},
  doi = {10.11591/telkomnika.v12i9.6264}
}

@article{osheaIntroductionConvolutionalNeural2015,
  title = {An Introduction to Convolutional Neural Networks},
  author = {O'Shea, Keiron and Nash, Ryan},
  year = {2015},
  journal = {arXiv preprint arXiv:1511.08458},
  eprint = {1511.08458},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/DVHFDFAY/O'Shea y Nash - 2015 - An Introduction to Convolutional Neural Networks.pdf}
}

@misc{OvertoneCollaborativeProgrammable,
  title = {Overtone - {{Collaborative Programmable Music}}},
  url = {https://overtone.github.io/},
  urldate = {2023-12-09},
  file = {/home/carlos/Zotero/storage/LXF2GLUX/overtone.github.io.html}
}

@misc{PapersCodeAudioLM,
  title = {Papers with {{Code}} - {{AudioLM}}: A {{Language Modeling Approach}} to {{Audio Generation}}},
  shorttitle = {Papers with {{Code}} - {{AudioLM}}},
  url = {https://paperswithcode.com/paper/audiolm-a-language-modeling-approach-to-audio},
  urldate = {2024-01-04},
  abstract = {Implemented in 4 code libraries.},
  language = {en},
  file = {/home/carlos/Zotero/storage/U33C57MP/audiolm-a-language-modeling-approach-to-audio.html}
}

@misc{PapersCodeMuseCoco,
  title = {Papers with {{Code}} - {{MuseCoco}}: {{Generating Symbolic Music}} from {{Text}}},
  shorttitle = {Papers with {{Code}} - {{MuseCoco}}},
  url = {https://paperswithcode.com/paper/musecoco-generating-symbolic-music-from-text},
  urldate = {2024-01-04},
  abstract = {Implemented in one code library.},
  language = {en},
  file = {/home/carlos/Zotero/storage/WCM7YULJ/musecoco-generating-symbolic-music-from-text.html}
}

@misc{PapersCodeRobustness,
  title = {Papers with {{Code}} - {{On}} the {{Robustness}} of {{Code Generation Techniques}}: {{An Empirical Study}} on {{GitHub Copilot}}},
  shorttitle = {Papers with {{Code}} - {{On}} the {{Robustness}} of {{Code Generation Techniques}}},
  url = {https://cs.paperswithcode.com/paper/on-the-robustness-of-code-generation},
  urldate = {2023-12-26},
  abstract = {Implemented in one code library.},
  language = {en},
  file = {/home/carlos/Zotero/storage/T33YVX3N/on-the-robustness-of-code-generation.html}
}

@misc{PapersCodeWavJourney,
  title = {Papers with {{Code}} - {{WavJourney}}: {{Compositional Audio Creation}} with {{Large Language Models}}},
  shorttitle = {Papers with {{Code}} - {{WavJourney}}},
  url = {https://paperswithcode.com/paper/wavjourney-compositional-audio-creation-with},
  urldate = {2024-01-04},
  abstract = {Implemented in one code library.},
  language = {en},
  file = {/home/carlos/Zotero/storage/WIJE3S3G/wavjourney-compositional-audio-creation-with.html}
}

@inproceedings{pascanuDifficultyTrainingRecurrent2013,
  title = {On the Difficulty of Training Recurrent Neural Networks},
  booktitle = {International Conference on Machine Learning},
  author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  year = {2013},
  pages = {1310--1318},
  publisher = {{Pmlr}},
  file = {/home/carlos/Zotero/storage/QGYUZ4WR/Pascanu et al. - 2013 - On the difficulty of training Recurrent Neural Networks.pdf}
}

@misc{pengImpactAIDeveloper2023a,
  title = {The {{Impact}} of {{AI}} on {{Developer Productivity}}: {{Evidence}} from {{GitHub Copilot}}},
  shorttitle = {The {{Impact}} of {{AI}} on {{Developer Productivity}}},
  author = {Peng, Sida and Kalliamvakou, Eirini and Cihon, Peter and Demirer, Mert},
  year = {2023},
  month = feb,
  number = {arXiv:2302.06590},
  eprint = {2302.06590},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2302.06590},
  abstract = {Generative AI tools hold promise to increase human productivity. This paper presents results from a controlled experiment with GitHub Copilot, an AI pair programmer. Recruited software developers were asked to implement an HTTP server in JavaScript as quickly as possible. The treatment group, with access to the AI pair programmer, completed the task 55.8\% faster than the control group. Observed heterogenous effects show promise for AI pair programmers to help people transition into software development careers.},
  archiveprefix = {arxiv},
  language = {en},
  keywords = {Computer Science - Software Engineering},
  file = {/home/carlos/Zotero/storage/UEPW89PF/Peng et al. - 2023 - The Impact of AI on Developer Productivity Eviden.pdf}
}

@book{penroseNuevaMenteEmperador2015,
  title = {{La nueva mente del emperador}},
  author = {Penrose, Roger},
  year = {2015},
  month = jul,
  publisher = {{Penguin Random House Grupo Editorial Espa{\~n}a}},
  abstract = {Un apasionante paseo por la matem{\'a}tica y la f{\'i}sica, y por los hallazgos del pensamiento humano de la mano de Roger Penrose, Premio Nobel de F{\'i}sica 2020. Durante d{\'e}cadas, los defensores de la inteligencia artificial han mantenido que los ordenadores har{\'a}n pronto todo lo que la mente humana puede hacer. En su favor, se puede utilizar, por ejemplo, el que ya hay m{\'a}quinas que juegan al ajedrez como los grandes maestros. Ahora bien, {\textquestiondown}comprenden el juego como lo hacemos nosotros?En este libro, Roger Penrose, probablemente el especialista en la teor{\'i}a general de la relatividad m{\'a}s prestigioso del mundo y una de las mentes anal{\'i}ticas m{\'a}s originales de la actualidad, sostiene que existen facetas del pensamiento humano que nunca ser{\'a}n emuladas por un ordenador. Para defender esa tesis, Penrose recurre a una amplia gama de conocimientos cient{\'i}ficos, que van desde la m{\'a}quina de Turing hasta la estructura del cerebro, pasando por el teorema de G{\"o}del, los agujeros negros y los blancos, la radiaci{\'o}n de Hawking, la entrop{\'i}a o la mec{\'a}nica cu{\'a}ntica. Entre los numerosos estudios existentes dedicados a la relaci{\'o}n entre la mente y el cuerpo, esta ambiciosa obra sobresale tanto por su lucidez y claridad como por su rigor y profundidad. Rese{\~n}a:{\guillemotleft}Un libro audaz, brillante e innovador. Cuando Penrose habla, los cient{\'i}ficos escuchan.{\guillemotright}The New York Times Book Review},
  googlebooks = {sLz3CQAAQBAJ},
  isbn = {978-84-663-3083-1},
  language = {es},
  keywords = {Computers / Artificial Intelligence / General,Science / Essays,Science / General,Science / Life Sciences / Neuroscience}
}

@misc{perezThreadsYaSupera2023,
  title = {{Threads ya supera los 100 millones de usuarios. Ha aplastado el r{\'e}cord que ten{\'i}a ChatGPT}},
  author = {P{\'e}rez, Enrique},
  year = {2023},
  month = jul,
  journal = {Xataka},
  url = {https://www.xataka.com/aplicaciones/threads-supera-100-millones-usuarios-ha-aplastado-record-que-tenia-chatgpt},
  urldate = {2023-12-08},
  abstract = {La tecnolog{\'i}a va a un ritmo vertiginoso. Threads, el rival de Twitter de Instagram, lleg{\'o} oficialmente la semana pasada. Y aunque para usarlo en Europa hay...},
  chapter = {aplicaciones},
  language = {es},
  file = {/home/carlos/Zotero/storage/9CKSGAN7/threads-supera-100-millones-usuarios-ha-aplastado-record-que-tenia-chatgpt.html}
}

@misc{ph.dTrainingTimeFoundation2023,
  title = {The Training Time of the Foundation Models (from Scratch)},
  author = {Ph.D, Micha{\l} Marci{\'n}czuk},
  year = {2023},
  month = nov,
  journal = {CodeNLP},
  url = {https://medium.com/codenlp/the-training-time-of-the-foundation-models-from-scratch-59bbce90cc87},
  urldate = {2024-01-21},
  abstract = {How many GPU hours does it take to train a large language model (LLM)?},
  language = {en},
  file = {/home/carlos/Zotero/storage/WE2QT4G9/the-training-time-of-the-foundation-models-from-scratch-59bbce90cc87.html}
}

@misc{PrimerosMesesChat2023,
  title = {{Primeros 6 meses del Chat GPT, {\textquestiondown}cu{\'a}les son sus logros y desaf{\'i}os?}},
  year = {2023},
  month = may,
  journal = {Noticias Radio Reflejos},
  url = {https://www.noticiasradioreflejos.com.ar/2021/primeros-6-meses-del-chat-gpt-cuales-son-sus-logros-y-desafios/},
  urldate = {2023-12-08},
  abstract = {Buenos Aires, mayo de 2023 {\textendash} Bruno Ruy{\'u}, profesor de la Maestr{\'i}a de Ciencias de Datos de la Universidad Austral, hace un repaso por los principales logros y desaf{\'i}os del ChatGPT. ``El ChatGPT logr{\'o} alcanzar los 100 millones de usuarios en dos meses, un n{\'u}mero impresionante. Mientras Instagram alcanz{\'o} este record en 30 meses, Spotify [{\dots}]},
  language = {es},
  file = {/home/carlos/Zotero/storage/K7W3P7AF/primeros-6-meses-del-chat-gpt-cuales-son-sus-logros-y-desafios.html}
}

@misc{PromptEngineeringGuide2023,
  title = {Prompt {{Engineering Guide}}},
  year = {2023},
  month = dec,
  url = {https://www.promptingguide.ai/},
  urldate = {2024-01-05},
  abstract = {A Comprehensive Overview of Prompt Engineering},
  language = {en},
  file = {/home/carlos/Zotero/storage/866LWP67/www.promptingguide.ai.html}
}

@misc{PureDataPd,
  title = {Pure {{Data}} {\textemdash} {{Pd Community Site}}},
  url = {https://puredata.info/},
  urldate = {2024-01-28},
  file = {/home/carlos/Zotero/storage/YN45KFD2/puredata.info.html}
}

@misc{QueConsistenModelos,
  title = {{{\textquestiondown}En qu{\'e} consisten los modelos autorregresivos?: Explicaci{\'o}n sobre los modelos autorregresivos: AWS}},
  shorttitle = {{{\textquestiondown}En qu{\'e} consisten los modelos autorregresivos?}},
  journal = {Amazon Web Services, Inc.},
  url = {https://aws.amazon.com/es/what-is/autoregressive-models/},
  urldate = {2024-01-26},
  abstract = {En qu{\'e} consisten los modelos autorregresivos, de qu{\'e} manera y por qu{\'e} las empresas recurren a estos y c{\'o}mo se utilizan con AWS.},
  language = {es-ES},
  file = {/home/carlos/Zotero/storage/6AHZ6W4R/autoregressive-models.html}
}

@misc{QueEsAjuste,
  title = {{{\textquestiondown}Qu{\'e} es el ajuste de hiperpar{\'a}metros?}},
  shorttitle = {{{\textquestiondown}Qu{\'e} es el ajuste de hiperpar{\'a}metros?}},
  journal = {Amazon Web Services, Inc.},
  url = {https://aws.amazon.com/es/what-is/hyperparameter-tuning/},
  urldate = {2024-01-26},
  abstract = {Qu{\'e} es el ajuste de hiperpar{\'a}metros, c{\'o}mo y por qu{\'e} lo utiliza las empresas, y c{\'o}mo se utiliza con AWS.},
  language = {es-ES},
  file = {/home/carlos/Zotero/storage/XYHKEV34/hyperparameter-tuning.html}
}

@inproceedings{radfordLanguageModelsAre2019,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, D. and Amodei, Dario and Sutskever, I.},
  year = {2019},
  url = {https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  file = {/home/carlos/Zotero/storage/IDI7KPUK/Radford et al. - 2019 - Language Models are Unsupervised Multitask Learners.pdf}
}

@misc{radovanovicEmergingArchitecturesLLM2023,
  title = {Emerging {{Architectures}} for {{LLM Applications}}},
  author = {Radovanovic, Rajko, Matt Bornstein},
  year = {2023},
  month = jun,
  journal = {Andreessen Horowitz},
  url = {https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/},
  urldate = {2023-06-27},
  abstract = {A reference architecture for the LLM app stack. It shows the most common systems, tools, and design patterns used by AI startups and tech companies.},
  language = {en-US},
  file = {/home/carlos/Zotero/storage/PAWYWIEW/emerging-architectures-for-llm-applications.html}
}

@article{ramsauerHopfieldNetworksAll2021,
  title = {Hopfield Networks Is All You Need},
  author = {Ramsauer, Hubert and Sch{\"a}fl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlovi{\'c}, Milena and Sandve, Geir Kjetil and others},
  year = {2020},
  journal = {arXiv preprint arXiv:2008.02217},
  eprint = {2008.02217},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/36MASUK3/Ramsauer et al. - 2021 - Hopfield Networks is All You Need.pdf}
}

@misc{RegressionVsClassification2021,
  title = {Regression vs {{Classification}}, {{Explained}} - {{Sharp Sight}}},
  shorttitle = {Regression vs {{Classification}}},
  year = {2021},
  month = apr,
  url = {https://www.sharpsightlabs.com/blog/regression-vs-classification/},
  urldate = {2024-01-26},
  abstract = {This article explains the difference between regression vs classification in machine learning. For ML tutorials, sign up for our email list.},
  chapter = {data science},
  language = {en-US},
  file = {/home/carlos/Zotero/storage/286HSH4E/regression-vs-classification.html}
}

@misc{RegressionVsClassification2021a,
  title = {Regression vs. {{Classification}} in {{Machine Learning}}: {{What}}'s the {{Difference}}?},
  shorttitle = {Regression vs. {{Classification}} in {{Machine Learning}}},
  year = {2021},
  month = oct,
  journal = {Springboard Blog},
  url = {https://www.springboard.com/blog/data-science/regression-vs-classification/},
  urldate = {2024-01-26},
  abstract = {Comparing regression vs classification in machine learning can sometimes confuse even the most seasoned data scientists. This can eventually make it difficult},
  chapter = {Data Science},
  language = {en},
  file = {/home/carlos/Zotero/storage/7CPDH77Z/regression-vs-classification.html}
}

@misc{RevistaBitsCiencia,
  title = {{Revista Bits de Ciencia N{$^\circ$} 21 - 2021}},
  url = {https://revistasdex.uchile.cl/files/full/BitsdeCiencia_(21)_2021/index.html},
  urldate = {2023-10-30},
  chapter = {Article Section},
  language = {es},
  file = {/home/carlos/Zotero/storage/6QXLYRRD/index.html}
}

@book{roadsComputerMusicTutorial1996,
  title = {The {{Computer Music Tutorial}}},
  author = {Roads, Curtis and Strawn, John and Abbott, Curtis and Gordon, John and Philip, Greenspun},
  year = {1996},
  publisher = {{The MIT Press}},
  address = {{Cambride}}
}

@article{rosenblattPerceptronProbabilisticModel1958,
  title = {The Perceptron: {{A}} Probabilistic Model for Information Storage and Organization in the Brain.},
  shorttitle = {The Perceptron},
  author = {Rosenblatt, F.},
  year = {1958},
  journal = {Psychological Review},
  volume = {65},
  number = {6},
  pages = {386--408},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/h0042519},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0042519},
  abstract = {The first of these questions is in the province of sensory physiology, and is the only one for which appreciable understanding has been achieved. This article will be concerned primarily with the second and third questions, which are still subject to a vast amount of speculation, and where the few relevant facts currently supplied by neurophysiology have not yet been integrated into an acceptable theory. With regard to the second question, two alternative positions have been maintained. The first suggests that storage of sensory information is in the form of coded representations or images, with some sort of one-to-one mapping between the sensory stimulus},
  language = {en},
  file = {/home/carlos/Zotero/storage/9Z6ZPSWA/Rosenblatt - 1958 - The perceptron A probabilistic model for informat.pdf}
}

@book{rothmanTransformersNaturalLanguage2021,
  title = {Transformers for {{Natural Language Processing}}: {{Build}} Innovative Deep Neural Network Architectures for {{NLP}} with {{Python}}, {{PyTorch}}, {{TensorFlow}}, {{BERT}}, {{RoBERTa}}, and More},
  shorttitle = {Transformers for {{Natural Language Processing}}},
  author = {Rothman, Denis},
  year = {2021},
  publisher = {{Packt Publishing}},
  address = {{Birmingham}},
  isbn = {978-1-80056-579-1},
  language = {eng}
}

@article{roziereCodeLlamaOpena,
  title = {Code {{Llama}}: {{Open Foundation Models}} for {{Code}}},
  author = {Rozi{\`e}re, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Ellen, Xiaoqing and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and Kozhevnikov, Artyom and Evtimov, Ivan and Bitton, Joanna and Bhatt, Manish and Ferrer, Cristian Canton and Grattafiori, Aaron and Xiong, Wenhan and D{\'e}fossez, Alexandre and Copet, Jade and Azhar, Faisal and Touvron, Hugo and Martin, Louis and Usunier, Nicolas and Scialom, Thomas and Synnaeve, Gabriel},
  language = {en},
  file = {/home/carlos/Zotero/storage/FEFJVJDY/Rozière et al. - Code Llama Open Foundation Models for Code.pdf}
}

@misc{RunTextGeneration2022,
  title = {Run Text Generation with {{Bloom}} and {{GPT}} Models on {{Amazon SageMaker JumpStart}} {\textbar} {{AWS Machine Learning Blog}}},
  year = {2022},
  month = nov,
  url = {https://aws.amazon.com/blogs/machine-learning/run-text-generation-with-gpt-and-bloom-models-on-amazon-sagemaker-jumpstart/},
  urldate = {2023-10-31},
  chapter = {Amazon SageMaker},
  language = {en-US},
  file = {/home/carlos/Zotero/storage/FTBUAU2Q/run-text-generation-with-gpt-and-bloom-models-on-amazon-sagemaker-jumpstart.html}
}

@book{RussellStuartJ2021AI:A,
  title = {Artificial Intelligence : {{A}} Modern Approach},
  author = {Russell, Stuart J},
  year = {2021},
  edition = {4th Edition.; Global edition.},
  publisher = {{Pearson}},
  address = {{Harlow}},
  isbn = {978-0-13-461099-3},
  language = {eng},
  keywords = {Inteligencia artificial,Inteligencia artificial Robotica,Transferencia del conocimiento}
}

@misc{ruviaroGentleIntroductionSuperCollider2015,
  title = {A {{Gentle Introduction}} to {{SuperCollider}}},
  author = {Ruviaro, Bruno},
  year = {2015},
  url = {https://github.com/brunoruviaro/A_Gentle_Introduction_To_SuperCollider/blob/master/A_Gentle_Introduction_To_SuperCollider_BOOK_9x7_Landscape.tex},
  urldate = {2024-01-30},
  abstract = {A step-by-step tutorial for total beginners.},
  language = {en},
  file = {/home/carlos/Zotero/storage/CN3KBP2D/A_Gentle_Introduction_To_SuperCollider_BOOK_9x7_Landscape.html}
}

@article{schaefferAreEmergentAbilities2023,
  title = {Are Emergent Abilities of Large Language Models a Mirage?},
  author = {Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  year = {2023},
  journal = {arXiv preprint arXiv:2304.15004},
  eprint = {2304.15004},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/IKAF8WT5/Schaeffer et al. - 2023 - Are Emergent Abilities of Large Language Models a .pdf}
}

@misc{schmidtRecurrentNeuralNetworks2019,
  title = {Recurrent {{Neural Networks}} ({{RNNs}}): {{A}} Gentle {{Introduction}} and {{Overview}}},
  shorttitle = {Recurrent {{Neural Networks}} ({{RNNs}})},
  author = {Schmidt, Robin M.},
  year = {2019},
  month = nov,
  number = {arXiv:1912.05911},
  eprint = {1912.05911},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1912.05911},
  url = {http://arxiv.org/abs/1912.05911},
  abstract = {State-of-the-art solutions in the areas of "Language Modelling \& Generating Text", "Speech Recognition", "Generating Image Descriptions" or "Video Tagging" have been using Recurrent Neural Networks as the foundation for their approaches. Understanding the underlying concepts is therefore of tremendous importance if we want to keep up with recent or upcoming publications in those areas. In this work we give a short overview over some of the most important concepts in the realm of Recurrent Neural Networks which enables readers to easily understand the fundamentals such as but not limited to "Backpropagation through Time" or "Long Short-Term Memory Units" as well as some of the more recent advances like the "Attention Mechanism" or "Pointer Networks". We also give recommendations for further reading regarding more complex topics where it is necessary.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/carlos/Zotero/storage/A4V23CJF/Schmidt - 2019 - Recurrent Neural Networks (RNNs) A gentle Introduction and Overview.pdf;/home/carlos/Zotero/storage/L4ULHTME/1912.html}
}

@book{searleMentesCerebrosCiencia1985,
  title = {{Mentes, cerebros y ciencia}},
  author = {Searle, John R.},
  year = {1985},
  month = dec,
  publisher = {{C{\'a}tedra}},
  abstract = {Mientras que el sentido comun nos presenta como seres racionales, conscientes y libres, la ciencia nos informa que el universo en el que operamos, y que constituimos, es un agregado de particulas inconscienles. Que sabemos del cerebro, punto de union entre nuestra mente y el mundo fisico? Muy poco. En los ultimos tiempos se abre paso una atrevida analogia que relaciona el cerebro con un ordenador digital. Algunos cientificos postulan que las maquinas pueden --o podran -- pensar. Resolvera la inteligencia Artificial los problemas planteados por la relacion mente/cerebro?},
  googlebooks = {jfFUVl03PYUC},
  isbn = {978-84-376-0569-2},
  language = {es}
}

@article{selAlgorithmThoughtsEnhancing2023,
  title = {Algorithm of Thoughts: {{Enhancing}} Exploration of Ideas in Large Language Models},
  author = {Sel, Bilgehan and {Al-Tawaha}, Ahmad and Khattar, Vanshaj and Wang, Lu and Jia, Ruoxi and Jin, Ming},
  year = {2023},
  journal = {arXiv preprint arXiv:2308.10379},
  eprint = {2308.10379},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/RUC6VMXG/Sel et al. - 2023 - Algorithm of Thoughts Enhancing Exploration of Id.pdf}
}

@article{seydeBangBangControlAll2021,
  title = {Is Bang-Bang Control All You Need? Solving Continuous Control with Bernoulli Policies},
  author = {Seyde, Tim and Gilitschenski, Igor and Schwarting, Wilko and Stellato, Bartolomeo and Riedmiller, Martin and Wulfmeier, Markus and Rus, Daniela},
  year = {2021},
  journal = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {27209--27221},
  file = {/home/carlos/Zotero/storage/CKZ7I9RM/Seyde et al. - 2021 - Is Bang-Bang Control All You Need Solving Continuous Control with Bernoulli Policies.pdf}
}

@article{shannon1951prediction,
  title = {Prediction and Entropy of Printed English},
  author = {Shannon, Claude Elwood},
  year = {1951},
  month = jan,
  journal = {Bell System Technical Journal},
  volume = {30},
  pages = {50--64},
  url = {http://languagelog.ldc.upenn.edu/myl/Shannon1950.pdf},
  added-at = {2013-02-27T08:26:04.000+0100},
  interhash = {daabc21c7f6e71f6e78a10c8d3492927},
  intrahash = {2e79cf0f6022645a632b13e081b0b035},
  keywords = {communication english entropy information prediction shannon toread},
  timestamp = {2014-07-28T15:57:31.000+0200}
}

@article{shevlaneModelEvaluationExtreme2023,
  title = {Model Evaluation for Extreme Risks},
  author = {Shevlane, Toby and Farquhar, Sebastian and Garfinkel, Ben and Phuong, Mary and Whittlestone, Jess and Leung, Jade and Kokotajlo, Daniel and Marchal, Nahema and Anderljung, Markus and Kolt, Noam and others},
  year = {2023},
  journal = {arXiv preprint arXiv:2305.15324},
  eprint = {2305.15324},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/BDHEV472/Shevlane et al. - 2023 - Model evaluation for extreme risks.pdf}
}

@misc{SonicPiLive,
  title = {Sonic {{Pi}} - {{The Live Coding Music Synth}} for {{Everyone}}},
  url = {https://sonic-pi.net/},
  urldate = {2024-01-28},
  file = {/home/carlos/Zotero/storage/RBEGXTK5/sonic-pi.net.html}
}

@misc{StableAudioFast,
  title = {Stable {{Audio}}: {{Fast Timing-Conditioned Latent Audio Diffusion}}},
  shorttitle = {Stable {{Audio}}},
  journal = {Stability AI},
  url = {https://stability.ai/research/stable-audio-efficient-timing-latent-diffusion},
  urldate = {2023-09-27},
  abstract = {Stable Audio represents the cutting-edge audio generation research by Stability AI's generative audio research lab, Harmonai. We continue to improve our model architectures, datasets, and training procedures to improve output quality, controllability, inference speed, and output length.},
  language = {en-GB},
  file = {/home/carlos/Zotero/storage/HE8PHSK2/stable-audio-efficient-timing-latent-diffusion.html}
}

@misc{stokesGuideLanguageModel2023,
  title = {A Guide to Language Model Sampling in {{AllenNLP}}},
  author = {Stokes, Jackson},
  year = {2023},
  month = sep,
  journal = {Medium},
  url = {https://blog.allenai.org/a-guide-to-language-model-sampling-in-allennlp-3b1239274bc3},
  urldate = {2024-01-30},
  abstract = {How randomness can be used to make language models more creative},
  language = {en},
  file = {/home/carlos/Zotero/storage/BGKPIPNS/a-guide-to-language-model-sampling-in-allennlp-3b1239274bc3.html}
}

@misc{StrudelREPL,
  title = {Strudel {{REPL}}},
  url = {https://strudel.cc/},
  urldate = {2023-12-09},
  file = {/home/carlos/Zotero/storage/6HPXIMRG/strudel.cc.html}
}

@misc{SunoAI,
  title = {Suno {{AI}}},
  url = {https://www.suno.ai/},
  urldate = {2023-12-26},
  abstract = {We are building a future where anyone can make great music. No instrument needed, just imagination. From your mind to music.},
  language = {en},
  file = {/home/carlos/Zotero/storage/6922LJJL/www.suno.ai.html}
}

@misc{SuperCollider12Help,
  title = {{{SuperCollider}} 3.12.2 {{Help}}},
  url = {https://doc.sccode.org/Help.html},
  urldate = {2024-01-30},
  file = {/home/carlos/Zotero/storage/98KDEH49/Help.html}
}

@misc{SuperCollider2024,
  title = {{{SuperCollider}}},
  year = {2024},
  month = jan,
  url = {https://github.com/supercollider/supercollider},
  urldate = {2024-01-28},
  abstract = {An audio server, programming language, and IDE for sound synthesis and algorithmic composition.},
  copyright = {GPL-3.0},
  howpublished = {SuperCollider},
  keywords = {algorithmic-composition,audio,c-plus-plus,computer-music,electronic-music,livecoding,music,programming-language,sclang,scsynth,sonification,sound,supercollider,synthesis}
}

@book{supperMusicaElectronicaMusica2012,
  title = {M{\'u}sica Electr{\'o}nica y M{\'u}sica Con Ordenador. {{Historia}}, Est{\'e}tica, M{\'e}todos, Sistemas},
  author = {Supper, Martin},
  year = {2012},
  publisher = {{Alianza M{\'u}sica}},
  address = {{Madrid}}
}

@misc{SyntheticDataGeneration,
  title = {Synthetic {{Data Generation}} with {{Large Language Models}} for {{Text Classification}}: {{Potential}} and {{Limitations}}},
  shorttitle = {Papers with {{Code}} - {{Synthetic Data Generation}} with {{Large Language Models}} for {{Text Classification}}},
  url = {https://paperswithcode.com/paper/synthetic-data-generation-with-large-language},
  urldate = {2024-01-28},
  abstract = {No code available yet.},
  language = {en},
  file = {/home/carlos/Zotero/storage/BEFLSYPQ/synthetic-data-generation-with-large-language.html}
}

@misc{teamChucKStronglyTimedMusic,
  title = {{{ChucK}}: {{A Strongly-Timed Music Programming Language}}},
  shorttitle = {{{ChucK}}},
  author = {Team, ChucK},
  url = {https://ccrma.stanford.edu/software/chuck/},
  urldate = {2023-12-09},
  abstract = {ChucK is a strongly-timed programming language for interactive sound synthesis and music creation.},
  language = {en},
  file = {/home/carlos/Zotero/storage/Q7Y4LBTF/chuck.stanford.edu.html}
}

@misc{TestTuring,
  title = {El {{Test}} de {{Turing}}},
  url = {https://edea.juntadeandalucia.es/bancorecursos/file/d1bb5f09-682f-4e93-a799-e90b1c3364ed/2/COM_3ESO_REA_01_V02.zip/411_el_test_de_turing.html},
  urldate = {2024-01-27},
  file = {/home/carlos/Zotero/storage/D9EYC4EE/411_el_test_de_turing.html}
}

@misc{TextGenerationStrategies,
  title = {Text Generation Strategies},
  url = {https://huggingface.co/docs/transformers/main/en/generation_strategies},
  urldate = {2023-10-31},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {/home/carlos/Zotero/storage/4ENA6UIN/generation_strategies.html}
}

@article{tianFinetuningLanguageModels2023,
  title = {Fine-Tuning Language Models for Factuality},
  author = {Tian, Katherine and Mitchell, Eric and Yao, Huaxiu and Manning, Christopher D and Finn, Chelsea},
  year = {2023},
  journal = {arXiv preprint arXiv:2311.08401},
  eprint = {2311.08401},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/G7PVTXL8/Tian et al. - 2023 - Fine-tuning Language Models for Factuality.pdf}
}

@misc{TidalCycles,
  title = {Tidal {{Cycles}}},
  url = {https://tidalcycles.org/},
  urldate = {2023-12-09},
  abstract = {Live coding environment for making algorithmic patterns},
  language = {en},
  file = {/home/carlos/Zotero/storage/6QLETVM5/tidalcycles.org.html}
}

@misc{TopkToppTemperature,
  title = {{Top-k, Top-p, Temperature}},
  journal = {知乎专栏},
  url = {https://zhuanlan.zhihu.com/p/631786282},
  urldate = {2024-01-30},
  abstract = {前言上一篇文章介绍了几个开源LLM的环境搭建和本地部署，在使用ChatGPT接口或者自己本地部署的LLM大模型的时候，经常会遇到这几个参数，本文简单介绍一下{\textasciitilde} temperaturetop\_ptop\_k关于LLM上一篇也有介绍过，这次看{\dots}},
  language = {zh},
  file = {/home/carlos/Zotero/storage/M942PT2V/631786282.html}
}

@book{torresivinalsPythonDeepLearning2020,
  title = {Python {{Deep Learning}}. {{Introducci{\'o}n}} Pr{\'a}ctica Con {{Keras}} y {{TensorFlow}} 2},
  author = {{Torres i Vi{\~n}als}, Jordi},
  year = {2020},
  edition = {1ª},
  publisher = {{Marcombo}},
  isbn = {978-84-267-2921-7}
}

@article{touvronLLaMAOpenEfficient2023,
  title = {Llama: {{Open}} and Efficient Foundation Language Models},
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  year = {2023},
  journal = {arXiv preprint arXiv:2302.13971},
  eprint = {2302.13971},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/TSWEP6YH/Touvron et al. - 2023 - LLaMA Open and Efficient Foundation Language Models.pdf}
}

@misc{UnderstandingLearningDemonstrations,
  title = {An {{Understanding}} of {{Learning}} from {{Demonstrations}} for {{Neural Text Generation}} {$\cdot$} {{The ICLR Blog Track}}},
  url = {https://iclr-blog-track.github.io/2022/03/25/text-gen-via-lfd/},
  urldate = {2024-01-26},
  file = {/home/carlos/Zotero/storage/ZP65IJW5/text-gen-via-lfd.html}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is All You Need},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  file = {/home/carlos/Zotero/storage/ZUQMSYXF/Vaswani et al. - 2017 - Attention Is All You Need.pdf}
}

@article{wang2021promising,
  title = {A Promising and Challenging Approach: {{Radiologists}}' Perspective on Deep Learning and Artificial Intelligence for Fighting {{COVID-19}}},
  author = {Wang, Tianming and Chen, Zhu and Shang, Quanliang and Ma, Cong and Chen, Xiangyu and Xiao, Enhua},
  year = {2021},
  journal = {Diagnostics},
  volume = {11},
  number = {10},
  pages = {1924},
  publisher = {{Multidisciplinary Digital Publishing Institute}}
}

@inproceedings{wangAllYouNeed2019,
  title = {All You Need Is Boundary: {{Toward}} Arbitrary-Shaped Text Spotting},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Wang, Hao and Lu, Pu and Zhang, Hui and Yang, Mingkun and Bai, Xiang and Xu, Yongchao and He, Mengchao and Wang, Yongpan and Liu, Wenyu},
  year = {2020},
  volume = {34},
  pages = {12160--12167},
  file = {/home/carlos/Zotero/storage/7KT9R5CE/Wang et al. - 2019 - All You Need Is Boundary Toward Arbitrary-Shaped Text Spotting.pdf}
}

@inproceedings{wangCostEffectiveHyperparameterOptimization2023,
  title = {Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},
  booktitle = {International Conference on Automated Machine Learning},
  author = {Wang, Chi and Liu, Xueqing and Awadallah, Ahmed Hassan},
  year = {2023},
  pages = {21--1},
  publisher = {{PMLR}},
  file = {/home/carlos/Zotero/storage/25YJQ69L/Wang et al. - 2023 - Cost-Effective Hyperparameter Optimization for Lar.pdf}
}

@article{wangHyperparameterOptimizationAlgorithm2022,
  title = {A {{Hyperparameter Optimization Algorithm}} for the {{LSTM Temperature Prediction Model}} in {{Data Center}}},
  author = {Wang, Simin and Ma, Chunmiao and Xu, Yixuan and Wang, Jinyu and Wu, Weiguo},
  year = {2022},
  month = dec,
  journal = {Scientific Programming},
  volume = {2022},
  pages = {e6519909},
  publisher = {{Hindawi}},
  issn = {1058-9244},
  doi = {10.1155/2022/6519909},
  url = {https://www.hindawi.com/journals/sp/2022/6519909/},
  abstract = {As the main tool to realize data mining and efficient knowledge acquisition in the era of big data, machine learning is widely used in data center energy-saving research. The temperature prediction model based on machine learning predicts the state of the data center according to the upcoming tasks. It can adjust the refrigeration equipment in advance to avoid temperature regulation lag and set the air conditioning temperature according to the actual demand to avoid excessive refrigeration. Task scheduling and migration algorithm based on temperature prediction can effectively avoid hot spots. However, the choice of hyperparameter of machine learning model has a great impact on its performance. In this study, a hyperparameter optimization algorithm based on MLP is proposed. On the basis of trying certain hyperparameters, the MLP model is used to predict the value of all hyperparameters' space, and then, a certain number of high-quality hyperparameters are selected to train the model repeatedly. In each iteration, the amount of training data decreases gradually, while the accuracy of the model improves rapidly, and finally, the appropriate hyperparameter are obtained. We use the idea of mutation in the genetic algorithm to improve the probability of high-quality solutions and the loss function weighting method to select the solution with the best stability. Experiments are carried out on two representative machine learning models, LSTM and Random Forest, and compared with the standard Gaussian Bayes and Random Search method. The results show that the method proposed in this study can obtain high-precision and high-stability hyperparameter through one run and can greatly improve the operation efficiency. This algorithm is not only effective for LSTM but also suitable for other machine learning models.},
  language = {en},
  file = {/home/carlos/Zotero/storage/CSFU4NGN/Wang et al. - 2022 - A Hyperparameter Optimization Algorithm for the LS.pdf}
}

@misc{wangSelfConsistencyImprovesChain2023,
  title = {Self-{{Consistency Improves Chain}} of {{Thought Reasoning}} in {{Language Models}}},
  author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  year = {2023},
  month = mar,
  number = {arXiv:2203.11171},
  eprint = {2203.11171},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.11171},
  url = {http://arxiv.org/abs/2203.11171},
  abstract = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  note = {Comment: Published at ICLR 2023. V2: added PaLM results; V3: added UL2 results; V4: camera ready version at ICLR 2023},
  file = {/home/carlos/Zotero/storage/937N86FR/Wang et al. - 2023 - Self-Consistency Improves Chain of Thought Reasoning in Language Models.pdf;/home/carlos/Zotero/storage/DV8TJ9QK/2203.html}
}

@article{weiChainofThoughtPromptingElicits2023,
  title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  year = {2022},
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {24824--24837},
  file = {/home/carlos/Zotero/storage/AZPLR9LR/Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf}
}

@article{weiEmergentAbilitiesLarge2022,
  title = {Emergent Abilities of Large Language Models},
  author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  year = {2022},
  journal = {arXiv preprint arXiv:2206.07682},
  eprint = {2206.07682},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/C3F58HUQ/Wei et al. - 2022 - Emergent Abilities of Large Language Models.pdf}
}

@misc{WhatRetrievalAugmented,
  title = {{What is Retrieval Augmented Generation?}},
  url = {https://www.linkedin.com/pulse/what-retrieval-augmented-generation-grow-right},
  urldate = {2024-01-05},
  abstract = {Retrieval Augmented Generation (RAG) is an approach that combines the power of large-scale neural language models with external retrieval or search mechanisms. Here's a breakdown of RAG in a semi-formal, business tone: Core Concept: At the heart of RAG is the idea of leveraging external knowledge so},
  language = {es},
  file = {/home/carlos/Zotero/storage/A9NJFVUJ/what-retrieval-augmented-generation-grow-right.html}
}

@misc{WhatRetrievalaugmentedGeneration2021,
  title = {What Is Retrieval-Augmented Generation?},
  year = {2021},
  month = feb,
  journal = {IBM Research Blog},
  url = {https://research.ibm.com/blog/retrieval-augmented-generation-RAG},
  urldate = {2023-11-07},
  abstract = {RAG is an AI framework for retrieving facts to ground LLMs on the most accurate information and to give users insight into AI's decisionmaking process.},
  copyright = {{\copyright} Copyright IBM Corp. 2021},
  language = {en-US},
  file = {/home/carlos/Zotero/storage/S4AMA7JD/retrieval-augmented-generation-RAG.html}
}

@book{wilsonSuperColliderBook2011a,
  title = {The {{SuperCollider Book}}},
  author = {Wilson, Scott and Cottle, David and Collins, Nick},
  year = {2011},
  month = mar,
  publisher = {{The MIT Press}},
  abstract = {SuperCollider is one of the most important domain-specific audio programming languages, with potential applications that include real-time interaction, installations, electroacoustic pieces, generative music, and audiovisuals. The SuperCollider Book is the essential reference to this powerful and flexible language, offering students and professionals a collection of tutorials, essays, and projects. With contributions from top academics, artists, and technologists that cover topics at levels from the introductory to the specialized, it will be a valuable sourcebook both for beginners and for advanced users. SuperCollider, first developed by James McCartney, is an accessible blend of Smalltalk, C, and further ideas from a number of programming languages. Free, open-source, cross-platform, and with a diverse and supportive developer community, it is often the first programming language sound artists and computer musicians learn. The SuperCollider Book is the long-awaited guide to the design, syntax, and use of the SuperCollider language. The first chapters offer an introduction to the basics, including a friendly tutorial for absolute beginners, providing the reader with skills that can serve as a foundation for further learning. Later chapters cover more advanced topics and particular topics in computer music, including programming, sonification, spatialization, microsound, GUIs, machine listening, alternative tunings, and non-real-time synthesis; practical applications and philosophical insigh"s from the composer's and artist's perspectives; and "under the hood," developer's-eye views of SuperCollider's inner workings. A Web site accompanying the book offers code, links to the application itself and its source code, and a variety of third-party extras, extensions, libraries, and examples.},
  isbn = {978-0-262-23269-2}
}

@inproceedings{xieAllYouNeed2017,
  title = {All You Need Is beyond a Good Init: {{Exploring}} Better Solution for Training Extremely Deep Convolutional Neural Networks with Orthonormality and Modulation},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Xie, Di and Xiong, Jiang and Pu, Shiliang},
  year = {2017},
  pages = {6176--6185},
  file = {/home/carlos/Zotero/storage/AXI3SGH8/Xie et al. - 2017 - All You Need is Beyond a Good Init Exploring Better Solution for Training Extremely Deep Convolutio.pdf}
}

@article{yangLargeLanguageModels2023,
  title = {Large Language Models as Optimizers},
  author = {Yang, Chengrun and Wang, Xuezhi and Lu, Yifeng and Liu, Hanxiao and Le, Quoc V and Zhou, Denny and Chen, Xinyun},
  year = {2023},
  journal = {arXiv preprint arXiv:2309.03409},
  eprint = {2309.03409},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/SLLUN5RF/Yang et al. - 2023 - Large Language Models as Optimizers.pdf}
}

@article{zengMusicBERTSymbolicMusic2021,
  title = {Musicbert: {{Symbolic}} Music Understanding with Large-Scale Pre-Training},
  author = {Zeng, Mingliang and Tan, Xu and Wang, Rui and Ju, Zeqian and Qin, Tao and Liu, Tie-Yan},
  year = {2021},
  journal = {arXiv preprint arXiv:2106.05630},
  eprint = {2106.05630},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/JL4NSPDK/Zeng et al. - 2021 - MusicBERT Symbolic Music Understanding with Large.pdf}
}

@book{zhang2023dive,
  title = {Dive into Deep Learning},
  author = {Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
  year = {2023},
  publisher = {{Cambridge University Press}},
  note = {\href{https://D2L.ai}{https://D2L.ai}}
}

@article{zhouLeasttoMostPromptingEnables2023,
  title = {Least-to-Most Prompting Enables Complex Reasoning in Large Language Models},
  author = {Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and others},
  year = {2022},
  journal = {arXiv preprint arXiv:2205.10625},
  eprint = {2205.10625},
  archiveprefix = {arxiv},
  file = {/home/carlos/Zotero/storage/7MQPZXWI/Zhou et al. - 2023 - Least-to-Most Prompting Enables Complex Reasoning .pdf}
}

@article{zhuangComprehensiveSurveyTransfer2020,
  title = {A Comprehensive Survey on Transfer Learning},
  author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  year = {2020},
  journal = {Proceedings of the IEEE},
  volume = {109},
  number = {1},
  pages = {43--76},
  publisher = {{IEEE}},
  file = {/home/carlos/Zotero/storage/N58MIQLS/Zhuang et al. - 2020 - A Comprehensive Survey on Transfer Learning.pdf}
}

@misc{zraraPORTFOLIOOPTIMIZATIONUSING2020,
  title = {{{PORTFOLIO OPTIMIZATION USING DEEP LEARNING FOR THE MOROCCAN MARKET}}},
  author = {Zrara, Lamiaa},
  year = {2020},
  url = {http://www.aui.ma/sse-capstone-repository/pdf/fall-2020/PORTFOLIO%20OPTIMIZATION%20USING%20DEEP%20LEARNING%20FOR%20THE%20MOROCCAN%20MARKET.pdf},
  urldate = {2024-01-27}
}
