@article{abeliukHistoriaEvoluacionInteligencia2021,
  title = {{Historia y evoluaci{\'o}n de la inteligencia artificial}},
  author = {Abeliuk, Andr{\'e}s and Guti{\'e}rrez, Claudio},
  year = {2021},
  month = aug,
  journal = {Revista Bits de Ciencia},
  number = {21},
  pages = {14--21},
  urldate = {2023-10-30},
  copyright = {Derechos de autor 2021 Revista Bits de Ciencia},
  langid = {spanish},
  file = {/home/carlos/Zotero/storage/3XUJEG4L/Abeliuk y Gutiérrez - 2021 - Historia y evoluación de la inteligencia artificia.pdf}
}

@misc{AGIEdgerunnersLLMAgentsPapers2024,
  title = {{{AGI-Edgerunners}}/{{LLM-Agents-Papers}}},
  year = {2024},
  month = jan,
  urldate = {2024-01-02},
  abstract = {A repo lists papers related to LLM based agent},
  howpublished = {AGI-Edgerunners},
  keywords = {agents,large-language-models,llm-agent,paper-list}
}

@article{alan1950a,
  title = {Computing Machinery and Intelligence},
  author = {Turing, Alan Mathison},
  year = {1950},
  journal = {Mind; a quarterly review of psychology and philosophy},
  volume = {49},
  pages = {433--460}
}

@article{arizaTwoPioneeringProjects2011,
  title = {Two {{Pioneering Projects}} from the {{Early History}} of {{Computer-Aided Algorithmic Composition}}},
  author = {Ariza, Christopher},
  year = {2011},
  month = sep,
  journal = {MIT Press},
  publisher = {{MIT Press}},
  issn = {0148-9267},
  urldate = {2023-12-26},
  abstract = {Lejaren Hiller's 1970 chapter, "Music Composed  with Computers: An Historical Survey" (Hiller  1970) contains numerous descriptions of projects  in the computer generation of musical structures.  By then, just over ten years after the publication  of Hiller's and Leonard Isaacson's seminal book  Experimental Music (Hiller and Isaacson 1959), a  startling number of experiments in generativemusic  with early computers had been completed. Hiller's  early research, compositions, and publications  established him as a leader in the then-emerging  field of computer-aided algorithmic composition  (CAAC). Some researchers, likely inspired by Hiller  and Isaacson's 1956 Illiac Suite string quartet, even  duplicated their instrumentation: in an amusing  footnote, Hiller writes that "it is curious to note  how many computer pieces have been written for  string quartet . . . particularly since string-quartet  performers seem to be among the least receptive to  newer compositional ideas such as computermusic"  (Hiller 1970, p. 70).},
  copyright = {Article is made available in accordance with the publisher's policy and may be subject to US copyright law. Please refer to the publisher's site for terms of use.},
  langid = {american},
  annotation = {Accepted: 2012-01-20T20:32:35Z},
  file = {/home/carlos/Zotero/storage/MN9QB5LS/Ariza - 2011 - Two Pioneering Projects from the Early History of .pdf}
}

@misc{Audio,
  title = {Audio},
  journal = {Stability AI},
  urldate = {2023-12-26},
  howpublished = {https://stability.ai/stable-audio},
  langid = {british},
  file = {/home/carlos/Zotero/storage/2ILU7GRZ/stable-audio.html}
}

@misc{BeginnerGuideNeural,
  title = {A {{Beginner}}'s {{Guide}} to {{Neural Networks}} and {{Deep Learning}}},
  journal = {Pathmind},
  urldate = {2023-12-21},
  abstract = {An introduction to deep artificial neural networks and deep learning.},
  howpublished = {http://wiki.pathmind.com/neural-network},
  langid = {english},
  file = {/home/carlos/Zotero/storage/QUNDLL5X/a_beginners_guide_to_neural_networks_and_deep_learning___pathmind.pdf;/home/carlos/Zotero/storage/A63S2JGX/neural-network.html}
}

@misc{borsosAudioLMLanguageModeling2023,
  title = {{{AudioLM}}: A {{Language Modeling Approach}} to {{Audio Generation}}},
  shorttitle = {{{AudioLM}}},
  author = {Borsos, Zal{\'a}n and Marinier, Rapha{\"e}l and Vincent, Damien and Kharitonov, Eugene and Pietquin, Olivier and Sharifi, Matt and Roblek, Dominik and Teboul, Olivier and Grangier, David and Tagliasacchi, Marco and Zeghidour, Neil},
  year = {2023},
  month = jul,
  number = {arXiv:2209.03143},
  eprint = {2209.03143},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  urldate = {2024-01-04},
  abstract = {We introduce AudioLM, a framework for highquality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/carlos/Zotero/storage/QXGW8ZAJ/Borsos et al. - 2023 - AudioLM a Language Modeling Approach to Audio Generation.pdf}
}

@book{boulangerCsoundBookPerspectives2000,
  title = {The {{Csound Book}}. {{Perspectives}} in {{Software Synthesis}}, {{Sound Design}}, {{Signal Processing}}, and {{Programming}}},
  editor = {Boulanger, Richard},
  year = {2000},
  publisher = {{The MIT Press}},
  address = {{Massachusetts}}
}

@misc{bowmanEightThingsKnow2023,
  title = {Eight {{Things}} to {{Know}} about {{Large Language Models}}},
  author = {Bowman, Samuel R.},
  year = {2023},
  month = apr,
  number = {arXiv:2304.00612},
  eprint = {2304.00612},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.00612},
  urldate = {2023-10-23},
  abstract = {The widespread public deployment of large language models (LLMs) in recent months has prompted a wave of new attention and engagement from advocates, policymakers, and scholars from many fields. This attention is a timely response to the many urgent questions that this technology raises, but it can sometimes miss important considerations. This paper surveys the evidence for eight potentially surprising such points: 1. LLMs predictably get more capable with increasing investment, even without targeted innovation. 2. Many important LLM behaviors emerge unpredictably as a byproduct of increasing investment. 3. LLMs often appear to learn and use representations of the outside world. 4. There are no reliable techniques for steering the behavior of LLMs. 5. Experts are not yet able to interpret the inner workings of LLMs. 6. Human performance on a task isn't an upper bound on LLM performance. 7. LLMs need not express the values of their creators nor the values encoded in web text. 8. Brief interactions with LLMs are often misleading.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/RDHLL7B3/Bowman - 2023 - Eight Things to Know about Large Language Models.pdf;/home/carlos/Zotero/storage/HETIVTEB/2304.html}
}

@misc{bretanUnitSelectionMethodology2016,
  title = {A {{Unit Selection Methodology}} for {{Music Generation Using Deep Neural Networks}}},
  author = {Bretan, Mason and Weinberg, Gil and Heck, Larry},
  year = {2016},
  month = dec,
  number = {arXiv:1612.03789},
  eprint = {1612.03789},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1612.03789},
  urldate = {2023-05-24},
  abstract = {Several methods exist for a computer to generate music based on data including Markov chains, recurrent neural networks, recombinancy, and grammars. We explore the use of unit selection and concatenation as a means of generating music using a procedure based on ranking, where, we consider a unit to be a variable length number of measures of music. We first examine whether a unit selection method, that is restricted to a finite size unit library, can be sufficient for encompassing a wide spectrum of music. We do this by developing a deep autoencoder that encodes a musical input and reconstructs the input by selecting from the library. We then describe a generative model that combines a deep structured semantic model (DSSM) with an LSTM to predict the next unit, where units consist of four, two, and one measures of music. We evaluate the generative model using objective metrics including mean rank and accuracy and with a subjective listening test in which expert musicians are asked to complete a forced-choiced ranking task. We compare our model to a note-level generative baseline that consists of a stacked LSTM trained to predict forward by one note.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Sound},
  file = {/home/carlos/Zotero/storage/EHW5C3CU/Bretan et al. - 2016 - A Unit Selection Methodology for Music Generation .pdf;/home/carlos/Zotero/storage/E2D7NJMW/1612.html}
}

@misc{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2005.14165},
  urldate = {2023-06-29},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/4ZY5M7ZW/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf;/home/carlos/Zotero/storage/PSWS6S3E/2005.html}
}

@misc{butlinConsciousnessArtificialIntelligence2023,
  title = {Consciousness in {{Artificial Intelligence}}: {{Insights}} from the {{Science}} of {{Consciousness}}},
  shorttitle = {Consciousness in {{Artificial Intelligence}}},
  author = {Butlin, Patrick and Long, Robert and Elmoznino, Eric and Bengio, Yoshua and Birch, Jonathan and Constant, Axel and Deane, George and Fleming, Stephen M. and Frith, Chris and Ji, Xu and Kanai, Ryota and Klein, Colin and Lindsay, Grace and Michel, Matthias and Mudrik, Liad and Peters, Megan A. K. and Schwitzgebel, Eric and Simon, Jonathan and VanRullen, Rufin},
  year = {2023},
  month = aug,
  number = {arXiv:2308.08708},
  eprint = {2308.08708},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.08708},
  urldate = {2023-09-27},
  abstract = {Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive "indicator properties" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also suggests that there are no obvious technical barriers to building AI systems which satisfy these indicators.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  file = {/home/carlos/Zotero/storage/3YM2L8NV/Butlin et al. - 2023 - Consciousness in Artificial Intelligence Insights.pdf}
}

@article{campbellDeepBlue2002,
  title = {Deep {{Blue}}},
  author = {Campbell, Murray and Hoane, A. Joseph and Hsu, Feng-hsiung},
  year = {2002},
  month = jan,
  journal = {Artificial Intelligence},
  volume = {134},
  number = {1},
  pages = {57--83},
  issn = {0004-3702},
  doi = {10.1016/S0004-3702(01)00129-1},
  urldate = {2023-10-29},
  abstract = {Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including: {\textbullet}a single-chip chess search engine,{\textbullet}a massively parallel system with multiple levels of parallelism,{\textbullet}a strong emphasis on search extensions,{\textbullet}a complex evaluation function, and{\textbullet}effective use of a Grandmaster game database. This paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue.},
  keywords = {Computer chess,Evaluation function,Game tree search,Parallel search,Search extensions,Selective search},
  file = {/home/carlos/Zotero/storage/IAV98XWL/Campbell et al. - 2002 - Deep Blue.pdf;/home/carlos/Zotero/storage/ARPI4JK5/S0004370201001291.html}
}

@misc{ChallengesAssociatedBuilding,
  title = {{The Challenges Associated With Building Products Using Large Language Models (LLMs).}},
  urldate = {2023-10-31},
  abstract = {Hello there. I hope you all are doing well.},
  howpublished = {https://www.linkedin.com/pulse/challenges-associated-building-products-using-large-language-das},
  langid = {spanish},
  file = {/home/carlos/Zotero/storage/A78XI6JS/challenges-associated-building-products-using-large-language-das.html}
}

@misc{chamandFinetuneYourClassifier2022,
  title = {Fine-Tune Your {{Classifier}}: {{Finding Correlations With Temperature}}},
  shorttitle = {Fine-Tune Your {{Classifier}}},
  author = {Chamand, Benjamin and {Risser-Maroix}, Olivier and Kurtz, Camille and Joly, Philippe and Lom{\'e}nie, Nicolas},
  year = {2022},
  month = oct,
  number = {arXiv:2210.09715},
  eprint = {2210.09715},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.09715},
  urldate = {2023-11-07},
  abstract = {Temperature is a widely used hyperparameter in various tasks involving neural networks, such as classification or metric learning, whose choice can have a direct impact on the model performance. Most of existing works select its value using hyperparameter optimization methods requiring several runs to find the optimal value. We propose to analyze the impact of temperature on classification tasks by describing a dataset as a set of statistics computed on representations on which we can build a heuristic giving us a default value of temperature. We study the correlation between these extracted statistics and the observed optimal temperatures. This preliminary study on more than a hundred combinations of different datasets and features extractors highlights promising results towards the construction of a general heuristic for temperature.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/carlos/Zotero/storage/EVJ4JQ22/Chamand et al. - 2022 - Fine-tune your Classifier Finding Correlations Wi.pdf;/home/carlos/Zotero/storage/7ZL2LZHW/2210.html}
}

@misc{chenTeachingLargeLanguage2023,
  title = {Teaching {{Large Language Models}} to {{Self-Debug}}},
  author = {Chen, Xinyun and Lin, Maxwell and Sch{\"a}rli, Nathanael and Zhou, Denny},
  year = {2023},
  month = oct,
  number = {arXiv:2304.05128},
  eprint = {2304.05128},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.05128},
  urldate = {2023-10-23},
  abstract = {Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3\%, and improves the prediction accuracy on problems of the hardest level by 9\%. On TransCoder and MBPP where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12\%. Meanwhile, by leveraging feedback messages and reusing failed predictions, Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/7APGY9B9/Chen et al. - 2023 - Teaching Large Language Models to Self-Debug.pdf;/home/carlos/Zotero/storage/B54ZXYI4/2304.html}
}

@misc{CodeLlamaOpen,
  title = {Code {{Llama}}: {{Open Foundation Models}} for {{Code}} | {{Research}} - {{AI}} at {{Meta}}},
  urldate = {2023-09-27},
  howpublished = {https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/},
  file = {/home/carlos/Zotero/storage/RIT3IM2P/code-llama-open-foundation-models-for-code.html}
}

@inproceedings{crookConversationalSemanticSearch2018,
  title = {Conversational {{Semantic Search}}: {{Looking BeyondWeb Search}}, {{Q}}\&{{A}} and {{Dialog Systems}}},
  shorttitle = {Conversational {{Semantic Search}}},
  booktitle = {The 11th {{ACM International Conference}} on {{Web Search}} and {{Data Minining}} ({{WSDM}} 2018)},
  author = {Crook, Paul A. and Marin, Alex and Agarwal, Vipul and Anderson, Samantha and Jang, Ohyoung and Lanewala, Aliasgar and Tangirala, Karthik and Zitouni, Imed},
  year = {2018},
  month = feb,
  urldate = {2023-07-06},
  abstract = {User expectations of web search are changing. They are expecting search engines to answer questions, to be more conversational, and to offer means to complete tasks on their behalf. At the same time, to increase the breadth of tasks that personal digital assistants (PDAs), such as Microsoft's Cortana or Amazon's Alexa, are capable of, PDAs [{\ldots}]},
  langid = {american},
  file = {/home/carlos/Zotero/storage/XS5AUBND/Crook et al. - 2018 - Conversational Semantic Search Looking BeyondWeb .pdf}
}

@misc{DeepLearningDL,
  title = {Deep {{Learning}} ({{DL}})},
  journal = {Questions and Answers \hspace{0pt}in MRI},
  urldate = {2023-12-21},
  abstract = {Deep Learning (DL)},
  howpublished = {http://mriquestions.com/what-is-a-neural-network.html},
  langid = {english},
  file = {/home/carlos/Zotero/storage/H68M27JI/what-is-a-neural-network.html}
}

@article{departmentofcomputersciencesrminstituteofscienceandtechnologychennaiindia.MusenetMusicGeneration2020a,
  title = {Musenet : {{Music Generation}} Using {{Abstractive}} and {{Generative Methods}}},
  shorttitle = {Musenet},
  author = {{Department of Computer Science, SRM Institute of Science and Technology, Chennai, India.} and Pal*, Abhilash and Saha, Sourav and {Department of Computer Science, SRM Institute of Science and Technology, Chennai, India.} and {Anita} and {Department of Computer Science, SRM Institute of Science and Technology, Chennai, India.}},
  year = {2020},
  month = apr,
  journal = {International Journal of Innovative Technology and Exploring Engineering},
  volume = {9},
  number = {6},
  pages = {784--788},
  issn = {22783075},
  doi = {10.35940/ijitee.F3580.049620},
  urldate = {2023-12-26},
  abstract = {Humans have been entertained by music for millennia. For ages it has been treated as an art form which requires a lot of imagination, creativity and accumulation of feelings and emotions. Recent trends in the field of Artificial Intelligence have been getting traction and Researchers have been developing and generating rudimentary forms of music through the use of AI. Our goal is to generate novel music, which will be non-repetitive and enjoyable. We aim to utilize a couple of Machine Learning models for the same. Given a seed bar of music, our first Discriminatory network consisting of Support Vector Machines and Neural Nets will choose a note/chord to direct the next bar. Based on this chord or note another network, a Generative Net consisting of Generative Pretrained Transformers(GPT2) and LSTMs will generate the entire bar of music. Our two fold method is novel and our aim is to make the generation method as similar to music composition in reality as possible. This in turn results in better concordant music. Machine generated music will be copyright free and can be generated conditioned on a few parameters for a given use.The paper presents several use cases and while the utilization will be for a niche audience, if a easy to use application can be built, almost anyone will be able to use deep learning to generate concordant music based on their needs.},
  file = {/home/carlos/Zotero/storage/MUPRNIQZ/Department of Computer Science, SRM Institute of Science and Technology, Chennai, India. et al. - 2020 - Musenet  Music Generation using Abstractive and G.pdf}
}

@misc{dhariwalJukeboxGenerativeModel2020,
  title = {Jukebox: {{A Generative Model}} for {{Music}}},
  shorttitle = {Jukebox},
  author = {Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
  year = {2020},
  month = apr,
  number = {arXiv:2005.00341},
  eprint = {2005.00341},
  primaryclass = {cs, eess, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2005.00341},
  urldate = {2023-12-26},
  abstract = {We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples at https://jukebox.openai.com, along with model weights and code at https://github.com/openai/jukebox},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  file = {/home/carlos/Zotero/storage/B294RLLC/Dhariwal et al. - 2020 - Jukebox A Generative Model for Music.pdf;/home/carlos/Zotero/storage/UKDJBJNM/2005.html}
}

@misc{dhuliawalaChainofVerificationReducesHallucination2023,
  title = {Chain-of-{{Verification Reduces Hallucination}} in {{Large Language Models}}},
  author = {Dhuliawala, Shehzaad and Komeili, Mojtaba and Xu, Jing and Raileanu, Roberta and Li, Xian and Celikyilmaz, Asli and Weston, Jason},
  year = {2023},
  month = sep,
  number = {arXiv:2309.11495},
  eprint = {2309.11495},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.11495},
  urldate = {2023-10-30},
  abstract = {Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/JLP8R4IU/Dhuliawala et al. - 2023 - Chain-of-Verification Reduces Hallucination in Lar.pdf;/home/carlos/Zotero/storage/5L75C6YB/2309.html}
}

@misc{douglasLargeLanguageModels2023,
  title = {Large {{Language Models}}},
  author = {Douglas, Michael R.},
  year = {2023},
  month = jul,
  number = {arXiv:2307.05782},
  eprint = {2307.05782},
  primaryclass = {hep-th, physics:physics},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.05782},
  urldate = {2023-09-27},
  abstract = {Artificial intelligence is making spectacular progress, and one of the best examples is the development of large language models (LLMs) such as OpenAI's GPT series. In these lectures, written for readers with a background in mathematics or physics, we give a brief history and survey of the state of the art, and describe the underlying transformer architecture in detail. We then explore some current ideas on how LLMs work and how models trained to predict the next word in a text are able to perform other tasks displaying intelligence.},
  archiveprefix = {arxiv},
  keywords = {68T01,Computer Science - Computation and Language,High Energy Physics - Theory,I.2.7,Mathematics - History and Overview,Physics - Computational Physics},
  file = {/home/carlos/Zotero/storage/MTB9SGS5/Douglas - 2023 - Large Language Models.pdf;/home/carlos/Zotero/storage/Y46IYSSF/2307.html}
}

@article{duAddressingSyntaxBasedSemantic2022,
  title = {Addressing {{Syntax-Based Semantic Complementation}}: {{Incorporating Entity}} and {{Soft Dependency Constraints}} into {{Metonymy Resolution}}},
  shorttitle = {Addressing {{Syntax-Based Semantic Complementation}}},
  author = {Du, Siyuan and Wang, Hao},
  year = {2022},
  month = mar,
  journal = {Future Internet},
  volume = {14},
  number = {3},
  pages = {85},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1999-5903},
  doi = {10.3390/fi14030085},
  urldate = {2023-12-25},
  abstract = {State-of-the-art methods for metonymy resolution (MR) consider the sentential context by modeling the entire sentence. However, entity representation, or syntactic structure that are informative may be beneficial for identifying metonymy. Other approaches only using deep neural network fail to capture such information. To leverage both entity and syntax constraints, this paper proposes a robust model EBAGCN for metonymy resolution. First, this work extracts syntactic dependency relations under the guidance of syntactic knowledge. Then the work constructs a neural network to incorporate both entity representation and syntactic structure into better resolution representations. In this way, the proposed model alleviates the impact of noisy information from entire sentences and breaks the limit of performance on the complicated texts. Experiments on the SemEval and ReLocaR dataset show that the proposed model significantly outperforms the state-of-the-art method BERT by more than 4\%. Ablation tests demonstrate that leveraging these two types of constraints benefits fine pre-trained language models in the MR task.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {dependency integration,entity representation,metonymy resolution},
  file = {/home/carlos/Zotero/storage/AP3AK6JY/Du y Wang - 2022 - Addressing Syntax-Based Semantic Complementation .pdf}
}

@misc{FindWaysDeal,
  title = {{Find ways to deal with the scarcity of labeled data}},
  urldate = {2023-12-21},
  abstract = {Compared to other available choices such as unsupervised and deep learning algorithms: the whole learning process of supervised machine learning (ML) based algorithms is much simpler; the training \& deployment costs are considerably low too; and most importantly supervised models work perfectly for},
  howpublished = {https://www.linkedin.com/pulse/find-ways-deal-scarcity-labeled-data-amit-asawa},
  langid = {spanish},
  file = {/home/carlos/Zotero/storage/K3EN3SDY/find-ways-deal-scarcity-labeled-data-amit-asawa.html}
}

@misc{frackiewiczEvolucionIAComposicion2023,
  title = {{La evoluci{\'o}n de la IA en la composici{\'o}n musical}},
  author = {Fr{\k{a}}ckiewicz, Marcin},
  year = {2023},
  month = jul,
  journal = {TS2 SPACE},
  urldate = {2023-11-03},
  abstract = {La evoluci{\'o}n de la IA en la composici{\'o}n musical TS2 SPACE},
  langid = {spanish},
  file = {/home/carlos/Zotero/storage/Z76TKKBM/la-evolucion-de-la-ia-en-la-composicion-musical.html}
}

@article{funkMusicalSuiteComposed2018a,
  title = {A {{Musical Suite Composed}} by an {{Electronic Brain}}: {{Reexamining}} the {{Illiac Suite}} and the {{Legacy}} of {{Lejaren A}}. {{Hiller Jr}}.},
  shorttitle = {A {{Musical Suite Composed}} by an {{Electronic Brain}}},
  author = {Funk, Tiffany},
  year = {2018},
  journal = {Leonardo Music Journal},
  volume = {28},
  pages = {19--24},
  publisher = {{MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info {\ldots}}},
  urldate = {2023-12-28},
  file = {/home/carlos/Zotero/storage/JSP23EKX/Funk - 2018 - A Musical Suite Composed by an Electronic Brain R.pdf}
}

@inproceedings{ganguliPredictabilitySurpriseLarge2022,
  title = {Predictability and {{Surprise}} in {{Large Generative Models}}},
  booktitle = {2022 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Ganguli, Deep and Hernandez, Danny and Lovitt, Liane and DasSarma, Nova and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Kernion, Jackson and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Elhage, Nelson and Showk, Sheer El and Fort, Stanislav and {Hatfield-Dodds}, Zac and Johnston, Scott and Kravec, Shauna and Nanda, Neel and Ndousse, Kamal and Olsson, Catherine and Amodei, Daniela and Amodei, Dario and Brown, Tom and Kaplan, Jared and McCandlish, Sam and Olah, Chris and Clark, Jack},
  year = {2022},
  month = jun,
  eprint = {2202.07785},
  primaryclass = {cs},
  pages = {1747--1764},
  doi = {10.1145/3531146.3533229},
  urldate = {2023-06-27},
  abstract = {Large-scale pre-training has recently emerged as a technique for creating capable, general purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have an unusual combination of predictable loss on a broad training distribution (as embodied in their "scaling laws"), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, and academics who want to analyze, critique, and potentially develop large generative models.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computers and Society},
  file = {/home/carlos/Zotero/storage/93WQ2BRW/Ganguli et al. - 2022 - Predictability and Surprise in Large Generative Mo.pdf}
}

@misc{GenerationLLMs,
  title = {Generation with {{LLMs}}},
  urldate = {2023-10-31},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/docs/transformers/main/en/llm\_tutorial},
  file = {/home/carlos/Zotero/storage/Q83ZWSQC/llm_tutorial.html}
}

@misc{gonzaloAsomandonosVentanaContextual2023,
  type = {{Billet}},
  title = {{Asom{\'a}ndonos a la ventana contextual de la Inteligencia Artificial: dec{\'a}logo de ayuda para la identificaci{\'o}n del uso de ChatGPT en textos acad{\'e}micos}},
  shorttitle = {{Asom{\'a}ndonos a la ventana contextual de la Inteligencia Artificial}},
  author = {Gonzalo, Jover and Carabantes, David and Gonz{\'a}lez Geraldo, Jos{\'e} L.},
  year = {2023},
  month = jun,
  journal = {Aula Magna 2.0},
  urldate = {2023-11-08},
  abstract = {Por Gonzalo Jover*, David Carabantes* y Jos{\'e} L. Gonz{\'a}lez Geraldo** *Universidad Complutense de Madrid **Universidad de Castilla La Mancha Palabras clave:~ChatGPT, OpenAI, Inteligencia Artificial ~ Que la Inteligencia Artificial (IA) ha contaminado cada rinc{\'o}n de nuestra sociedad a un ritmo tan espeluznante como atractivo y peligroso es m{\'a}s que evidente a {\ldots} Continuar leyendo "Asom{\'a}ndonos a la ventana contextual de la Inteligencia Artificial: dec{\'a}logo de ayuda para la identificaci{\'o}n del uso de ChatGPT en textos acad{\'e}micos"},
  langid = {spanish},
  file = {/home/carlos/Zotero/storage/FZ5IHE5I/13299.html}
}

@book{Goodfellow-et-al-2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {{MIT Press}}
}

@misc{GPT3RiseFoundation,
  title = {{GPT-3 and the rise of foundation models}},
  urldate = {2023-12-10},
  abstract = {GPT-3 (Generative Pre-Trained Transformer 3) is a large language model with 175 billion parameters, trained using the Common Crawl internet dataset, Wikipedia, and several large digital document collections. Its transformer-based algorithm has demonstrated superior performance in text generation, co},
  howpublished = {https://www.linkedin.com/pulse/gpt-3-rise-foundation-models-joseph-boland},
  langid = {spanish},
  file = {/home/carlos/Zotero/storage/9E6YS6FB/gpt-3-rise-foundation-models-joseph-boland.html}
}

@phdthesis{guerraparraMesjetiuTFM_Arte_Sonoro_MEMORIA2020,
  title = {Mesjetiu/{{TFM}}\_{{Arte}}\_{{Sonoro}}\_{{MEMORIA}}},
  author = {Guerra Parra, Carlos Arturo},
  year = {2020},
  urldate = {2023-12-09},
  school = {Universidad de Barcelona},
  file = {/home/carlos/Zotero/storage/TRE2DQ8J/TFM_Arte_Sonoro_MEMORIA.html}
}

@misc{gunasekarTextbooksAreAll2023,
  title = {Textbooks {{Are All You Need}}},
  author = {Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and {de Rosa}, Gustavo and Saarikivi, Olli and Salim, Adil and Shah, Shital and Behl, Harkirat Singh and Wang, Xin and Bubeck, S{\'e}bastien and Eldan, Ronen and Kalai, Adam Tauman and Lee, Yin Tat and Li, Yuanzhi},
  year = {2023},
  month = jun,
  number = {arXiv:2306.11644},
  eprint = {2306.11644},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.11644},
  urldate = {2023-06-29},
  abstract = {We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6\% on HumanEval and 55.5\% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45\% on HumanEval.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/carlos/Zotero/storage/IMBD7FR8/Gunasekar et al. - 2023 - Textbooks Are All You Need.pdf;/home/carlos/Zotero/storage/PP9DGAW6/2306.html}
}

@misc{hernandez-olivanSurveyArtificialIntelligence2022,
  title = {A {{Survey}} on {{Artificial Intelligence}} for {{Music Generation}}: {{Agents}}, {{Domains}} and {{Perspectives}}},
  shorttitle = {A {{Survey}} on {{Artificial Intelligence}} for {{Music Generation}}},
  author = {{Hernandez-Olivan}, Carlos and {Hernandez-Olivan}, Javier and Beltran, Jose R.},
  year = {2022},
  month = nov,
  number = {arXiv:2210.13944},
  eprint = {2210.13944},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.13944},
  urldate = {2023-09-27},
  abstract = {Music is one of the Gardner's intelligences in his theory of multiple intelligences. How humans perceive and understand music is still being studied and is crucial to develop artificial intelligence models that imitate such processes. Music generation with Artificial Intelligence is an emerging field that is gaining much attention in the recent years. In this paper, we describe how humans compose music and how new AI systems could imitate such process by comparing past and recent advances in the field with music composition techniques. To understand how AI models and algorithms generate music and the potential applications that might appear in the future, we explore, analyze and describe the agents that take part of the music generation process: the datasets, models, interfaces, the users and the generated music. We mention possible applications that might benefit from this field and we also propose new trends and future research directions that could be explored in the future.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/carlos/Zotero/storage/498MZNDW/Hernandez-Olivan et al. - 2022 - A Survey on Artificial Intelligence for Music Gene.pdf;/home/carlos/Zotero/storage/AMJVURKG/2210.html}
}

@misc{HistoriaInteligenciaArtificial,
  title = {{Historia de la Inteligencia Artificial, el Machine Learning y el Deep Learning}},
  urldate = {2023-10-30},
  abstract = {Conoce toda la historia de la Inteligencia Artificial (IA), el Machine Learning (ML) y el Deep Learning (DL), de manera breve, sencilla e ilustrativa.},
  howpublished = {https://www.algotive.ai/es-mx/blog/historia-de-la-inteligencia-artificial-el-machine-learning-y-el-deep-learning},
  langid = {mexican},
  file = {/home/carlos/Zotero/storage/LU4PW86U/historia-de-la-inteligencia-artificial-el-machine-learning-y-el-deep-learning.html}
}

@misc{holtzmanCuriousCaseNeural2020,
  title = {The {{Curious Case}} of {{Neural Text Degeneration}}},
  author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  year = {2020},
  month = feb,
  number = {arXiv:1904.09751},
  eprint = {1904.09751},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1904.09751},
  urldate = {2023-11-07},
  abstract = {Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/J4H86DF3/Holtzman et al. - 2020 - The Curious Case of Neural Text Degeneration.pdf;/home/carlos/Zotero/storage/6Z9WQDE7/1904.html}
}

@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = {1989},
  month = jan,
  journal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(89)90020-8},
  urldate = {2023-12-21},
  abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
  keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
  file = {/home/carlos/Zotero/storage/DIC8KESB/0893608089900208.html}
}

@misc{huangAgentCoderMultiAgentbasedCode2023,
  title = {{{AgentCoder}}: {{Multi-Agent-based Code Generation}} with {{Iterative Testing}} and {{Optimisation}}},
  shorttitle = {{{AgentCoder}}},
  author = {Huang, Dong and Bu, Qingwen and Zhang, Jie M. and Luck, Michael and Cui, Heming},
  year = {2023},
  month = dec,
  number = {arXiv:2312.13010},
  eprint = {2312.13010},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2312.13010},
  urldate = {2024-01-02},
  abstract = {The advancement of natural language processing (NLP) has been significantly boosted by the development of transformer-based large language models (LLMs). These models have revolutionized NLP tasks, particularly in code generation, aiding developers in creating software with enhanced efficiency. Despite their advancements, challenges in balancing code snippet generation with effective test case generation and execution persist. To address these issues, this paper introduces Multi-Agent Assistant Code Generation (AgentCoder), a novel solution comprising a multi-agent framework with specialized agents: the programmer agent, the test designer agent, and the test executor agent. During the coding procedure, the programmer agent will focus on the code generation and refinement based on the test executor agent's feedback. The test designer agent will generate test cases for the generated code, and the test executor agent will run the code with the test cases and write the feedback to the programmer. This collaborative system ensures robust code generation, surpassing the limitations of single-agent models and traditional methodologies. Our extensive experiments on 9 code generation models and 12 enhancement approaches showcase AgentCoder's superior performance over existing code generation models and prompt engineering techniques across various benchmarks. For example, AgentCoder achieves 77.4\% and 89.1\% pass@1 in HumanEval-ET and MBPP-ET with GPT-3.5, while SOTA baselines obtain only 69.5\% and 63.0\%.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/495L4V9P/Huang et al. - 2023 - AgentCoder Multi-Agent-based Code Generation with Iterative Testing and Optimisation.pdf;/home/carlos/Zotero/storage/GBNPDX77/2312.html}
}

@misc{jzh2074,
  title = {{{AI}} Types. {{Tipos}} Inteligencia {{Artificial}}.Svg},
  author = {{Jzh2074}},
  year = {2022},
  howpublished = {https://commons.wikimedia.org/wiki/File:AI\_Types.\_Tipos\_Inteligencia\_Artificial.svg}
}

@misc{kaddourChallengesApplicationsLarge2023,
  title = {Challenges and {{Applications}} of {{Large Language Models}}},
  author = {Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and Bradley, Herbie and Raileanu, Roberta and McHardy, Robert},
  year = {2023},
  month = jul,
  number = {arXiv:2307.10169},
  eprint = {2307.10169},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.10169},
  urldate = {2023-09-27},
  abstract = {Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field's current state more quickly and become productive.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/carlos/Zotero/storage/N4TI6Q5V/Kaddour et al. - 2023 - Challenges and Applications of Large Language Mode.pdf;/home/carlos/Zotero/storage/EWGBTNGU/2307.html}
}

@misc{kirkbrideQirkyFoxDot2023,
  title = {Qirky/{{FoxDot}}},
  author = {Kirkbride, Ryan},
  year = {2023},
  month = dec,
  urldate = {2023-12-09},
  abstract = {Python driven environment for Live Coding}
}

@misc{lewisRetrievalAugmentedGenerationKnowledgeIntensive2021,
  title = {Retrieval-{{Augmented Generation}} for {{Knowledge-Intensive NLP Tasks}}},
  author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
  year = {2021},
  month = apr,
  number = {arXiv:2005.11401},
  eprint = {2005.11401},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2005.11401},
  urldate = {2023-11-07},
  abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/carlos/Zotero/storage/WVAFYFB3/Lewis et al. - 2021 - Retrieval-Augmented Generation for Knowledge-Inten.pdf;/home/carlos/Zotero/storage/C495Y3UY/2005.html}
}

@misc{liSelfAlignmentInstructionBacktranslation2023,
  title = {Self-{{Alignment}} with {{Instruction Backtranslation}}},
  author = {Li, Xian and Yu, Ping and Zhou, Chunting and Schick, Timo and Zettlemoyer, Luke and Levy, Omer and Weston, Jason and Lewis, Mike},
  year = {2023},
  month = aug,
  number = {arXiv:2308.06259},
  eprint = {2308.06259},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.06259},
  urldate = {2023-09-27},
  abstract = {We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/JADHWQUA/Li et al. - 2023 - Self-Alignment with Instruction Backtranslation.pdf;/home/carlos/Zotero/storage/4SRRSDIW/2308.html}
}

@article{ListAudioProgramming2023,
  title = {List of Audio Programming Languages},
  year = {2023},
  month = jul,
  journal = {Wikipedia},
  urldate = {2023-12-11},
  abstract = {This is a list of notable programming languages optimized for sound production, algorithmic composition, and sound synthesis. ABC notation, a language for notating music using the ASCII character set Bol Processor, a model of formal grammars enriched with polymetric expressions for the representation of time structures ChucK, strongly timed, concurrent, and on-the-fly audio programming language Real-time Cmix, a MUSIC-N synthesis language somewhat similar to Csound Cmajor, a high-performance JIT-compiled C-style language for DSP Common Lisp Music (CLM), a music synthesis and signal processing package in the Music V family Csound, a MUSIC-N synthesis language released under the LGPL with many available unit generators Extempore, a live-coding environment that borrows a core foundation from the Impromptu environment FAUST, Functional Audio Stream, a functional compiled language for efficient real-time audio signal processing GLICOL, a graph-oriented live coding language written in Rust Hierarchical Music Specification Language (HMSL), optimized more for music than synthesis, developed in the 1980s in Forth Impromptu, a Scheme language environment for Mac OS X capable of sound and video synthesis, algorithmic composition, and 2D and 3D graphics programming Ixi lang, a programming language for live coding musical expression. JFugue, a Java and JVM library for programming music that outputs to MIDI and has the ability to convert to formats including ABC Notation, Lilypond, and MusicXML jMusic JSyn Keykit, programming language and portable graphical environment for MIDI music composition Kyma (sound design language) LilyPond, a computer program and file format for music engraving. Max/MSP, a proprietary, modular visual programming language aimed at sound synthesis for music Music Macro Language (MML), often used to produce chiptune music in Japan MUSIC-N, includes versions I, II, III, IV, IV-B, IV-BF, V, 11, and 360 Nyquist OpenMusic Orca (music programming language) Pure Data, a modular visual programming language for signal processing aimed at music creation Reaktor Sonic Pi Structured Audio Orchestra Language (SAOL), part of the MPEG-4 Structured Audio standard SuperCollider SynthEdit, a modular visual programming language for signal processing aimed at creating audio plug-ins},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1167204231},
  file = {/home/carlos/Zotero/storage/GFR76MYV/List_of_audio_programming_languages.html}
}

@misc{liStructuredChainofThoughtPrompting2023,
  title = {Structured {{Chain-of-Thought Prompting}} for {{Code Generation}}},
  author = {Li, Jia and Li, Ge and Li, Yongmin and Jin, Zhi},
  year = {2023},
  month = sep,
  number = {arXiv:2305.06599},
  eprint = {2305.06599},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-23},
  abstract = {Large Language Models (LLMs) (e.g., ChatGPT) have shown impressive performance in code generation. LLMs take prompts as inputs, and Chain-of-Thought (CoT) prompting is the state-of-theart prompting technique. CoT prompting asks LLMs first to generate CoTs (i.e., intermediate natural language reasoning steps) and then output the code. However, CoT prompting is designed for natural language generation and has low accuracy in code generation.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Software Engineering},
  file = {/home/carlos/Zotero/storage/KQ5L5V5X/Li et al. - 2023 - Structured Chain-of-Thought Prompting for Code Gen.pdf}
}

@misc{liuLostMiddleHow2023,
  title = {Lost in the {{Middle}}: {{How Language Models Use Long Contexts}}},
  shorttitle = {Lost in the {{Middle}}},
  author = {Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  year = {2023},
  month = nov,
  number = {arXiv:2307.03172},
  eprint = {2307.03172},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-12-12},
  abstract = {While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/45RW8HRL/Liu et al. - 2023 - Lost in the Middle How Language Models Use Long C.pdf}
}

@misc{liuWavJourneyCompositionalAudio2023,
  title = {{{WavJourney}}: {{Compositional Audio Creation}} with {{Large Language Models}}},
  shorttitle = {{{WavJourney}}},
  author = {Liu, Xubo and Zhu, Zhongkai and Liu, Haohe and Yuan, Yi and Cui, Meng and Huang, Qiushi and Liang, Jinhua and Cao, Yin and Kong, Qiuqiang and Plumbley, Mark D. and Wang, Wenwu},
  year = {2023},
  month = nov,
  number = {arXiv:2307.14335},
  eprint = {2307.14335},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  urldate = {2024-01-04},
  abstract = {Despite breakthroughs in audio generation models, their capabilities are often confined to domain-specific conditions such as speech transcriptions and audio captions. However, real-world audio creation aims to generate harmonious audio containing various elements such as speech, music, and sound effects with controllable conditions, which is challenging to address using existing audio generation systems. We present WavJourney, a novel framework that leverages Large Language Models (LLMs) to connect various audio models for audio creation. WavJourney allows users to create storytelling audio content with diverse audio elements simply from textual descriptions. Specifically, given a text instruction, WavJourney first prompts LLMs to generate an audio script that serves as a structured semantic representation of audio elements. The audio script is then converted into a computer program, where each line of the program calls a task-specific audio generation model or computational operation function. The computer program is then executed to obtain a compositional and interpretable solution for audio creation. Experimental results suggest that WavJourney is capable of synthesizing realistic audio aligned with textually-described semantic, spatial and temporal conditions, achieving state-of-the-art results on text-toaudio generation benchmarks. Additionally, we introduce a new multi-genre story benchmark. Subjective evaluations demonstrate the potential of WavJourney in crafting engaging storytelling audio content from text. We further demonstrate that WavJourney can facilitate human-machine co-creation in multi-round dialogues. To foster future research, the code and synthesized audio are available at: https://audio-agi.github.io/WavJourney\_demopage/.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multimedia,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/carlos/Zotero/storage/FT53GGQ9/Liu et al. - 2023 - WavJourney Compositional Audio Creation with Large Language Models.pdf}
}

@misc{LiveCodeTidal,
  title = {Live Code with {{Tidal Cycles}} | {{Tidal Cycles}}},
  urldate = {2023-12-09},
  abstract = {Live coding environment for making algorithmic patterns},
  howpublished = {https://tidalcycles.org/},
  langid = {english},
  file = {/home/carlos/Zotero/storage/6QLETVM5/tidalcycles.org.html}
}

@misc{LLMPromptingGuide,
  title = {{{LLM}} Prompting Guide},
  urldate = {2023-10-30},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/docs/transformers/main/en/tasks/prompting},
  file = {/home/carlos/Zotero/storage/MLZ8EYCS/prompting.html}
}

@misc{luMuseCocoGeneratingSymbolic2023,
  title = {{{MuseCoco}}: {{Generating Symbolic Music}} from {{Text}}},
  shorttitle = {{{MuseCoco}}},
  author = {Lu, Peiling and Xu, Xin and Kang, Chenfei and Yu, Botao and Xing, Chengyi and Tan, Xu and Bian, Jiang},
  year = {2023},
  month = may,
  number = {arXiv:2306.00110},
  eprint = {2306.00110},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  urldate = {2024-01-04},
  abstract = {Generating music from text descriptions is a user-friendly mode since the text is a relatively easy interface for user engagement. While some approaches utilize texts to control music audio generation, editing musical elements in generated audio is challenging for users. In contrast, symbolic music offers ease of editing, making it more accessible for users to manipulate specific musical elements. In this paper, we propose MuseCoco, which generates symbolic music from text descriptions with musical attributes as the bridge to break down the task into text-to-attribute understanding and attribute-to-music generation stages. MuseCoCo stands for Music Composition Copilot that empowers musicians to generate music directly from given text descriptions, offering a significant improvement in efficiency compared to creating music entirely from scratch. The system has two main advantages: Firstly, it is data efficient. In the attribute-to-music generation stage, the attributes can be directly extracted from music sequences, making the model training self-supervised. In the text-to-attribute understanding stage, the text is synthesized and refined by ChatGPT based on the defined attribute templates. Secondly, the system can achieve precise control with specific attributes in text descriptions and offers multiple control options through attribute-conditioned or text-conditioned approaches. MuseCoco outperforms baseline systems in terms of musicality, controllability, and overall score by at least 1.27, 1.08, and 1.32 respectively. Besides, there is a notable enhancement of about 20\% in objective control accuracy. In addition, we have developed a robust large-scale model with 1.2 billion parameters, showcasing exceptional controllability and musicality. Music samples generated by MuseCoco are available via this link 1, and the code is available at this link 2.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Multimedia,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/carlos/Zotero/storage/PW28ZPDB/Lu et al. - 2023 - MuseCoco Generating Symbolic Music from Text.pdf}
}

@misc{malachAutoRegressiveNextTokenPredictors2023,
  title = {Auto-{{Regressive Next-Token Predictors}} Are {{Universal Learners}}},
  author = {Malach, Eran},
  year = {2023},
  month = sep,
  number = {arXiv:2309.06979},
  eprint = {2309.06979},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-27},
  abstract = {Large language models display remarkable capabilities in logical and mathematical reasoning, allowing them to solve complex tasks. Interestingly, these abilities emerge in networks trained on the simple task of next-token prediction. In this work, we present a theoretical framework for studying auto-regressive next-token predictors. We demonstrate that even simple models such as linear next-token predictors, trained on Chain-ofThought (CoT) data, can approximate any function efficiently computed by a Turing machine. We introduce a new complexity measure{\textemdash}length complexity{\textemdash}which measures the number of intermediate tokens in a CoT sequence required to approximate some target function, and analyze the interplay between length complexity and other notions of complexity. Finally, we show experimentally that simple next-token predictors, such as linear networks and shallow Multi-Layer Perceptrons (MLPs), display non-trivial performance on text generation and arithmetic tasks. Our results demonstrate that the power of language models can be attributed, to a great extent, to the auto-regressive next-token training scheme, and not necessarily to a particular choice of architecture.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/carlos/Zotero/storage/3B7X3RYX/Malach - 2023 - Auto-Regressive Next-Token Predictors are Universa.pdf}
}

@misc{mastropaoloRobustnessCodeGeneration2023,
  title = {On the {{Robustness}} of {{Code Generation Techniques}}: {{An Empirical Study}} on {{GitHub Copilot}}},
  shorttitle = {On the {{Robustness}} of {{Code Generation Techniques}}},
  author = {Mastropaolo, Antonio and Pascarella, Luca and Guglielmi, Emanuela and Ciniselli, Matteo and Scalabrino, Simone and Oliveto, Rocco and Bavota, Gabriele},
  year = {2023},
  month = feb,
  number = {arXiv:2302.00438},
  eprint = {2302.00438},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-12-26},
  abstract = {Software engineering research has always being concerned with the improvement of code completion approaches, which suggest the next tokens a developer will likely type while coding. The release of GitHub Copilot constitutes a big step forward, also because of its unprecedented ability to automatically generate even entire functions from their natural language description. While the usefulness of Copilot is evident, it is still unclear to what extent it is robust. Specifically, we do not know the extent to which semantic-preserving changes in the natural language description provided to the model have an effect on the generated code function. In this paper we present an empirical study in which we aim at understanding whether different but semantically equivalent natural language descriptions result in the same recommended function. A negative answer would pose questions on the robustness of deep learning (DL)-based code generators since it would imply that developers using different wordings to describe the same code would obtain different recommendations. We asked Copilot to automatically generate 892 Java methods starting from their original Javadoc description. Then, we generated different semantically equivalent descriptions for each method both manually and automatically, and we analyzed the extent to which predictions generated by Copilot changed. Our results show that modifying the description results in different code recommendations in {\textasciitilde}46\% of cases. Also, differences in the semantically equivalent descriptions might impact the correctness of the generated code {\textasciitilde}28\%.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Software Engineering},
  file = {/home/carlos/Zotero/storage/HYGTSJJX/Mastropaolo et al. - 2023 - On the Robustness of Code Generation Techniques A.pdf}
}

@book{minsky1969perceptrons,
  title = {Perceptrons: An Introduction to Computational Geometry},
  author = {Minsky, M. and Papert, S.},
  year = {1969},
  publisher = {{MIT Press}},
  isbn = {978-0-262-63022-1},
  lccn = {69014379}
}

@book{minskyPerceptronsIntroductionComputational2017,
  title = {Perceptrons: {{An Introduction}} to {{Computational Geometry}}},
  shorttitle = {Perceptrons},
  author = {Minsky, Marvin and Papert, Seymour A.},
  year = {2017},
  month = sep,
  publisher = {{The MIT Press}},
  doi = {10.7551/mitpress/11301.001.0001},
  urldate = {2023-12-21},
  abstract = {The first systematic study of parallelism in computation by two pioneers in the field.Reissue of the 1988 Expanded Edition with a new foreword by L{\'e}on BottouIn},
  isbn = {978-0-262-34393-0},
  langid = {english},
  file = {/home/carlos/Zotero/storage/VUWTY5QW/PerceptronsAn-Introduction-to-Computational.html}
}

@article{moradidakhelGitHubCopilotAI2023,
  title = {{{GitHub Copilot AI}} Pair Programmer: {{Asset}} or {{Liability}}?},
  shorttitle = {{{GitHub Copilot AI}} Pair Programmer},
  author = {Moradi Dakhel, Arghavan and Majdinasab, Vahid and Nikanjam, Amin and Khomh, Foutse and Desmarais, Michel C. and Jiang, Zhen Ming (Jack)},
  year = {2023},
  month = sep,
  journal = {Journal of Systems and Software},
  volume = {203},
  pages = {111734},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2023.111734},
  urldate = {2023-12-26},
  abstract = {Automatic program synthesis is a long-lasting dream in software engineering. Recently, a promising Deep Learning (DL) based solution, called Copilot, has been proposed by OpenAI and Microsoft as an industrial product. Although some studies evaluate the correctness of Copilot solutions and report its issues, more empirical evaluations are necessary to understand how developers can benefit from it effectively. In this paper, we study the capabilities of Copilot in two different programming tasks: (i) generating (and reproducing) correct and efficient solutions for fundamental algorithmic problems, and (ii) comparing Copilot's proposed solutions with those of human programmers on a set of programming tasks. For the former, we assess the performance and functionality of Copilot in solving selected fundamental problems in computer science, like sorting and implementing data structures. In the latter, a dataset of programming problems with human-provided solutions is used. The results show that Copilot is capable of providing solutions for almost all fundamental algorithmic problems, however, some solutions are buggy and non-reproducible. Moreover, Copilot has some difficulties in combining multiple methods to generate a solution. Comparing Copilot to humans, our results show that the correct ratio of humans' solutions is greater than Copilot's suggestions, while the buggy solutions generated by Copilot require less effort to be repaired. Based on our findings, if Copilot is used by expert developers in software projects, it can become an asset since its suggestions could be comparable to humans' contributions in terms of quality. However, Copilot can become a liability if it is used by novice developers who may fail to filter its buggy or non-optimal solutions due to a lack of expertise.},
  keywords = {Code completion,GitHub copilot,Language model,Testing},
  file = {/home/carlos/Zotero/storage/UZ645QAQ/Moradi Dakhel et al. - 2023 - GitHub Copilot AI pair programmer Asset or Liabil.pdf;/home/carlos/Zotero/storage/U3TB2FHT/S0164121223001292.html}
}

@article{mukherjeeOrcaProgressiveLearning2023,
  title = {Orca: {{Progressive Learning}} from {{Complex Explanation Traces}} of {{GPT-4}}},
  shorttitle = {Orca},
  author = {Mukherjee, Subhabrata (Subho) and Mitra, Arindam and Jawahar, Ganesh and Agarwal, Sahaj and Palangi, Hamid and Awadallah, Ahmed H.},
  year = {2023},
  month = jun,
  urldate = {2023-06-25},
  abstract = {Recent research has focused on enhancing the capability of smaller models through imitation~learning, drawing on the outputs generated by large foundation models (LFMs). A number of issues impact the quality of these models, ranging from~limited imitation signals~from shallow LFM outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting in [{\ldots}]},
  langid = {american},
  file = {/home/carlos/Zotero/storage/ANQ2XMEH/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4.html}
}

@article{nielsenNeuralNetworksDeep2015,
  title = {Neural {{Networks}} and {{Deep Learning}}},
  author = {Nielsen, Michael A.},
  year = {2015},
  publisher = {{Determination Press}},
  urldate = {2023-05-24},
  langid = {english},
  file = {/home/carlos/Zotero/storage/TZXGVB5T/index.html}
}

@misc{OvertoneCollaborativeProgrammable,
  title = {Overtone - {{Collaborative Programmable Music}}},
  urldate = {2023-12-09},
  howpublished = {https://overtone.github.io/},
  file = {/home/carlos/Zotero/storage/LXF2GLUX/overtone.github.io.html}
}

@misc{PapersCodeAudioLM,
  title = {Papers with {{Code}} - {{AudioLM}}: A {{Language Modeling Approach}} to {{Audio Generation}}},
  shorttitle = {Papers with {{Code}} - {{AudioLM}}},
  urldate = {2024-01-04},
  abstract = {Implemented in 4 code libraries.},
  howpublished = {https://paperswithcode.com/paper/audiolm-a-language-modeling-approach-to-audio},
  langid = {english},
  file = {/home/carlos/Zotero/storage/U33C57MP/audiolm-a-language-modeling-approach-to-audio.html}
}

@misc{PapersCodeMuseCoco,
  title = {Papers with {{Code}} - {{MuseCoco}}: {{Generating Symbolic Music}} from {{Text}}},
  shorttitle = {Papers with {{Code}} - {{MuseCoco}}},
  urldate = {2024-01-04},
  abstract = {Implemented in one code library.},
  howpublished = {https://paperswithcode.com/paper/musecoco-generating-symbolic-music-from-text},
  langid = {english},
  file = {/home/carlos/Zotero/storage/WCM7YULJ/musecoco-generating-symbolic-music-from-text.html}
}

@misc{PapersCodeRobustness,
  title = {Papers with {{Code}} - {{On}} the {{Robustness}} of {{Code Generation Techniques}}: {{An Empirical Study}} on {{GitHub Copilot}}},
  shorttitle = {Papers with {{Code}} - {{On}} the {{Robustness}} of {{Code Generation Techniques}}},
  urldate = {2023-12-26},
  abstract = {Implemented in one code library.},
  howpublished = {https://cs.paperswithcode.com/paper/on-the-robustness-of-code-generation},
  langid = {english},
  file = {/home/carlos/Zotero/storage/T33YVX3N/on-the-robustness-of-code-generation.html}
}

@misc{PapersCodeWavJourney,
  title = {Papers with {{Code}} - {{WavJourney}}: {{Compositional Audio Creation}} with {{Large Language Models}}},
  shorttitle = {Papers with {{Code}} - {{WavJourney}}},
  urldate = {2024-01-04},
  abstract = {Implemented in one code library.},
  howpublished = {https://paperswithcode.com/paper/wavjourney-compositional-audio-creation-with},
  langid = {english},
  file = {/home/carlos/Zotero/storage/WIJE3S3G/wavjourney-compositional-audio-creation-with.html}
}

@misc{pengImpactAIDeveloper2023a,
  title = {The {{Impact}} of {{AI}} on {{Developer Productivity}}: {{Evidence}} from {{GitHub Copilot}}},
  shorttitle = {The {{Impact}} of {{AI}} on {{Developer Productivity}}},
  author = {Peng, Sida and Kalliamvakou, Eirini and Cihon, Peter and Demirer, Mert},
  year = {2023},
  month = feb,
  number = {arXiv:2302.06590},
  eprint = {2302.06590},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-12-27},
  abstract = {Generative AI tools hold promise to increase human productivity. This paper presents results from a controlled experiment with GitHub Copilot, an AI pair programmer. Recruited software developers were asked to implement an HTTP server in JavaScript as quickly as possible. The treatment group, with access to the AI pair programmer, completed the task 55.8\% faster than the control group. Observed heterogenous effects show promise for AI pair programmers to help people transition into software development careers.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Software Engineering},
  file = {/home/carlos/Zotero/storage/UEPW89PF/Peng et al. - 2023 - The Impact of AI on Developer Productivity Eviden.pdf}
}

@book{penroseNuevaMenteEmperador2015,
  title = {{La nueva mente del emperador}},
  author = {Penrose, Roger},
  year = {2015},
  month = jul,
  publisher = {{Penguin Random House Grupo Editorial Espa{\~n}a}},
  abstract = {Un apasionante paseo por la matem{\'a}tica y la f{\'i}sica, y por los hallazgos del pensamiento humano de la mano de Roger Penrose, Premio Nobel de F{\'i}sica 2020. Durante d{\'e}cadas, los defensores de la inteligencia artificial han mantenido que los ordenadores har{\'a}n pronto todo lo que la mente humana puede hacer. En su favor, se puede utilizar, por ejemplo, el que ya hay m{\'a}quinas que juegan al ajedrez como los grandes maestros. Ahora bien, {\textquestiondown}comprenden el juego como lo hacemos nosotros?En este libro, Roger Penrose, probablemente el especialista en la teor{\'i}a general de la relatividad m{\'a}s prestigioso del mundo y una de las mentes anal{\'i}ticas m{\'a}s originales de la actualidad, sostiene que existen facetas del pensamiento humano que nunca ser{\'a}n emuladas por un ordenador. Para defender esa tesis, Penrose recurre a una amplia gama de conocimientos cient{\'i}ficos, que van desde la m{\'a}quina de Turing hasta la estructura del cerebro, pasando por el teorema de G{\"o}del, los agujeros negros y los blancos, la radiaci{\'o}n de Hawking, la entrop{\'i}a o la mec{\'a}nica cu{\'a}ntica. Entre los numerosos estudios existentes dedicados a la relaci{\'o}n entre la mente y el cuerpo, esta ambiciosa obra sobresale tanto por su lucidez y claridad como por su rigor y profundidad. Rese{\~n}a:{\guillemotleft}Un libro audaz, brillante e innovador. Cuando Penrose habla, los cient{\'i}ficos escuchan.{\guillemotright}The New York Times Book Review},
  googlebooks = {sLz3CQAAQBAJ},
  isbn = {978-84-663-3083-1},
  langid = {spanish},
  keywords = {Computers / Artificial Intelligence / General,Science / Essays,Science / General,Science / Life Sciences / Neuroscience}
}

@misc{perezThreadsYaSupera2023,
  title = {{Threads ya supera los 100 millones de usuarios. Ha aplastado el r{\'e}cord que ten{\'i}a ChatGPT}},
  author = {P{\'e}rez, Enrique},
  year = {2023},
  month = jul,
  journal = {Xataka},
  urldate = {2023-12-08},
  abstract = {La tecnolog{\'i}a va a un ritmo vertiginoso. Threads, el rival de Twitter de Instagram, lleg{\'o} oficialmente la semana pasada. Y aunque para usarlo en Europa hay...},
  chapter = {aplicaciones},
  howpublished = {https://www.xataka.com/aplicaciones/threads-supera-100-millones-usuarios-ha-aplastado-record-que-tenia-chatgpt},
  langid = {spanish},
  file = {/home/carlos/Zotero/storage/9CKSGAN7/threads-supera-100-millones-usuarios-ha-aplastado-record-que-tenia-chatgpt.html}
}

@misc{PrimerosMesesChat2023,
  title = {{Primeros 6 meses del Chat GPT, {\textquestiondown}cu{\'a}les son sus logros y desaf{\'i}os?}},
  year = {2023},
  month = may,
  journal = {Noticias Radio Reflejos},
  urldate = {2023-12-08},
  abstract = {Buenos Aires, mayo de 2023 {\textendash} Bruno Ruy{\'u}, profesor de la Maestr{\'i}a de Ciencias de Datos de la Universidad Austral, hace un repaso por los principales logros y desaf{\'i}os del ChatGPT. ``El ChatGPT logr{\'o} alcanzar los 100 millones de usuarios en dos meses, un n{\'u}mero impresionante. Mientras Instagram alcanz{\'o} este record en 30 meses, Spotify [{\ldots}]},
  langid = {spanish},
  file = {/home/carlos/Zotero/storage/K7W3P7AF/primeros-6-meses-del-chat-gpt-cuales-son-sus-logros-y-desafios.html}
}

@misc{PromptEngineeringGuide2023,
  title = {Prompt {{Engineering Guide}}},
  year = {2023},
  month = dec,
  urldate = {2024-01-05},
  abstract = {A Comprehensive Overview of Prompt Engineering},
  howpublished = {https://www.promptingguide.ai/},
  langid = {english},
  file = {/home/carlos/Zotero/storage/866LWP67/www.promptingguide.ai.html}
}

@article{radfordLanguageModelsAre2019a,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year = {2019},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  langid = {english},
  file = {/home/carlos/Zotero/storage/F5MP43DF/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf}
}

@misc{radovanovicEmergingArchitecturesLLM2023,
  title = {Emerging {{Architectures}} for {{LLM Applications}}},
  author = {Radovanovic, Rajko, Matt Bornstein},
  year = {2023},
  month = jun,
  journal = {Andreessen Horowitz},
  urldate = {2023-06-27},
  abstract = {A reference architecture for the LLM app stack. It shows the most common systems, tools, and design patterns used by AI startups and tech companies.},
  howpublished = {https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/},
  langid = {american},
  file = {/home/carlos/Zotero/storage/PAWYWIEW/emerging-architectures-for-llm-applications.html}
}

@misc{RevistaBitsCiencia,
  title = {{Revista Bits de Ciencia N{\textdegree} 21 - 2021}},
  urldate = {2023-10-30},
  chapter = {Article Section},
  howpublished = {https://revistasdex.uchile.cl/files/full/BitsdeCiencia\_(21)\_2021/index.html},
  langid = {spanish},
  file = {/home/carlos/Zotero/storage/6QXLYRRD/index.html}
}

@book{roadsComputerMusicTutorial1996,
  title = {The {{Computer Music Tutorial}}},
  author = {Roads, Curtis and Strawn, John and Abbott, Curtis and Gordon, John and Philip, Greenspun},
  year = {1996},
  publisher = {{The MIT Press}},
  address = {{Cambride}}
}

@article{rosenblattPerceptronProbabilisticModel1958,
  title = {The Perceptron: {{A}} Probabilistic Model for Information Storage and Organization in the Brain.},
  shorttitle = {The Perceptron},
  author = {Rosenblatt, F.},
  year = {1958},
  journal = {Psychological Review},
  volume = {65},
  number = {6},
  pages = {386--408},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/h0042519},
  urldate = {2023-12-21},
  abstract = {The first of these questions is in the province of sensory physiology, and is the only one for which appreciable understanding has been achieved. This article will be concerned primarily with the second and third questions, which are still subject to a vast amount of speculation, and where the few relevant facts currently supplied by neurophysiology have not yet been integrated into an acceptable theory. With regard to the second question, two alternative positions have been maintained. The first suggests that storage of sensory information is in the form of coded representations or images, with some sort of one-to-one mapping between the sensory stimulus},
  langid = {english},
  file = {/home/carlos/Zotero/storage/9Z6ZPSWA/Rosenblatt - 1958 - The perceptron A probabilistic model for informat.pdf}
}

@book{rothmanTransformersNaturalLanguage2021,
  title = {Transformers for {{Natural Language Processing}}: {{Build}} Innovative Deep Neural Network Architectures for {{NLP}} with {{Python}}, {{PyTorch}}, {{TensorFlow}}, {{BERT}}, {{RoBERTa}}, and More},
  shorttitle = {Transformers for {{Natural Language Processing}}},
  author = {Rothman, Denis},
  year = {2021},
  publisher = {{Packt Publishing}},
  address = {{Birmingham}},
  isbn = {978-1-80056-579-1},
  langid = {english}
}

@misc{RunTextGeneration2022,
  title = {Run Text Generation with {{Bloom}} and {{GPT}} Models on {{Amazon SageMaker JumpStart}} | {{AWS Machine Learning Blog}}},
  year = {2022},
  month = nov,
  urldate = {2023-10-31},
  chapter = {Amazon SageMaker},
  howpublished = {https://aws.amazon.com/blogs/machine-learning/run-text-generation-with-gpt-and-bloom-models-on-amazon-sagemaker-jumpstart/},
  langid = {american},
  file = {/home/carlos/Zotero/storage/FTBUAU2Q/run-text-generation-with-gpt-and-bloom-models-on-amazon-sagemaker-jumpstart.html}
}

@book{RussellStuartJ2021AI:A,
  title = {Artificial Intelligence : {{A}} Modern Approach},
  author = {Russell, Stuart J},
  year = {2021},
  edition = {4th Edition.; Global edition.},
  publisher = {{Pearson}},
  address = {{Harlow}},
  isbn = {978-0-13-461099-3},
  langid = {english},
  keywords = {Inteligencia artificial,Inteligencia artificial Rob{\'o}tica,Transferencia del conocimiento}
}

@misc{schaefferAreEmergentAbilities2023,
  title = {Are {{Emergent Abilities}} of {{Large Language Models}} a {{Mirage}}?},
  author = {Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  year = {2023},
  month = may,
  number = {arXiv:2304.15004},
  eprint = {2304.15004},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-23},
  abstract = {Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, emergent abilities appear due the researcher's choice of metric rather than due to fundamental changes in model behavior with scale. Specifically, nonlinear or discontinuous metrics produce apparent emergent abilities, whereas linear or continuous metrics produce smooth, continuous, predictable changes in model performance. We present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities, (2) make, test and confirm two predictions about metric choices in a metaanalysis of emergent abilities on BIG-Bench; and (3) show how to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks. Via all three analyses, we provide evidence that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/carlos/Zotero/storage/IKAF8WT5/Schaeffer et al. - 2023 - Are Emergent Abilities of Large Language Models a .pdf}
}

@misc{selAlgorithmThoughtsEnhancing2023,
  title = {Algorithm of {{Thoughts}}: {{Enhancing Exploration}} of {{Ideas}} in {{Large Language Models}}},
  shorttitle = {Algorithm of {{Thoughts}}},
  author = {Sel, Bilgehan and {Al-Tawaha}, Ahmad and Khattar, Vanshaj and Wang, Lu and Jia, Ruoxi and Jin, Ming},
  year = {2023},
  month = aug,
  number = {arXiv:2308.10379},
  eprint = {2308.10379},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.10379},
  urldate = {2023-09-27},
  abstract = {Current literature, aiming to surpass the "Chain-of-Thought" approach, often resorts to an external modus operandi involving halting, modifying, and then resuming the generation process to boost Large Language Models' (LLMs) reasoning capacities. This mode escalates the number of query requests, leading to increased costs, memory, and computational overheads. Addressing this, we propose the Algorithm of Thoughts -- a novel strategy that propels LLMs through algorithmic reasoning pathways, pioneering a new mode of in-context learning. By employing algorithmic examples, we exploit the innate recurrence dynamics of LLMs, expanding their idea exploration with merely one or a few queries. Our technique outperforms earlier single-query methods and stands on par with a recent multi-query strategy that employs an extensive tree search algorithm. Intriguingly, our results suggest that instructing an LLM using an algorithm can lead to performance surpassing that of the algorithm itself, hinting at LLM's inherent ability to weave its intuition into optimized searches. We probe into the underpinnings of our method's efficacy and its nuances in application.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/RUC6VMXG/Sel et al. - 2023 - Algorithm of Thoughts Enhancing Exploration of Id.pdf;/home/carlos/Zotero/storage/DMUQY952/2308.html}
}

@article{shannon1951prediction,
  title = {Prediction and Entropy of Printed English},
  author = {Shannon, Claude Elwood},
  year = {1951},
  month = jan,
  journal = {Bell System Technical Journal},
  volume = {30},
  pages = {50--64},
  added-at = {2013-02-27T08:26:04.000+0100},
  interhash = {daabc21c7f6e71f6e78a10c8d3492927},
  intrahash = {2e79cf0f6022645a632b13e081b0b035},
  keywords = {communication english entropy information prediction shannon toread},
  timestamp = {2014-07-28T15:57:31.000+0200}
}

@misc{shevlaneModelEvaluationExtreme2023,
  title = {Model Evaluation for Extreme Risks},
  author = {Shevlane, Toby and Farquhar, Sebastian and Garfinkel, Ben and Phuong, Mary and Whittlestone, Jess and Leung, Jade and Kokotajlo, Daniel and Marchal, Nahema and Anderljung, Markus and Kolt, Noam and Ho, Lewis and Siddarth, Divya and Avin, Shahar and Hawkins, Will and Kim, Been and Gabriel, Iason and Bolina, Vijay and Clark, Jack and Bengio, Yoshua and Christiano, Paul and Dafoe, Allan},
  year = {2023},
  month = may,
  number = {arXiv:2305.15324},
  eprint = {2305.15324},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-06-27},
  abstract = {Current approaches to building general-purpose AI systems tend to produce systems with both beneficial and harmful capabilities. Further progress in AI development could lead to capabilities that pose extreme risks, such as offensive cyber capabilities or strong manipulation skills. We explain why model evaluation is critical for addressing extreme risks. Developers must be able to identify dangerous capabilities (through "dangerous capability evaluations") and the propensity of models to apply their capabilities for harm (through "alignment evaluations"). These evaluations will become critical for keeping policymakers and other stakeholders informed, and for making responsible decisions about model training, deployment, and security.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,K.4.1},
  file = {/home/carlos/Zotero/storage/BDHEV472/Shevlane et al. - 2023 - Model evaluation for extreme risks.pdf}
}

@misc{StableAudioFast,
  title = {Stable {{Audio}}: {{Fast Timing-Conditioned Latent Audio Diffusion}}},
  shorttitle = {Stable {{Audio}}},
  journal = {Stability AI},
  urldate = {2023-09-27},
  abstract = {Stable Audio represents the cutting-edge audio generation research by Stability AI's generative audio research lab, Harmonai. We continue to improve our model architectures, datasets, and training procedures to improve output quality, controllability, inference speed, and output length.},
  howpublished = {https://stability.ai/research/stable-audio-efficient-timing-latent-diffusion},
  langid = {british},
  file = {/home/carlos/Zotero/storage/HE8PHSK2/stable-audio-efficient-timing-latent-diffusion.html}
}

@misc{StrudelREPL,
  title = {Strudel {{REPL}}},
  urldate = {2023-12-09},
  howpublished = {https://strudel.cc/},
  file = {/home/carlos/Zotero/storage/6HPXIMRG/strudel.cc.html}
}

@misc{SunoAI,
  title = {Suno {{AI}}},
  urldate = {2023-12-26},
  abstract = {We are building a future where anyone can make great music. No instrument needed, just imagination. From your mind to music.},
  howpublished = {https://www.suno.ai/},
  langid = {english},
  file = {/home/carlos/Zotero/storage/6922LJJL/www.suno.ai.html}
}

@book{supperMusicaElectronicaMusica2012,
  title = {M{\'u}sica Electr{\'o}nica y M{\'u}sica Con Ordenador. {{Historia}}, Est{\'e}tica, M{\'e}todos, Sistemas},
  author = {Supper, Martin},
  year = {2012},
  publisher = {{Alianza M{\'u}sica}},
  address = {{Madrid}}
}

@misc{teamChucKStronglyTimedMusic,
  title = {{{ChucK}}: {{A Strongly-Timed Music Programming Language}}},
  shorttitle = {{{ChucK}}},
  author = {Team, ChucK},
  urldate = {2023-12-09},
  abstract = {ChucK is a strongly-timed programming language for interactive sound synthesis and music creation.},
  howpublished = {https://ccrma.stanford.edu/software/chuck/},
  langid = {english},
  file = {/home/carlos/Zotero/storage/Q7Y4LBTF/chuck.stanford.edu.html}
}

@misc{TextGenerationStrategies,
  title = {Text Generation Strategies},
  urldate = {2023-10-31},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/docs/transformers/main/en/generation\_strategies},
  file = {/home/carlos/Zotero/storage/4ENA6UIN/generation_strategies.html}
}

@book{torresivinalsPythonDeepLearning2020,
  title = {Python {{Deep Learning}}. {{Introducci{\'o}n}} Pr{\'a}ctica Con {{Keras}} y {{TensorFlow}} 2},
  author = {{Torres i Vi{\~n}als}, Jordi},
  year = {2020},
  edition = {1{\textordfeminine}},
  publisher = {{Marcombo}},
  isbn = {978-84-267-2921-7}
}

@misc{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.03762},
  urldate = {2023-05-23},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/carlos/Zotero/storage/ZUQMSYXF/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/home/carlos/Zotero/storage/E7YRG247/1706.html}
}

@article{wang2021promising,
  title = {A Promising and Challenging Approach: {{Radiologists}}' Perspective on Deep Learning and Artificial Intelligence for Fighting {{COVID-19}}},
  author = {Wang, Tianming and Chen, Zhu and Shang, Quanliang and Ma, Cong and Chen, Xiangyu and Xiao, Enhua},
  year = {2021},
  journal = {Diagnostics},
  volume = {11},
  number = {10},
  pages = {1924},
  publisher = {{Multidisciplinary Digital Publishing Institute}}
}

@misc{wangCostEffectiveHyperparameterOptimization2023,
  title = {Cost-{{Effective Hyperparameter Optimization}} for {{Large Language Model Generation Inference}}},
  author = {Wang, Chi and Liu, Susan Xueqing and Awadallah, Ahmed H.},
  year = {2023},
  month = aug,
  number = {arXiv:2303.04673},
  eprint = {2303.04673},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-11-07},
  abstract = {Large Language Models (LLMs) have sparked significant interest in their generative capabilities, leading to the development of various commercial applications. The high cost of using the models drives application builders to maximize the value of generation under a limited inference budget. This paper presents a study of optimizing inference hyperparameters such as the number of responses, temperature and max tokens, which significantly affects the utility/cost of text generation. We design a framework named EcoOptiGen which leverages economical hyperparameter optimization and cost-based pruning. Experiments with the GPT-3.5/GPT-4 models on a variety of tasks verify its effectiveness. EcoOptiGen is implemented in the `autogen' package of the FLAML library: https: //aka.ms/autogen.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/carlos/Zotero/storage/25YJQ69L/Wang et al. - 2023 - Cost-Effective Hyperparameter Optimization for Lar.pdf}
}

@article{wangHyperparameterOptimizationAlgorithm2022,
  title = {A {{Hyperparameter Optimization Algorithm}} for the {{LSTM Temperature Prediction Model}} in {{Data Center}}},
  author = {Wang, Simin and Ma, Chunmiao and Xu, Yixuan and Wang, Jinyu and Wu, Weiguo},
  year = {2022},
  month = dec,
  journal = {Scientific Programming},
  volume = {2022},
  pages = {e6519909},
  publisher = {{Hindawi}},
  issn = {1058-9244},
  doi = {10.1155/2022/6519909},
  urldate = {2023-11-07},
  abstract = {As the main tool to realize data mining and efficient knowledge acquisition in the era of big data, machine learning is widely used in data center energy-saving research. The temperature prediction model based on machine learning predicts the state of the data center according to the upcoming tasks. It can adjust the refrigeration equipment in advance to avoid temperature regulation lag and set the air conditioning temperature according to the actual demand to avoid excessive refrigeration. Task scheduling and migration algorithm based on temperature prediction can effectively avoid hot spots. However, the choice of hyperparameter of machine learning model has a great impact on its performance. In this study, a hyperparameter optimization algorithm based on MLP is proposed. On the basis of trying certain hyperparameters, the MLP model is used to predict the value of all hyperparameters' space, and then, a certain number of high-quality hyperparameters are selected to train the model repeatedly. In each iteration, the amount of training data decreases gradually, while the accuracy of the model improves rapidly, and finally, the appropriate hyperparameter are obtained. We use the idea of mutation in the genetic algorithm to improve the probability of high-quality solutions and the loss function weighting method to select the solution with the best stability. Experiments are carried out on two representative machine learning models, LSTM and Random Forest, and compared with the standard Gaussian Bayes and Random Search method. The results show that the method proposed in this study can obtain high-precision and high-stability hyperparameter through one run and can greatly improve the operation efficiency. This algorithm is not only effective for LSTM but also suitable for other machine learning models.},
  langid = {english},
  file = {/home/carlos/Zotero/storage/CSFU4NGN/Wang et al. - 2022 - A Hyperparameter Optimization Algorithm for the LS.pdf}
}

@misc{weiChainofThoughtPromptingElicits2023,
  title = {Chain-of-{{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  year = {2023},
  month = jan,
  number = {arXiv:2201.11903},
  eprint = {2201.11903},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-23},
  abstract = {We explore how generating a chain of thought{\textemdash}a series of intermediate reasoning steps{\textemdash}significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-ofthought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/AZPLR9LR/Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf}
}

@misc{weiEmergentAbilitiesLarge2022,
  title = {Emergent {{Abilities}} of {{Large Language Models}}},
  author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  year = {2022},
  month = oct,
  number = {arXiv:2206.07682},
  eprint = {2206.07682},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.07682},
  urldate = {2023-06-27},
  abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/C3F58HUQ/Wei et al. - 2022 - Emergent Abilities of Large Language Models.pdf;/home/carlos/Zotero/storage/SK77QAMT/2206.html}
}

@misc{WhatRetrievalAugmented,
  title = {{What is Retrieval Augmented Generation?}},
  urldate = {2024-01-05},
  abstract = {Retrieval Augmented Generation (RAG) is an approach that combines the power of large-scale neural language models with external retrieval or search mechanisms. Here's a breakdown of RAG in a semi-formal, business tone: Core Concept: At the heart of RAG is the idea of leveraging external knowledge so},
  howpublished = {https://www.linkedin.com/pulse/what-retrieval-augmented-generation-grow-right},
  langid = {spanish},
  file = {/home/carlos/Zotero/storage/A9NJFVUJ/what-retrieval-augmented-generation-grow-right.html}
}

@misc{WhatRetrievalaugmentedGeneration2021,
  title = {What Is Retrieval-Augmented Generation?},
  year = {2021},
  month = feb,
  journal = {IBM Research Blog},
  urldate = {2023-11-07},
  abstract = {RAG is an AI framework for retrieving facts to ground LLMs on the most accurate information and to give users insight into AI's decisionmaking process.},
  copyright = {{\textcopyright} Copyright IBM Corp. 2021},
  howpublished = {https://research.ibm.com/blog/retrieval-augmented-generation-RAG},
  langid = {american},
  file = {/home/carlos/Zotero/storage/S4AMA7JD/retrieval-augmented-generation-RAG.html}
}

@book{wilsonSuperColliderBook2011a,
  title = {The {{SuperCollider Book}}},
  author = {Wilson, Scott and Cottle, David and Collins, Nick},
  year = {2011},
  month = mar,
  publisher = {{The MIT Press}},
  abstract = {SuperCollider is one of the most important domain-specific audio programming languages, with potential applications that include real-time interaction, installations, electroacoustic pieces, generative music, and audiovisuals. The SuperCollider Book is the essential reference to this powerful and flexible language, offering students and professionals a collection of tutorials, essays, and projects. With contributions from top academics, artists, and technologists that cover topics at levels from the introductory to the specialized, it will be a valuable sourcebook both for beginners and for advanced users. SuperCollider, first developed by James McCartney, is an accessible blend of Smalltalk, C, and further ideas from a number of programming languages. Free, open-source, cross-platform, and with a diverse and supportive developer community, it is often the first programming language sound artists and computer musicians learn. The SuperCollider Book is the long-awaited guide to the design, syntax, and use of the SuperCollider language. The first chapters offer an introduction to the basics, including a friendly tutorial for absolute beginners, providing the reader with skills that can serve as a foundation for further learning. Later chapters cover more advanced topics and particular topics in computer music, including programming, sonification, spatialization, microsound, GUIs, machine listening, alternative tunings, and non-real-time synthesis; practical applications and philosophical insigh"s from the composer's and artist's perspectives; and "under the hood," developer's-eye views of SuperCollider's inner workings. A Web site accompanying the book offers code, links to the application itself and its source code, and a variety of third-party extras, extensions, libraries, and examples.},
  isbn = {978-0-262-23269-2}
}

@misc{yangLargeLanguageModels2023,
  title = {Large {{Language Models}} as {{Optimizers}}},
  author = {Yang, Chengrun and Wang, Xuezhi and Lu, Yifeng and Liu, Hanxiao and Le, Quoc V. and Zhou, Denny and Chen, Xinyun},
  year = {2023},
  month = sep,
  number = {arXiv:2309.03409},
  eprint = {2309.03409},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.03409},
  urldate = {2023-10-30},
  abstract = {Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8\% on GSM8K, and by up to 50\% on Big-Bench Hard tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/carlos/Zotero/storage/SLLUN5RF/Yang et al. - 2023 - Large Language Models as Optimizers.pdf;/home/carlos/Zotero/storage/HAGLRUNJ/2309.html}
}

@misc{zengMusicBERTSymbolicMusic2021,
  title = {{{MusicBERT}}: {{Symbolic Music Understanding}} with {{Large-Scale Pre-Training}}},
  shorttitle = {{{MusicBERT}}},
  author = {Zeng, Mingliang and Tan, Xu and Wang, Rui and Ju, Zeqian and Qin, Tao and Liu, Tie-Yan},
  year = {2021},
  month = jun,
  number = {arXiv:2106.05630},
  eprint = {2106.05630},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.05630},
  urldate = {2023-05-27},
  abstract = {Symbolic music understanding, which refers to the understanding of music from the symbolic data (e.g., MIDI format, but not audio), covers many music applications such as genre classification, emotion classification, and music pieces matching. While good music representations are beneficial for these applications, the lack of training data hinders representation learning. Inspired by the success of pre-training models in natural language processing, in this paper, we develop MusicBERT, a large-scale pre-trained model for music understanding. To this end, we construct a large-scale symbolic music corpus that contains more than 1 million music songs. Since symbolic music contains more structural (e.g., bar, position) and diverse information (e.g., tempo, instrument, and pitch), simply adopting the pre-training techniques from NLP to symbolic music only brings marginal gains. Therefore, we design several mechanisms, including OctupleMIDI encoding and bar-level masking strategy, to enhance pre-training with symbolic music data. Experiments demonstrate the advantages of MusicBERT on four music understanding tasks, including melody completion, accompaniment suggestion, genre classification, and style classification. Ablation studies also verify the effectiveness of our designs of OctupleMIDI encoding and bar-level masking strategy in MusicBERT.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Multimedia,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/carlos/Zotero/storage/JL4NSPDK/Zeng et al. - 2021 - MusicBERT Symbolic Music Understanding with Large.pdf;/home/carlos/Zotero/storage/VIRCCY7D/2106.html}
}

@misc{zhouLeasttoMostPromptingEnables2023,
  title = {Least-to-{{Most Prompting Enables Complex Reasoning}} in {{Large Language Models}}},
  author = {Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and Chi, Ed},
  year = {2023},
  month = apr,
  number = {arXiv:2205.10625},
  eprint = {2205.10625},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.10625},
  urldate = {2023-06-25},
  abstract = {Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99\% using just 14 exemplars, compared to only 16\% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/7MQPZXWI/Zhou et al. - 2023 - Least-to-Most Prompting Enables Complex Reasoning .pdf;/home/carlos/Zotero/storage/S48WGUZE/2205.html}
}

@article{zraraPORTFOLIOOPTIMIZATIONUSING2021,
  title = {{{PORTFOLIO OPTIMIZATION USING DEEP LEARNING FOR THE MOROCCAN MARKET}}},
  author = {Zrara, Lamiaa},
  year = {2021},
  month = jan,
  file = {/home/carlos/Zotero/storage/KRZQ9EEH/Zrara - 2021 - PORTFOLIO OPTIMIZATION USING DEEP LEARNING FOR THE.pdf}
}
