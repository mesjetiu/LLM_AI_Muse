@article{abeliukHistoriaEvoluacionInteligencia2021,
  title = {Historia y evoluación de la inteligencia artificial},
  author = {Abeliuk, Andrés and Gutiérrez, Claudio},
  date = {2021-08-03},
  journaltitle = {Revista Bits de Ciencia},
  number = {21},
  pages = {14--21},
  url = {https://revistasdex.uchile.cl/index.php/bits/index},
  urldate = {2023-10-30},
  issue = {21},
  langid = {spanish},
  file = {/home/carlos/Zotero/storage/3XUJEG4L/Abeliuk y Gutiérrez - 2021 - Historia y evoluación de la inteligencia artificia.pdf}
}

@article{alan1950a,
  title = {Computing Machinery and Intelligence},
  author = {Turing, Alan Mathison},
  date = {1950},
  journaltitle = {Mind; a quarterly review of psychology and philosophy},
  shortjournal = {Mind},
  volume = {49},
  pages = {433--460}
}

@online{bowmanEightThingsKnow2023,
  title = {Eight {{Things}} to {{Know}} about {{Large Language Models}}},
  author = {Bowman, Samuel R.},
  date = {2023-04-02},
  eprint = {2304.00612},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.00612},
  url = {http://arxiv.org/abs/2304.00612},
  urldate = {2023-10-23},
  abstract = {The widespread public deployment of large language models (LLMs) in recent months has prompted a wave of new attention and engagement from advocates, policymakers, and scholars from many fields. This attention is a timely response to the many urgent questions that this technology raises, but it can sometimes miss important considerations. This paper surveys the evidence for eight potentially surprising such points: 1. LLMs predictably get more capable with increasing investment, even without targeted innovation. 2. Many important LLM behaviors emerge unpredictably as a byproduct of increasing investment. 3. LLMs often appear to learn and use representations of the outside world. 4. There are no reliable techniques for steering the behavior of LLMs. 5. Experts are not yet able to interpret the inner workings of LLMs. 6. Human performance on a task isn't an upper bound on LLM performance. 7. LLMs need not express the values of their creators nor the values encoded in web text. 8. Brief interactions with LLMs are often misleading.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/RDHLL7B3/Bowman - 2023 - Eight Things to Know about Large Language Models.pdf;/home/carlos/Zotero/storage/HETIVTEB/2304.html}
}

@online{bretanUnitSelectionMethodology2016,
  title = {A {{Unit Selection Methodology}} for {{Music Generation Using Deep Neural Networks}}},
  author = {Bretan, Mason and Weinberg, Gil and Heck, Larry},
  date = {2016-12-12},
  eprint = {1612.03789},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1612.03789},
  url = {http://arxiv.org/abs/1612.03789},
  urldate = {2023-05-24},
  abstract = {Several methods exist for a computer to generate music based on data including Markov chains, recurrent neural networks, recombinancy, and grammars. We explore the use of unit selection and concatenation as a means of generating music using a procedure based on ranking, where, we consider a unit to be a variable length number of measures of music. We first examine whether a unit selection method, that is restricted to a finite size unit library, can be sufficient for encompassing a wide spectrum of music. We do this by developing a deep autoencoder that encodes a musical input and reconstructs the input by selecting from the library. We then describe a generative model that combines a deep structured semantic model (DSSM) with an LSTM to predict the next unit, where units consist of four, two, and one measures of music. We evaluate the generative model using objective metrics including mean rank and accuracy and with a subjective listening test in which expert musicians are asked to complete a forced-choiced ranking task. We compare our model to a note-level generative baseline that consists of a stacked LSTM trained to predict forward by one note.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Sound},
  file = {/home/carlos/Zotero/storage/EHW5C3CU/Bretan et al. - 2016 - A Unit Selection Methodology for Music Generation .pdf;/home/carlos/Zotero/storage/E2D7NJMW/1612.html}
}

@online{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020-07-22},
  eprint = {2005.14165},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2005.14165},
  url = {http://arxiv.org/abs/2005.14165},
  urldate = {2023-06-29},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/4ZY5M7ZW/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf;/home/carlos/Zotero/storage/PSWS6S3E/2005.html}
}

@online{butlinConsciousnessArtificialIntelligence2023,
  title = {Consciousness in {{Artificial Intelligence}}: {{Insights}} from the {{Science}} of {{Consciousness}}},
  shorttitle = {Consciousness in {{Artificial Intelligence}}},
  author = {Butlin, Patrick and Long, Robert and Elmoznino, Eric and Bengio, Yoshua and Birch, Jonathan and Constant, Axel and Deane, George and Fleming, Stephen M. and Frith, Chris and Ji, Xu and Kanai, Ryota and Klein, Colin and Lindsay, Grace and Michel, Matthias and Mudrik, Liad and Peters, Megan A. K. and Schwitzgebel, Eric and Simon, Jonathan and VanRullen, Rufin},
  date = {2023-08-22},
  eprint = {2308.08708},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  doi = {10.48550/arXiv.2308.08708},
  url = {http://arxiv.org/abs/2308.08708},
  urldate = {2023-09-27},
  abstract = {Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive "indicator properties" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also suggests that there are no obvious technical barriers to building AI systems which satisfy these indicators.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  file = {/home/carlos/Zotero/storage/3YM2L8NV/Butlin et al. - 2023 - Consciousness in Artificial Intelligence Insights.pdf}
}

@article{campbellDeepBlue2002,
  title = {Deep {{Blue}}},
  author = {Campbell, Murray and Hoane, A. Joseph and Hsu, Feng-hsiung},
  date = {2002-01-01},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {134},
  number = {1},
  pages = {57--83},
  issn = {0004-3702},
  doi = {10.1016/S0004-3702(01)00129-1},
  url = {https://www.sciencedirect.com/science/article/pii/S0004370201001291},
  urldate = {2023-10-29},
  abstract = {Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including: •a single-chip chess search engine,•a massively parallel system with multiple levels of parallelism,•a strong emphasis on search extensions,•a complex evaluation function, and•effective use of a Grandmaster game database. This paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue.},
  keywords = {Computer chess,Evaluation function,Game tree search,Parallel search,Search extensions,Selective search},
  file = {/home/carlos/Zotero/storage/IAV98XWL/Campbell et al. - 2002 - Deep Blue.pdf;/home/carlos/Zotero/storage/ARPI4JK5/S0004370201001291.html}
}

@online{chenTeachingLargeLanguage2023,
  title = {Teaching {{Large Language Models}} to {{Self-Debug}}},
  author = {Chen, Xinyun and Lin, Maxwell and Schärli, Nathanael and Zhou, Denny},
  date = {2023-10-05},
  eprint = {2304.05128},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.05128},
  url = {http://arxiv.org/abs/2304.05128},
  urldate = {2023-10-23},
  abstract = {Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3\%, and improves the prediction accuracy on problems of the hardest level by 9\%. On TransCoder and MBPP where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12\%. Meanwhile, by leveraging feedback messages and reusing failed predictions, Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/7APGY9B9/Chen et al. - 2023 - Teaching Large Language Models to Self-Debug.pdf;/home/carlos/Zotero/storage/B54ZXYI4/2304.html}
}

@online{CodeLlamaOpen,
  title = {Code {{Llama}}: {{Open Foundation Models}} for {{Code}} | {{Research}} - {{AI}} at {{Meta}}},
  url = {https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/},
  urldate = {2023-09-27},
  file = {/home/carlos/Zotero/storage/RIT3IM2P/code-llama-open-foundation-models-for-code.html}
}

@inproceedings{crookConversationalSemanticSearch2018,
  title = {Conversational {{Semantic Search}}: {{Looking BeyondWeb Search}}, {{Q}}\&{{A}} and {{Dialog Systems}}},
  shorttitle = {Conversational {{Semantic Search}}},
  author = {Crook, Paul A. and Marin, Alex and Agarwal, Vipul and Anderson, Samantha and Jang, Ohyoung and Lanewala, Aliasgar and Tangirala, Karthik and Zitouni, Imed},
  date = {2018-02-05},
  url = {https://www.microsoft.com/en-us/research/publication/conversational-semantic-search-looking-beyondweb-search-qa-and-dialog-systems/},
  urldate = {2023-07-06},
  abstract = {User expectations of web search are changing. They are expecting search engines to answer questions, to be more conversational, and to offer means to complete tasks on their behalf. At the same time, to increase the breadth of tasks that personal digital assistants (PDAs), such as Microsoft’s Cortana or Amazon’s Alexa, are capable of, PDAs […]},
  eventtitle = {The 11th {{ACM International Conference}} on {{Web Search}} and {{Data Minining}} ({{WSDM}} 2018)},
  langid = {american},
  file = {/home/carlos/Zotero/storage/XS5AUBND/Crook et al. - 2018 - Conversational Semantic Search Looking BeyondWeb .pdf}
}

@online{dhuliawalaChainofVerificationReducesHallucination2023,
  title = {Chain-of-{{Verification Reduces Hallucination}} in {{Large Language Models}}},
  author = {Dhuliawala, Shehzaad and Komeili, Mojtaba and Xu, Jing and Raileanu, Roberta and Li, Xian and Celikyilmaz, Asli and Weston, Jason},
  date = {2023-09-25},
  eprint = {2309.11495},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.11495},
  url = {http://arxiv.org/abs/2309.11495},
  urldate = {2023-10-30},
  abstract = {Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/JLP8R4IU/Dhuliawala et al. - 2023 - Chain-of-Verification Reduces Hallucination in Lar.pdf;/home/carlos/Zotero/storage/5L75C6YB/2309.html}
}

@online{douglasLargeLanguageModels2023,
  title = {Large {{Language Models}}},
  author = {Douglas, Michael R.},
  date = {2023-07-11},
  eprint = {2307.05782},
  eprinttype = {arxiv},
  eprintclass = {hep-th, physics:physics},
  doi = {10.48550/arXiv.2307.05782},
  url = {http://arxiv.org/abs/2307.05782},
  urldate = {2023-09-27},
  abstract = {Artificial intelligence is making spectacular progress, and one of the best examples is the development of large language models (LLMs) such as OpenAI's GPT series. In these lectures, written for readers with a background in mathematics or physics, we give a brief history and survey of the state of the art, and describe the underlying transformer architecture in detail. We then explore some current ideas on how LLMs work and how models trained to predict the next word in a text are able to perform other tasks displaying intelligence.},
  pubstate = {preprint},
  keywords = {68T01,Computer Science - Computation and Language,High Energy Physics - Theory,I.2.7,Mathematics - History and Overview,Physics - Computational Physics},
  file = {/home/carlos/Zotero/storage/MTB9SGS5/Douglas - 2023 - Large Language Models.pdf;/home/carlos/Zotero/storage/Y46IYSSF/2307.html}
}

@inproceedings{ganguliPredictabilitySurpriseLarge2022,
  title = {Predictability and {{Surprise}} in {{Large Generative Models}}},
  booktitle = {2022 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Ganguli, Deep and Hernandez, Danny and Lovitt, Liane and DasSarma, Nova and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Kernion, Jackson and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Elhage, Nelson and Showk, Sheer El and Fort, Stanislav and Hatfield-Dodds, Zac and Johnston, Scott and Kravec, Shauna and Nanda, Neel and Ndousse, Kamal and Olsson, Catherine and Amodei, Daniela and Amodei, Dario and Brown, Tom and Kaplan, Jared and McCandlish, Sam and Olah, Chris and Clark, Jack},
  date = {2022-06-21},
  eprint = {2202.07785},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {1747--1764},
  doi = {10.1145/3531146.3533229},
  url = {http://arxiv.org/abs/2202.07785},
  urldate = {2023-06-27},
  abstract = {Large-scale pre-training has recently emerged as a technique for creating capable, general purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have an unusual combination of predictable loss on a broad training distribution (as embodied in their "scaling laws"), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, and academics who want to analyze, critique, and potentially develop large generative models.},
  langid = {english},
  keywords = {Computer Science - Computers and Society},
  file = {/home/carlos/Zotero/storage/93WQ2BRW/Ganguli et al. - 2022 - Predictability and Surprise in Large Generative Mo.pdf}
}

@online{GenerationLLMs,
  title = {Generation with {{LLMs}}},
  url = {https://huggingface.co/docs/transformers/main/en/llm_tutorial},
  urldate = {2023-10-31},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {/home/carlos/Zotero/storage/Q83ZWSQC/llm_tutorial.html}
}

@book{Goodfellow-et-al-2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  date = {2016},
  publisher = {{MIT Press}}
}

@online{gunasekarTextbooksAreAll2023,
  title = {Textbooks {{Are All You Need}}},
  author = {Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio César Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and family=Rosa, given=Gustavo, prefix=de, useprefix=true and Saarikivi, Olli and Salim, Adil and Shah, Shital and Behl, Harkirat Singh and Wang, Xin and Bubeck, Sébastien and Eldan, Ronen and Kalai, Adam Tauman and Lee, Yin Tat and Li, Yuanzhi},
  date = {2023-06-20},
  eprint = {2306.11644},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.11644},
  url = {http://arxiv.org/abs/2306.11644},
  urldate = {2023-06-29},
  abstract = {We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6\% on HumanEval and 55.5\% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45\% on HumanEval.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/carlos/Zotero/storage/IMBD7FR8/Gunasekar et al. - 2023 - Textbooks Are All You Need.pdf;/home/carlos/Zotero/storage/PP9DGAW6/2306.html}
}

@online{hernandez-olivanSurveyArtificialIntelligence2022,
  title = {A {{Survey}} on {{Artificial Intelligence}} for {{Music Generation}}: {{Agents}}, {{Domains}} and {{Perspectives}}},
  shorttitle = {A {{Survey}} on {{Artificial Intelligence}} for {{Music Generation}}},
  author = {Hernandez-Olivan, Carlos and Hernandez-Olivan, Javier and Beltran, Jose R.},
  date = {2022-11-03},
  eprint = {2210.13944},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  doi = {10.48550/arXiv.2210.13944},
  url = {http://arxiv.org/abs/2210.13944},
  urldate = {2023-09-27},
  abstract = {Music is one of the Gardner's intelligences in his theory of multiple intelligences. How humans perceive and understand music is still being studied and is crucial to develop artificial intelligence models that imitate such processes. Music generation with Artificial Intelligence is an emerging field that is gaining much attention in the recent years. In this paper, we describe how humans compose music and how new AI systems could imitate such process by comparing past and recent advances in the field with music composition techniques. To understand how AI models and algorithms generate music and the potential applications that might appear in the future, we explore, analyze and describe the agents that take part of the music generation process: the datasets, models, interfaces, the users and the generated music. We mention possible applications that might benefit from this field and we also propose new trends and future research directions that could be explored in the future.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/carlos/Zotero/storage/498MZNDW/Hernandez-Olivan et al. - 2022 - A Survey on Artificial Intelligence for Music Gene.pdf;/home/carlos/Zotero/storage/AMJVURKG/2210.html}
}

@online{HistoriaInteligenciaArtificial,
  title = {Historia de la Inteligencia Artificial, el Machine Learning y el Deep Learning},
  url = {https://www.algotive.ai/es-mx/blog/historia-de-la-inteligencia-artificial-el-machine-learning-y-el-deep-learning},
  urldate = {2023-10-30},
  abstract = {Conoce toda la historia de la Inteligencia Artificial (IA), el Machine Learning (ML) y el Deep Learning (DL), de manera breve, sencilla e ilustrativa.},
  langid = {mexican},
  file = {/home/carlos/Zotero/storage/LU4PW86U/historia-de-la-inteligencia-artificial-el-machine-learning-y-el-deep-learning.html}
}

@online{jzh2074,
  title = {{{AI}} Types. {{Tipos}} Inteligencia {{Artificial}}.Svg},
  author = {{Jzh2074}},
  date = {2022},
  url = {https://commons.wikimedia.org/wiki/File:AI_Types._Tipos_Inteligencia_Artificial.svg}
}

@online{kaddourChallengesApplicationsLarge2023,
  title = {Challenges and {{Applications}} of {{Large Language Models}}},
  author = {Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and Bradley, Herbie and Raileanu, Roberta and McHardy, Robert},
  date = {2023-07-19},
  eprint = {2307.10169},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.10169},
  url = {http://arxiv.org/abs/2307.10169},
  urldate = {2023-09-27},
  abstract = {Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field's current state more quickly and become productive.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/carlos/Zotero/storage/N4TI6Q5V/Kaddour et al. - 2023 - Challenges and Applications of Large Language Mode.pdf;/home/carlos/Zotero/storage/EWGBTNGU/2307.html}
}

@online{liSelfAlignmentInstructionBacktranslation2023,
  title = {Self-{{Alignment}} with {{Instruction Backtranslation}}},
  author = {Li, Xian and Yu, Ping and Zhou, Chunting and Schick, Timo and Zettlemoyer, Luke and Levy, Omer and Weston, Jason and Lewis, Mike},
  date = {2023-08-14},
  eprint = {2308.06259},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.06259},
  url = {http://arxiv.org/abs/2308.06259},
  urldate = {2023-09-27},
  abstract = {We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/JADHWQUA/Li et al. - 2023 - Self-Alignment with Instruction Backtranslation.pdf;/home/carlos/Zotero/storage/4SRRSDIW/2308.html}
}

@online{liStructuredChainofThoughtPrompting2023,
  title = {Structured {{Chain-of-Thought Prompting}} for {{Code Generation}}},
  author = {Li, Jia and Li, Ge and Li, Yongmin and Jin, Zhi},
  date = {2023-09-07},
  eprint = {2305.06599},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.06599},
  urldate = {2023-10-23},
  abstract = {Large Language Models (LLMs) (e.g., ChatGPT) have shown impressive performance in code generation. LLMs take prompts as inputs, and Chain-of-Thought (CoT) prompting is the state-of-theart prompting technique. CoT prompting asks LLMs first to generate CoTs (i.e., intermediate natural language reasoning steps) and then output the code. However, CoT prompting is designed for natural language generation and has low accuracy in code generation.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Software Engineering},
  file = {/home/carlos/Zotero/storage/KQ5L5V5X/Li et al. - 2023 - Structured Chain-of-Thought Prompting for Code Gen.pdf}
}

@online{LLMPromptingGuide,
  title = {{{LLM}} Prompting Guide},
  url = {https://huggingface.co/docs/transformers/main/en/tasks/prompting},
  urldate = {2023-10-30},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {/home/carlos/Zotero/storage/MLZ8EYCS/prompting.html}
}

@article{mukherjeeOrcaProgressiveLearning2023,
  title = {Orca: {{Progressive Learning}} from {{Complex Explanation Traces}} of {{GPT-4}}},
  shorttitle = {Orca},
  author = {Mukherjee, Subhabrata (Subho) and Mitra, Arindam and Jawahar, Ganesh and Agarwal, Sahaj and Palangi, Hamid and Awadallah, Ahmed H.},
  date = {2023-06-01},
  url = {https://www.microsoft.com/en-us/research/publication/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4/},
  urldate = {2023-06-25},
  abstract = {Recent research has focused on enhancing the capability of smaller models through imitation~learning, drawing on the outputs generated by large foundation models (LFMs). A number of issues impact the quality of these models, ranging from~limited imitation signals~from shallow LFM outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting in […]},
  langid = {american},
  file = {/home/carlos/Zotero/storage/ANQ2XMEH/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4.html}
}

@article{nielsenNeuralNetworksDeep2015,
  title = {Neural {{Networks}} and {{Deep Learning}}},
  author = {Nielsen, Michael A.},
  date = {2015},
  publisher = {{Determination Press}},
  url = {http://neuralnetworksanddeeplearning.com},
  urldate = {2023-05-24},
  langid = {english},
  file = {/home/carlos/Zotero/storage/TZXGVB5T/index.html}
}

@book{penroseNuevaMenteEmperador2015,
  title = {La nueva mente del emperador},
  author = {Penrose, Roger},
  date = {2015-07-02},
  eprint = {sLz3CQAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Penguin Random House Grupo Editorial España}},
  abstract = {Un apasionante paseo por la matemática y la física, y por los hallazgos del pensamiento humano de la mano de Roger Penrose, Premio Nobel de Física 2020. Durante décadas, los defensores de la inteligencia artificial han mantenido que los ordenadores harán pronto todo lo que la mente humana puede hacer. En su favor, se puede utilizar, por ejemplo, el que ya hay máquinas que juegan al ajedrez como los grandes maestros. Ahora bien, ¿comprenden el juego como lo hacemos nosotros?En este libro, Roger Penrose, probablemente el especialista en la teoría general de la relatividad más prestigioso del mundo y una de las mentes analíticas más originales de la actualidad, sostiene que existen facetas del pensamiento humano que nunca serán emuladas por un ordenador. Para defender esa tesis, Penrose recurre a una amplia gama de conocimientos científicos, que van desde la máquina de Turing hasta la estructura del cerebro, pasando por el teorema de Gödel, los agujeros negros y los blancos, la radiación de Hawking, la entropía o la mecánica cuántica. Entre los numerosos estudios existentes dedicados a la relación entre la mente y el cuerpo, esta ambiciosa obra sobresale tanto por su lucidez y claridad como por su rigor y profundidad. Reseña:«Un libro audaz, brillante e innovador. Cuando Penrose habla, los científicos escuchan.»The New York Times Book Review},
  isbn = {978-84-663-3083-1},
  langid = {spanish},
  pagetotal = {870},
  keywords = {Computers / Artificial Intelligence / General,Science / Essays,Science / General,Science / Life Sciences / Neuroscience}
}

@online{radovanovicEmergingArchitecturesLLM2023,
  title = {Emerging {{Architectures}} for {{LLM Applications}}},
  author = {Radovanovic, Rajko, Matt Bornstein},
  date = {2023-06-20T19:23:48+00:00},
  url = {https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/},
  urldate = {2023-06-27},
  abstract = {A reference architecture for the LLM app stack. It shows the most common systems, tools, and design patterns used by AI startups and tech companies.},
  langid = {american},
  organization = {{Andreessen Horowitz}},
  file = {/home/carlos/Zotero/storage/PAWYWIEW/emerging-architectures-for-llm-applications.html}
}

@online{RevistaBitsCiencia,
  title = {Revista Bits de Ciencia N° 21 - 2021},
  url = {https://revistasdex.uchile.cl/files/full/BitsdeCiencia_(21)_2021/index.html},
  urldate = {2023-10-30},
  langid = {spanish},
  file = {/home/carlos/Zotero/storage/6QXLYRRD/index.html}
}

@book{RussellStuartJ2021AI:A,
  title = {Artificial Intelligence : {{A}} Modern Approach},
  author = {Russell, Stuart J},
  date = {2021},
  edition = {4th Edition.; Global edition.},
  publisher = {{Pearson}},
  location = {{Harlow}},
  isbn = {978-0-13-461099-3},
  langid = {english},
  keywords = {Inteligencia artificial,Inteligencia artificial Robótica,Transferencia del conocimiento}
}

@online{schaefferAreEmergentAbilities2023,
  title = {Are {{Emergent Abilities}} of {{Large Language Models}} a {{Mirage}}?},
  author = {Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  date = {2023-05-22},
  eprint = {2304.15004},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2304.15004},
  urldate = {2023-10-23},
  abstract = {Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, emergent abilities appear due the researcher’s choice of metric rather than due to fundamental changes in model behavior with scale. Specifically, nonlinear or discontinuous metrics produce apparent emergent abilities, whereas linear or continuous metrics produce smooth, continuous, predictable changes in model performance. We present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities, (2) make, test and confirm two predictions about metric choices in a metaanalysis of emergent abilities on BIG-Bench; and (3) show how to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks. Via all three analyses, we provide evidence that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/carlos/Zotero/storage/IKAF8WT5/Schaeffer et al. - 2023 - Are Emergent Abilities of Large Language Models a .pdf}
}

@online{selAlgorithmThoughtsEnhancing2023,
  title = {Algorithm of {{Thoughts}}: {{Enhancing Exploration}} of {{Ideas}} in {{Large Language Models}}},
  shorttitle = {Algorithm of {{Thoughts}}},
  author = {Sel, Bilgehan and Al-Tawaha, Ahmad and Khattar, Vanshaj and Wang, Lu and Jia, Ruoxi and Jin, Ming},
  date = {2023-08-20},
  eprint = {2308.10379},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.10379},
  url = {http://arxiv.org/abs/2308.10379},
  urldate = {2023-09-27},
  abstract = {Current literature, aiming to surpass the "Chain-of-Thought" approach, often resorts to an external modus operandi involving halting, modifying, and then resuming the generation process to boost Large Language Models' (LLMs) reasoning capacities. This mode escalates the number of query requests, leading to increased costs, memory, and computational overheads. Addressing this, we propose the Algorithm of Thoughts -- a novel strategy that propels LLMs through algorithmic reasoning pathways, pioneering a new mode of in-context learning. By employing algorithmic examples, we exploit the innate recurrence dynamics of LLMs, expanding their idea exploration with merely one or a few queries. Our technique outperforms earlier single-query methods and stands on par with a recent multi-query strategy that employs an extensive tree search algorithm. Intriguingly, our results suggest that instructing an LLM using an algorithm can lead to performance surpassing that of the algorithm itself, hinting at LLM's inherent ability to weave its intuition into optimized searches. We probe into the underpinnings of our method's efficacy and its nuances in application.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/RUC6VMXG/Sel et al. - 2023 - Algorithm of Thoughts Enhancing Exploration of Id.pdf;/home/carlos/Zotero/storage/DMUQY952/2308.html}
}

@article{shannon1951prediction,
  title = {Prediction and Entropy of Printed English},
  author = {Shannon, Claude Elwood},
  date = {1951-01},
  journaltitle = {Bell System Technical Journal},
  volume = {30},
  pages = {50--64},
  url = {http://languagelog.ldc.upenn.edu/myl/Shannon1950.pdf},
  added-at = {2013-02-27T08:26:04.000+0100},
  interhash = {daabc21c7f6e71f6e78a10c8d3492927},
  intrahash = {2e79cf0f6022645a632b13e081b0b035},
  keywords = {communication english entropy information prediction shannon toread},
  timestamp = {2014-07-28T15:57:31.000+0200}
}

@online{shevlaneModelEvaluationExtreme2023,
  title = {Model Evaluation for Extreme Risks},
  author = {Shevlane, Toby and Farquhar, Sebastian and Garfinkel, Ben and Phuong, Mary and Whittlestone, Jess and Leung, Jade and Kokotajlo, Daniel and Marchal, Nahema and Anderljung, Markus and Kolt, Noam and Ho, Lewis and Siddarth, Divya and Avin, Shahar and Hawkins, Will and Kim, Been and Gabriel, Iason and Bolina, Vijay and Clark, Jack and Bengio, Yoshua and Christiano, Paul and Dafoe, Allan},
  date = {2023-05-24},
  eprint = {2305.15324},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.15324},
  urldate = {2023-06-27},
  abstract = {Current approaches to building general-purpose AI systems tend to produce systems with both beneficial and harmful capabilities. Further progress in AI development could lead to capabilities that pose extreme risks, such as offensive cyber capabilities or strong manipulation skills. We explain why model evaluation is critical for addressing extreme risks. Developers must be able to identify dangerous capabilities (through "dangerous capability evaluations") and the propensity of models to apply their capabilities for harm (through "alignment evaluations"). These evaluations will become critical for keeping policymakers and other stakeholders informed, and for making responsible decisions about model training, deployment, and security.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,K.4.1},
  file = {/home/carlos/Zotero/storage/BDHEV472/Shevlane et al. - 2023 - Model evaluation for extreme risks.pdf}
}

@online{StableAudioFast,
  title = {Stable {{Audio}}: {{Fast Timing-Conditioned Latent Audio Diffusion}}},
  shorttitle = {Stable {{Audio}}},
  url = {https://stability.ai/research/stable-audio-efficient-timing-latent-diffusion},
  urldate = {2023-09-27},
  abstract = {Stable Audio represents the cutting-edge audio generation research by Stability AI’s generative audio research lab, Harmonai. We continue to improve our model architectures, datasets, and training procedures to improve output quality, controllability, inference speed, and output length.},
  langid = {british},
  organization = {{Stability AI}},
  file = {/home/carlos/Zotero/storage/HE8PHSK2/stable-audio-efficient-timing-latent-diffusion.html}
}

@online{TextGenerationStrategies,
  title = {Text Generation Strategies},
  url = {https://huggingface.co/docs/transformers/main/en/generation_strategies},
  urldate = {2023-10-31},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {/home/carlos/Zotero/storage/4ENA6UIN/generation_strategies.html}
}

@book{torresivinalsPythonDeepLearning2020,
  title = {Python {{Deep Learning}}. {{Introducción}} Práctica Con {{Keras}} y {{TensorFlow}} 2},
  author = {Torres i Viñals, Jordi},
  date = {2020},
  edition = {1ª},
  publisher = {{Marcombo}},
  isbn = {978-84-267-2921-7}
}

@online{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017-12-05},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2023-05-23},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/carlos/Zotero/storage/ZUQMSYXF/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/home/carlos/Zotero/storage/E7YRG247/1706.html}
}

@article{wang2021promising,
  title = {A Promising and Challenging Approach: {{Radiologists}}’ Perspective on Deep Learning and Artificial Intelligence for Fighting {{COVID-19}}},
  author = {Wang, Tianming and Chen, Zhu and Shang, Quanliang and Ma, Cong and Chen, Xiangyu and Xiao, Enhua},
  date = {2021},
  journaltitle = {Diagnostics},
  volume = {11},
  number = {10},
  pages = {1924},
  publisher = {{Multidisciplinary Digital Publishing Institute}}
}

@online{weiChainofThoughtPromptingElicits2023,
  title = {Chain-of-{{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  date = {2023-01-10},
  eprint = {2201.11903},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2201.11903},
  urldate = {2023-10-23},
  abstract = {We explore how generating a chain of thought—a series of intermediate reasoning steps—significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-ofthought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/AZPLR9LR/Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf}
}

@online{weiEmergentAbilitiesLarge2022,
  title = {Emergent {{Abilities}} of {{Large Language Models}}},
  author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  date = {2022-10-26},
  eprint = {2206.07682},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.07682},
  url = {http://arxiv.org/abs/2206.07682},
  urldate = {2023-06-27},
  abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/C3F58HUQ/Wei et al. - 2022 - Emergent Abilities of Large Language Models.pdf;/home/carlos/Zotero/storage/SK77QAMT/2206.html}
}

@online{yangLargeLanguageModels2023,
  title = {Large {{Language Models}} as {{Optimizers}}},
  author = {Yang, Chengrun and Wang, Xuezhi and Lu, Yifeng and Liu, Hanxiao and Le, Quoc V. and Zhou, Denny and Chen, Xinyun},
  date = {2023-09-06},
  eprint = {2309.03409},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.03409},
  url = {http://arxiv.org/abs/2309.03409},
  urldate = {2023-10-30},
  abstract = {Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8\% on GSM8K, and by up to 50\% on Big-Bench Hard tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/carlos/Zotero/storage/SLLUN5RF/Yang et al. - 2023 - Large Language Models as Optimizers.pdf;/home/carlos/Zotero/storage/HAGLRUNJ/2309.html}
}

@online{zengMusicBERTSymbolicMusic2021,
  title = {{{MusicBERT}}: {{Symbolic Music Understanding}} with {{Large-Scale Pre-Training}}},
  shorttitle = {{{MusicBERT}}},
  author = {Zeng, Mingliang and Tan, Xu and Wang, Rui and Ju, Zeqian and Qin, Tao and Liu, Tie-Yan},
  date = {2021-06-10},
  eprint = {2106.05630},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  doi = {10.48550/arXiv.2106.05630},
  url = {http://arxiv.org/abs/2106.05630},
  urldate = {2023-05-27},
  abstract = {Symbolic music understanding, which refers to the understanding of music from the symbolic data (e.g., MIDI format, but not audio), covers many music applications such as genre classification, emotion classification, and music pieces matching. While good music representations are beneficial for these applications, the lack of training data hinders representation learning. Inspired by the success of pre-training models in natural language processing, in this paper, we develop MusicBERT, a large-scale pre-trained model for music understanding. To this end, we construct a large-scale symbolic music corpus that contains more than 1 million music songs. Since symbolic music contains more structural (e.g., bar, position) and diverse information (e.g., tempo, instrument, and pitch), simply adopting the pre-training techniques from NLP to symbolic music only brings marginal gains. Therefore, we design several mechanisms, including OctupleMIDI encoding and bar-level masking strategy, to enhance pre-training with symbolic music data. Experiments demonstrate the advantages of MusicBERT on four music understanding tasks, including melody completion, accompaniment suggestion, genre classification, and style classification. Ablation studies also verify the effectiveness of our designs of OctupleMIDI encoding and bar-level masking strategy in MusicBERT.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Multimedia,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/carlos/Zotero/storage/JL4NSPDK/Zeng et al. - 2021 - MusicBERT Symbolic Music Understanding with Large.pdf;/home/carlos/Zotero/storage/VIRCCY7D/2106.html}
}

@online{zhouLeasttoMostPromptingEnables2023,
  title = {Least-to-{{Most Prompting Enables Complex Reasoning}} in {{Large Language Models}}},
  author = {Zhou, Denny and Schärli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and Chi, Ed},
  date = {2023-04-16},
  eprint = {2205.10625},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2205.10625},
  url = {http://arxiv.org/abs/2205.10625},
  urldate = {2023-06-25},
  abstract = {Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99\% using just 14 exemplars, compared to only 16\% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/7MQPZXWI/Zhou et al. - 2023 - Least-to-Most Prompting Enables Complex Reasoning .pdf;/home/carlos/Zotero/storage/S48WGUZE/2205.html}
}
