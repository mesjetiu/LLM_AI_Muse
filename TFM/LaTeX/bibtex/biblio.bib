@article{abeliukHistoriaEvoluacionInteligencia2021,
  title = {{Historia y evoluaci{\'o}n de la inteligencia artificial}},
  author = {Abeliuk, Andr{\'e}s and Guti{\'e}rrez, Claudio},
  year = {2021},
  month = aug,
  journal = {Revista Bits de Ciencia},
  number = {21},
  pages = {14--21},
  url = {https://revistasdex.uchile.cl/index.php/bits/index},
  urldate = {2023-10-30},
  copyright = {Derechos de autor 2021 Revista Bits de Ciencia},
  language = {es},
  file = {/home/carlos/Zotero/storage/3XUJEG4L/Abeliuk y Gutiérrez - 2021 - Historia y evoluación de la inteligencia artificia.pdf}
}

@misc{AGIEdgerunnersLLMAgentsPapers2024,
  title = {{{AGI-Edgerunners}}/{{LLM-Agents-Papers}}},
  year = {2024},
  month = jan,
  url = {https://github.com/AGI-Edgerunners/LLM-Agents-Papers},
  urldate = {2024-01-02},
  abstract = {A repo lists papers related to LLM based agent},
  howpublished = {AGI-Edgerunners},
  keywords = {agents,large-language-models,llm-agent,paper-list}
}

@article{alan1950a,
  title = {Computing Machinery and Intelligence},
  author = {Turing, Alan Mathison},
  year = {1950},
  journal = {Mind; a quarterly review of psychology and philosophy},
  volume = {49},
  pages = {433--460}
}

@article{Algorave2023,
  title = {{Algorave}},
  year = {2023},
  month = nov,
  journal = {Wikipedia, la enciclopedia libre},
  url = {https://es.wikipedia.org/w/index.php?title=Algorave&oldid=155173649},
  urldate = {2024-02-08},
  abstract = {Una algorave (de algoritmo y rave) es un evento en el que la gente baila con m{\'u}sica generada a partir de algoritmos, a menudo utilizando t{\'e}cnicas de codificaci{\'o}n en vivo.[1]\hspace{0pt} Alex McLean de Slub y Nick Collins acu{\~n}aron la palabra "algorave" en 2011, y el primer evento con ese nombre se organiz{\'o} en Londres, Reino Unido.[2]\hspace{0pt} Desde entonces se ha convertido en un movimiento, con algoraves que tienen lugar en todo el mundo.[3]\hspace{0pt}[4]\hspace{0pt}},
  copyright = {Creative Commons Attribution-ShareAlike License},
  language = {es},
  annotation = {Page Version ID: 155173649},
  file = {/home/carlos/Zotero/storage/G8JCPP7I/Algorave.html}
}

@misc{almazroueiFalconSeriesOpen2023,
  title = {The {{Falcon Series}} of {{Open Language Models}}},
  author = {Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, M{\'e}rouane and Goffinet, {\'E}tienne and Hesslow, Daniel and Launay, Julien and Malartic, Quentin and Mazzotta, Daniele and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},
  year = {2023},
  month = nov,
  number = {arXiv:2311.16867},
  eprint = {2311.16867},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2311.16867},
  url = {http://arxiv.org/abs/2311.16867},
  urldate = {2024-01-28},
  abstract = {We introduce the Falcon series: 7B, 40B, and 180B parameters causal decoder-only models trained on a diverse high-quality corpora predominantly assembled from web data. The largest model, Falcon-180B, has been trained on over 3.5 trillion tokens of text--the largest openly documented pretraining run. Falcon-180B significantly outperforms models such as PaLM or Chinchilla, and improves upon concurrently developed models such as LLaMA 2 or Inflection-1. It nears the performance of PaLM-2-Large at a reduced pretraining and inference cost, making it, to our knowledge, one of the three best language models in the world along with GPT-4 and PaLM-2-Large. We report detailed evaluations, as well as a deep dive into the methods and custom tooling employed to pretrain Falcon. Notably, we report on our custom distributed training codebase, allowing us to efficiently pretrain these models on up to 4,096 A100s on cloud AWS infrastructure with limited interconnect. We release a 600B tokens extract of our web dataset, as well as the Falcon-7/40/180B models under a permissive license to foster open-science and accelerate the development of an open ecosystem of large language models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/RS363NL2/Almazrouei et al. - 2023 - The Falcon Series of Open Language Models.pdf;/home/carlos/Zotero/storage/TWRD9XPB/2311.html}
}

@article{arizaTwoPioneeringProjects2011,
  title = {Two {{Pioneering Projects}} from the {{Early History}} of {{Computer-Aided Algorithmic Composition}}},
  author = {Ariza, Christopher},
  year = {2011},
  month = sep,
  journal = {MIT Press},
  publisher = {{MIT Press}},
  issn = {0148-9267},
  url = {https://dspace.mit.edu/handle/1721.1/68626},
  urldate = {2023-12-26},
  abstract = {Lejaren Hiller's 1970 chapter, "Music Composed  with Computers: An Historical Survey" (Hiller  1970) contains numerous descriptions of projects  in the computer generation of musical structures.  By then, just over ten years after the publication  of Hiller's and Leonard Isaacson's seminal book  Experimental Music (Hiller and Isaacson 1959), a  startling number of experiments in generativemusic  with early computers had been completed. Hiller's  early research, compositions, and publications  established him as a leader in the then-emerging  field of computer-aided algorithmic composition  (CAAC). Some researchers, likely inspired by Hiller  and Isaacson's 1956 Illiac Suite string quartet, even  duplicated their instrumentation: in an amusing  footnote, Hiller writes that "it is curious to note  how many computer pieces have been written for  string quartet . . . particularly since string-quartet  performers seem to be among the least receptive to  newer compositional ideas such as computermusic"  (Hiller 1970, p. 70).},
  copyright = {Article is made available in accordance with the publisher's policy and may be subject to US copyright law. Please refer to the publisher's site for terms of use.},
  language = {en\_US},
  annotation = {Accepted: 2012-01-20T20:32:35Z},
  file = {/home/carlos/Zotero/storage/MN9QB5LS/Ariza - 2011 - Two Pioneering Projects from the Early History of .pdf}
}

@misc{arunbijiRAGVsFinetuning,
  title = {{{RAG}} vs {{Finetuning}} vs {{Prompt Engineering}}: {{A}} Pragmatic View on {{LLM}} Implementation},
  shorttitle = {{{RAG}} vs {{Finetuning}} vs {{Prompt Engineering}}},
  author = {Arun Biji, Mathew},
  url = {https://www.linkedin.com/pulse/rag-vs-finetuning-prompt-engineering-pragmatic-view-llm-mathew},
  urldate = {2024-01-21},
  abstract = {The first half of this article tries to give a brief introduction to challenges in practical implementation of LLMs, whereas the second half focuses on solution approaches for addressing these challenges and how they compare to each other. Please skip to the later section if you are already aware of},
  language = {en},
  file = {/home/carlos/Zotero/storage/56G7M9X9/rag-vs-finetuning-prompt-engineering-pragmatic-view-llm-mathew.html}
}

@misc{ArXivOrgEPrint,
  title = {{{arXiv}}.Org e-{{Print}} Archive},
  url = {https://arxiv.org/},
  urldate = {2024-01-29},
  file = {/home/carlos/Zotero/storage/TKFF6YEY/arxiv.org.html}
}

@misc{Audio,
  title = {Audio},
  journal = {Stability AI},
  url = {https://stability.ai/stable-audio},
  urldate = {2023-12-26},
  language = {en-GB},
  file = {/home/carlos/Zotero/storage/2ILU7GRZ/stable-audio.html}
}

@article{BardChatbot2024,
  title = {Bard (Chatbot)},
  year = {2024},
  month = jan,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Bard_(chatbot)&oldid=1199893524},
  urldate = {2024-01-28},
  abstract = {Bard is a conversational generative artificial intelligence chatbot developed by Google. Initially based on the LaMDA family of large language models (LLMs), it was later upgraded to PaLM and then to Gemini. Bard was developed as a direct response to the meteoric rise of OpenAI's ChatGPT, and was released in a limited capacity in March 2023 to lukewarm responses before expanding to other countries in May. LaMDA was developed and announced in 2021, but was not released to the public out of an abundance of caution. OpenAI's launch of ChatGPT in November 2022 and its subsequent popularity caught Google executives off-guard and sent them into a panic, prompting a sweeping response in the ensuing months. After mobilizing its workforce, the company launched Bard in February 2023, with the chatbot taking center stage during the 2023 Google I/O keynote in May.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  language = {en},
  annotation = {Page Version ID: 1199893524},
  file = {/home/carlos/Zotero/storage/8EX7LFT7/Bard_(chatbot).html}
}

@misc{BeginnerGuideNeural,
  title = {A {{Beginner}}'s {{Guide}} to {{Neural Networks}} and {{Deep Learning}}},
  journal = {Pathmind},
  url = {http://wiki.pathmind.com/neural-network},
  urldate = {2023-12-21},
  abstract = {An introduction to deep artificial neural networks and deep learning.},
  language = {en},
  file = {/home/carlos/Zotero/storage/QUNDLL5X/a_beginners_guide_to_neural_networks_and_deep_learning___pathmind.pdf;/home/carlos/Zotero/storage/A63S2JGX/neural-network.html}
}

@misc{bhavsarPromptEngineeringArithmetic2023,
  title = {Prompt {{Engineering}} for {{Arithmetic Reasoning Problems}}},
  author = {Bhavsar, Kaustubh},
  year = {2023},
  month = nov,
  journal = {Medium},
  url = {https://towardsdatascience.com/prompt-engineering-for-arithmetic-reasoning-problems-28c8bcd5bf0e},
  urldate = {2024-01-05},
  abstract = {Explore various prompt engineering techniques for arithmetic reasoning problems, best practices, and rapid experimentations for{\dots}},
  language = {en},
  file = {/home/carlos/Zotero/storage/JPHKF3VK/prompt-engineering-for-arithmetic-reasoning-problems-28c8bcd5bf0e.html}
}

@misc{borsosAudioLMLanguageModeling2023,
  title = {{{AudioLM}}: A {{Language Modeling Approach}} to {{Audio Generation}}},
  shorttitle = {{{AudioLM}}},
  author = {Borsos, Zal{\'a}n and Marinier, Rapha{\"e}l and Vincent, Damien and Kharitonov, Eugene and Pietquin, Olivier and Sharifi, Matt and Roblek, Dominik and Teboul, Olivier and Grangier, David and Tagliasacchi, Marco and Zeghidour, Neil},
  year = {2023},
  month = jul,
  number = {arXiv:2209.03143},
  eprint = {2209.03143},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2209.03143},
  urldate = {2024-01-04},
  abstract = {We introduce AudioLM, a framework for highquality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.},
  archiveprefix = {arxiv},
  language = {en},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/carlos/Zotero/storage/QXGW8ZAJ/Borsos et al. - 2023 - AudioLM a Language Modeling Approach to Audio Generation.pdf}
}

@book{boulangerCsoundBookPerspectives2000,
  title = {The {{Csound Book}}. {{Perspectives}} in {{Software Synthesis}}, {{Sound Design}}, {{Signal Processing}}, and {{Programming}}},
  editor = {Boulanger, Richard},
  year = {2000},
  publisher = {{The MIT Press}},
  address = {{Massachusetts}}
}

@misc{bowmanEightThingsKnow2023,
  title = {Eight {{Things}} to {{Know}} about {{Large Language Models}}},
  author = {Bowman, Samuel R.},
  year = {2023},
  month = apr,
  number = {arXiv:2304.00612},
  eprint = {2304.00612},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.00612},
  url = {http://arxiv.org/abs/2304.00612},
  urldate = {2023-10-23},
  abstract = {The widespread public deployment of large language models (LLMs) in recent months has prompted a wave of new attention and engagement from advocates, policymakers, and scholars from many fields. This attention is a timely response to the many urgent questions that this technology raises, but it can sometimes miss important considerations. This paper surveys the evidence for eight potentially surprising such points: 1. LLMs predictably get more capable with increasing investment, even without targeted innovation. 2. Many important LLM behaviors emerge unpredictably as a byproduct of increasing investment. 3. LLMs often appear to learn and use representations of the outside world. 4. There are no reliable techniques for steering the behavior of LLMs. 5. Experts are not yet able to interpret the inner workings of LLMs. 6. Human performance on a task isn't an upper bound on LLM performance. 7. LLMs need not express the values of their creators nor the values encoded in web text. 8. Brief interactions with LLMs are often misleading.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/RDHLL7B3/Bowman - 2023 - Eight Things to Know about Large Language Models.pdf;/home/carlos/Zotero/storage/HETIVTEB/2304.html}
}

@misc{bretanUnitSelectionMethodology2016,
  title = {A {{Unit Selection Methodology}} for {{Music Generation Using Deep Neural Networks}}},
  author = {Bretan, Mason and Weinberg, Gil and Heck, Larry},
  year = {2016},
  month = dec,
  number = {arXiv:1612.03789},
  eprint = {1612.03789},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1612.03789},
  url = {http://arxiv.org/abs/1612.03789},
  urldate = {2023-05-24},
  abstract = {Several methods exist for a computer to generate music based on data including Markov chains, recurrent neural networks, recombinancy, and grammars. We explore the use of unit selection and concatenation as a means of generating music using a procedure based on ranking, where, we consider a unit to be a variable length number of measures of music. We first examine whether a unit selection method, that is restricted to a finite size unit library, can be sufficient for encompassing a wide spectrum of music. We do this by developing a deep autoencoder that encodes a musical input and reconstructs the input by selecting from the library. We then describe a generative model that combines a deep structured semantic model (DSSM) with an LSTM to predict the next unit, where units consist of four, two, and one measures of music. We evaluate the generative model using objective metrics including mean rank and accuracy and with a subjective listening test in which expert musicians are asked to complete a forced-choiced ranking task. We compare our model to a note-level generative baseline that consists of a stacked LSTM trained to predict forward by one note.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Sound},
  file = {/home/carlos/Zotero/storage/EHW5C3CU/Bretan et al. - 2016 - A Unit Selection Methodology for Music Generation .pdf;/home/carlos/Zotero/storage/E2D7NJMW/1612.html}
}

@misc{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2005.14165},
  url = {http://arxiv.org/abs/2005.14165},
  urldate = {2023-06-29},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: 40+32 pages
\par
Comment: 40+32 pages},
  file = {/home/carlos/Zotero/storage/4ZY5M7ZW/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf;/home/carlos/Zotero/storage/PSWS6S3E/2005.html}
}

@misc{butlinConsciousnessArtificialIntelligence2023,
  title = {Consciousness in {{Artificial Intelligence}}: {{Insights}} from the {{Science}} of {{Consciousness}}},
  shorttitle = {Consciousness in {{Artificial Intelligence}}},
  author = {Butlin, Patrick and Long, Robert and Elmoznino, Eric and Bengio, Yoshua and Birch, Jonathan and Constant, Axel and Deane, George and Fleming, Stephen M. and Frith, Chris and Ji, Xu and Kanai, Ryota and Klein, Colin and Lindsay, Grace and Michel, Matthias and Mudrik, Liad and Peters, Megan A. K. and Schwitzgebel, Eric and Simon, Jonathan and VanRullen, Rufin},
  year = {2023},
  month = aug,
  number = {arXiv:2308.08708},
  eprint = {2308.08708},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.08708},
  url = {http://arxiv.org/abs/2308.08708},
  urldate = {2023-09-27},
  abstract = {Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive "indicator properties" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also suggests that there are no obvious technical barriers to building AI systems which satisfy these indicators.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  file = {/home/carlos/Zotero/storage/3YM2L8NV/Butlin et al. - 2023 - Consciousness in Artificial Intelligence Insights.pdf}
}

@misc{caiStructuredPruningAll2022,
  title = {Structured {{Pruning}} Is {{All You Need}} for {{Pruning CNNs}} at {{Initialization}}},
  author = {Cai, Yaohui and Hua, Weizhe and Chen, Hongzheng and Suh, G. Edward and De Sa, Christopher and Zhang, Zhiru},
  year = {2022},
  month = may,
  number = {arXiv:2203.02549},
  eprint = {2203.02549},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.02549},
  url = {http://arxiv.org/abs/2203.02549},
  urldate = {2024-01-22},
  abstract = {Pruning is a popular technique for reducing the model size and computational cost of convolutional neural networks (CNNs). However, a slow retraining or fine-tuning procedure is often required to recover the accuracy loss caused by pruning. Recently, a new research direction on weight pruning, pruning-at-initialization (PAI), is proposed to directly prune CNNs before training so that fine-tuning or retraining can be avoided. While PAI has shown promising results in reducing the model size, existing approaches rely on fine-grained weight pruning which requires unstructured sparse matrix computation, making it difficult to achieve real speedup in practice unless the sparsity is very high. This work is the first to show that fine-grained weight pruning is in fact not necessary for PAI. Instead, the layerwise compression ratio is the main critical factor to determine the accuracy of a CNN model pruned at initialization. Based on this key observation, we propose PreCropping, a structured hardware-efficient model compression scheme. PreCropping directly compresses the model at the channel level following the layerwise compression ratio. Compared to weight pruning, the proposed scheme is regular and dense in both storage and computation without sacrificing accuracy. In addition, since PreCropping compresses CNNs at initialization, the computational and memory costs of CNNs are reduced for both training and inference on commodity hardware. We empirically demonstrate our approaches on several modern CNN architectures, including ResNet, ShuffleNet, and MobileNet for both CIFAR-10 and ImageNet.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/carlos/Zotero/storage/5ICLM59D/Cai et al. - 2022 - Structured Pruning is All You Need for Pruning CNNs at Initialization.pdf;/home/carlos/Zotero/storage/3PNZ4KJR/2203.html}
}

@misc{calvoRedNeuronalRecurrente2018,
  title = {{Red Neuronal Recurrente - RNN}},
  author = {Calvo, Diego},
  year = {2018},
  month = dec,
  journal = {Diego Calvo},
  url = {https://www.diegocalvo.es/red-neuronal-recurrente/},
  urldate = {2024-01-22},
  abstract = {Definici{\'o}n de Red Neuronal Recurrente Una red neuronal recurrente no tiene una estructura de capas definida, sino que permiten conexiones arbitrarias entre las neuronas, incluso pudiendo crear ciclos, con esto se consigue crear la temporalidad, permitiendo que la red tenga memoria. Las redes neuronales recurrentes son muy potentes para todo lo que tiene que ver [{\dots}]},
  language = {es-ES},
  file = {/home/carlos/Zotero/storage/HMJ2MIVB/red-neuronal-recurrente.html}
}

@article{campbellDeepBlue2002,
  title = {Deep {{Blue}}},
  author = {Campbell, Murray and Hoane, A. Joseph and Hsu, Feng-hsiung},
  year = {2002},
  month = jan,
  journal = {Artificial Intelligence},
  volume = {134},
  number = {1},
  pages = {57--83},
  issn = {0004-3702},
  doi = {10.1016/S0004-3702(01)00129-1},
  url = {https://www.sciencedirect.com/science/article/pii/S0004370201001291},
  urldate = {2023-10-29},
  abstract = {Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including: {$\bullet$}a single-chip chess search engine,{$\bullet$}a massively parallel system with multiple levels of parallelism,{$\bullet$}a strong emphasis on search extensions,{$\bullet$}a complex evaluation function, and{$\bullet$}effective use of a Grandmaster game database. This paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue.},
  keywords = {Computer chess,Evaluation function,Game tree search,Parallel search,Search extensions,Selective search},
  file = {/home/carlos/Zotero/storage/IAV98XWL/Campbell et al. - 2002 - Deep Blue.pdf;/home/carlos/Zotero/storage/ARPI4JK5/S0004370201001291.html}
}

@misc{caoAllYouNeed2020,
  title = {All You Need Is a Second Look: {{Towards Tighter Arbitrary}} Shape Text Detection},
  shorttitle = {All You Need Is a Second Look},
  author = {Cao, Meng and Zou, Yuexian},
  year = {2020},
  month = apr,
  number = {arXiv:2004.12436},
  eprint = {2004.12436},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2004.12436},
  url = {http://arxiv.org/abs/2004.12436},
  urldate = {2024-01-22},
  abstract = {Deep learning-based scene text detection methods have progressed substantially over the past years. However, there remain several problems to be solved. Generally, long curve text instances tend to be fragmented because of the limited receptive field size of CNN. Besides, simple representations using rectangle or quadrangle bounding boxes fall short when dealing with more challenging arbitrary-shaped texts. In addition, the scale of text instances varies greatly which leads to the difficulty of accurate prediction through a single segmentation network. To address these problems, we innovatively propose a two-stage segmentation based arbitrary text detector named {\textbackslash}textit\{NASK\} ({\textbackslash}textbf\{N\}eed {\textbackslash}textbf\{A\} {\textbackslash}textbf\{S\}econd loo{\textbackslash}textbf\{K\}). Specifically, {\textbackslash}textit\{NASK\} consists of a Text Instance Segmentation network namely {\textbackslash}textit\{TIS\} ({\textbackslash}(1\^\{st\}{\textbackslash}) stage), a Text RoI Pooling module and a Fiducial pOint eXpression module termed as {\textbackslash}textit\{FOX\} ({\textbackslash}(2\^\{nd\}{\textbackslash}) stage). Firstly, {\textbackslash}textit\{TIS\} conducts instance segmentation to obtain rectangle text proposals with a proposed Group Spatial and Channel Attention module ({\textbackslash}textit\{GSCA\}) to augment the feature expression. Then, Text RoI Pooling transforms these rectangles to the fixed size. Finally, {\textbackslash}textit\{FOX\} is introduced to reconstruct text instances with a more tighter representation using the predicted geometrical attributes including text center line, text line orientation, character scale and character orientation. Experimental results on two public benchmarks including {\textbackslash}textit\{Total-Text\} and {\textbackslash}textit\{SCUT-CTW1500\} have demonstrated that the proposed {\textbackslash}textit\{NASK\} achieves state-of-the-art results.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 5 pages, 6 figures},
  file = {/home/carlos/Zotero/storage/7FSIZ6HW/Cao y Zou - 2020 - All you need is a second look Towards Tighter Arbitrary shape text detection.pdf;/home/carlos/Zotero/storage/ALQ7N894/2004.html}
}

@misc{ChallengesAssociatedBuilding,
  title = {{The Challenges Associated With Building Products Using Large Language Models (LLMs).}},
  url = {https://www.linkedin.com/pulse/challenges-associated-building-products-using-large-language-das},
  urldate = {2023-10-31},
  abstract = {Hello there. I hope you all are doing well.},
  language = {es},
  file = {/home/carlos/Zotero/storage/A78XI6JS/challenges-associated-building-products-using-large-language-das.html}
}

@misc{chamandFinetuneYourClassifier2022,
  title = {Fine-Tune Your {{Classifier}}: {{Finding Correlations With Temperature}}},
  shorttitle = {Fine-Tune Your {{Classifier}}},
  author = {Chamand, Benjamin and {Risser-Maroix}, Olivier and Kurtz, Camille and Joly, Philippe and Lom{\'e}nie, Nicolas},
  year = {2022},
  month = oct,
  number = {arXiv:2210.09715},
  eprint = {2210.09715},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.09715},
  url = {http://arxiv.org/abs/2210.09715},
  urldate = {2023-11-07},
  abstract = {Temperature is a widely used hyperparameter in various tasks involving neural networks, such as classification or metric learning, whose choice can have a direct impact on the model performance. Most of existing works select its value using hyperparameter optimization methods requiring several runs to find the optimal value. We propose to analyze the impact of temperature on classification tasks by describing a dataset as a set of statistics computed on representations on which we can build a heuristic giving us a default value of temperature. We study the correlation between these extracted statistics and the observed optimal temperatures. This preliminary study on more than a hundred combinations of different datasets and features extractors highlights promising results towards the construction of a general heuristic for temperature.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/carlos/Zotero/storage/EVJ4JQ22/Chamand et al. - 2022 - Fine-tune your Classifier Finding Correlations Wi.pdf;/home/carlos/Zotero/storage/7ZL2LZHW/2210.html}
}

@misc{chenTeachingLargeLanguage2023,
  title = {Teaching {{Large Language Models}} to {{Self-Debug}}},
  author = {Chen, Xinyun and Lin, Maxwell and Sch{\"a}rli, Nathanael and Zhou, Denny},
  year = {2023},
  month = oct,
  number = {arXiv:2304.05128},
  eprint = {2304.05128},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.05128},
  url = {http://arxiv.org/abs/2304.05128},
  urldate = {2023-10-23},
  abstract = {Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3\%, and improves the prediction accuracy on problems of the hardest level by 9\%. On TransCoder and MBPP where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12\%. Meanwhile, by leveraging feedback messages and reusing failed predictions, Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/7APGY9B9/Chen et al. - 2023 - Teaching Large Language Models to Self-Debug.pdf;/home/carlos/Zotero/storage/B54ZXYI4/2304.html}
}

@misc{CompositorMusicaIA,
  title = {{Compositor de m{\'u}sica de IA - AWS DeepComposer - AWS}},
  journal = {Amazon Web Services, Inc.},
  url = {https://aws.amazon.com/es/deepcomposer/},
  urldate = {2024-02-05},
  abstract = {AWS DeepComposer brinda a los desarrolladores una forma creativa de empezar a usar el aprendizaje autom{\'a}tico. P{\'o}ngase manos a la obra, literalmente, con un teclado musical y las t{\'e}cnicas de aprendizaje autom{\'a}tico m{\'a}s novedosas, dise{\~n}adas para aumentar sus habilidades de ML.},
  language = {es-ES},
  file = {/home/carlos/Zotero/storage/CL4YPVPN/deepcomposer.html}
}

@article{ConjuntoMandelbrot2024,
  title = {{Conjunto de Mandelbrot}},
  year = {2024},
  month = jan,
  journal = {Wikipedia, la enciclopedia libre},
  url = {https://es.wikipedia.org/w/index.php?title=Conjunto_de_Mandelbrot&oldid=157536731},
  urldate = {2024-01-28},
  abstract = {El conjunto de Mandelbrot es el m{\'a}s estudiado de los fractales. Se conoce as{\'i} en honor al matem{\'a}tico Beno{\^i}t Mandelbrot (1924-2010), que investig{\'o} sobre {\'e}l en los a{\~n}os setenta. Este conjunto se define en el plano complejo fijando un n{\'u}mero complejo c cualquiera. A partir de c, se construye una sucesi{\'o}n por recursi{\'o}n:                                                \{                                                                                     z                                            0                                                           =                   0                   {$\in$}                                        C                                                                                             (t{\'e}rmino inicial)                                                                                                                                              z                                            n                       +                       1                                                           =                                        z                                            n                                                                 2                                                           +                   c                                                                          (sucesi{\'o}n recursiva)                                                                                                                  \{{\textbackslash}displaystyle \{{\textbackslash}begin\{cases\}z\_\{0\}=0{\textbackslash}in {\textbackslash}mathbb \{C\} \&\{{\textbackslash}text\{(t{\'e}rmino inicial)\}\}{\textbackslash}qquad {\textbackslash}{\textbackslash}z\_\{n+1\}=z\_\{n\}\^\{2\}+c\&\{{\textbackslash}text\{(sucesi{\'o}n recursiva)\}\}{\textbackslash}end\{cases\}\}\}    Si esta sucesi{\'o}n queda acotada, entonces se dice que c pertenece al conjunto de Mandelbrot, y si no, queda excluido del mismo. Por ejemplo, si c = 1 obtenemos la sucesi{\'o}n 0, 1, 2, 5, 26, {\dots}, que diverge. Como no est{\'a} acotada, 1 no es un elemento del conjunto de Mandelbrot. En cambio, si c = {\textendash}1 obtenemos la sucesi{\'o}n 0, {\textendash}1, 0, {\textendash}1, {\dots}, que s{\'i} es acotada y, por tanto, {\textendash}1 s{\'i} pertenece al conjunto de Mandelbrot. A menudo se representa el conjunto mediante el algoritmo de tiempo de escape. En ese caso, los colores de los puntos que no pertenecen al conjunto indican la velocidad con la que diverge (tiende al infinito, en m{\'o}dulo) la sucesi{\'o}n correspondiente a dicho punto. En la imagen de ejemplo, observamos que el rojo oscuro indica que al cabo de pocos c{\'a}lculos se sabe que el punto no est{\'a} en el conjunto mientras que el blanco informa de que se ha tardado mucho m{\'a}s en comprobarlo. Como no se puede calcular un sinf{\'i}n de valores, es preciso poner un l{\'i}mite y decidir que si los p primeros t{\'e}rminos de la sucesi{\'o}n est{\'a}n acotados entonces se considera que el punto pertenece al conjunto. Al aumentar el valor de p se mejora la precisi{\'o}n de la imagen. Por otra parte, se sabe que los puntos cuya distancia al origen es superior a 2, es decir,                                   x                        2                             +                    y                        2                             {$>$}         4                 \{{\textbackslash}displaystyle x\^\{2\}+y\^\{2\}{$>$}4\}    no pertenecen al conjunto. Por lo tanto basta encontrar un solo t{\'e}rmino de la sucesi{\'o}n que verifique                                    {\textbar}                             z                        n                                        {\textbar}                  {$>$}         2                 \{{\textbackslash}displaystyle {\textbar}z\_\{n\}{\textbar}{$>$}2\}    para estar seguro de que c no est{\'a} en el conjunto.\vphantom\}},
  copyright = {Creative Commons Attribution-ShareAlike License},
  language = {es},
  annotation = {Page Version ID: 157536731},
  file = {/home/carlos/Zotero/storage/QDFYZIQZ/Conjunto_de_Mandelbrot.html}
}

@inproceedings{crookConversationalSemanticSearch2018,
  title = {Conversational {{Semantic Search}}: {{Looking BeyondWeb Search}}, {{Q}}\&{{A}} and {{Dialog Systems}}},
  shorttitle = {Conversational {{Semantic Search}}},
  booktitle = {The 11th {{ACM International Conference}} on {{Web Search}} and {{Data Minining}} ({{WSDM}} 2018)},
  author = {Crook, Paul A. and Marin, Alex and Agarwal, Vipul and Anderson, Samantha and Jang, Ohyoung and Lanewala, Aliasgar and Tangirala, Karthik and Zitouni, Imed},
  year = {2018},
  month = feb,
  url = {https://www.microsoft.com/en-us/research/publication/conversational-semantic-search-looking-beyondweb-search-qa-and-dialog-systems/},
  urldate = {2023-07-06},
  abstract = {User expectations of web search are changing. They are expecting search engines to answer questions, to be more conversational, and to offer means to complete tasks on their behalf. At the same time, to increase the breadth of tasks that personal digital assistants (PDAs), such as Microsoft's Cortana or Amazon's Alexa, are capable of, PDAs [{\dots}]},
  language = {en-US},
  file = {/home/carlos/Zotero/storage/XS5AUBND/Crook et al. - 2018 - Conversational Semantic Search Looking BeyondWeb .pdf}
}

@misc{DeepLearningDL,
  title = {Deep {{Learning}} ({{DL}})},
  journal = {Questions and Answers \hspace{0pt}in MRI},
  url = {http://mriquestions.com/what-is-a-neural-network.html},
  urldate = {2023-12-21},
  abstract = {Deep Learning (DL)},
  language = {en},
  file = {/home/carlos/Zotero/storage/H68M27JI/what-is-a-neural-network.html}
}

@article{departmentofcomputersciencesrminstituteofscienceandtechnologychennaiindia.MusenetMusicGeneration2020a,
  title = {Musenet : {{Music Generation}} Using {{Abstractive}} and {{Generative Methods}}},
  shorttitle = {Musenet},
  author = {{Department of Computer Science, SRM Institute of Science and Technology, Chennai, India.} and Pal*, Abhilash and Saha, Sourav and {Department of Computer Science, SRM Institute of Science and Technology, Chennai, India.} and {Anita} and {Department of Computer Science, SRM Institute of Science and Technology, Chennai, India.}},
  year = {2020},
  month = apr,
  journal = {International Journal of Innovative Technology and Exploring Engineering},
  volume = {9},
  number = {6},
  pages = {784--788},
  issn = {22783075},
  doi = {10.35940/ijitee.F3580.049620},
  url = {https://www.ijitee.org/portfolio-item/F3580049620/},
  urldate = {2023-12-26},
  abstract = {Humans have been entertained by music for millennia. For ages it has been treated as an art form which requires a lot of imagination, creativity and accumulation of feelings and emotions. Recent trends in the field of Artificial Intelligence have been getting traction and Researchers have been developing and generating rudimentary forms of music through the use of AI. Our goal is to generate novel music, which will be non-repetitive and enjoyable. We aim to utilize a couple of Machine Learning models for the same. Given a seed bar of music, our first Discriminatory network consisting of Support Vector Machines and Neural Nets will choose a note/chord to direct the next bar. Based on this chord or note another network, a Generative Net consisting of Generative Pretrained Transformers(GPT2) and LSTMs will generate the entire bar of music. Our two fold method is novel and our aim is to make the generation method as similar to music composition in reality as possible. This in turn results in better concordant music. Machine generated music will be copyright free and can be generated conditioned on a few parameters for a given use.The paper presents several use cases and while the utilization will be for a niche audience, if a easy to use application can be built, almost anyone will be able to use deep learning to generate concordant music based on their needs.},
  note = {[TLDR] The goal is to generate novel music, which will be non-repetitive and enjoyable, and almost anyone will be able to use deep learning to generate concordant music based on their needs.},
  file = {/home/carlos/Zotero/storage/MUPRNIQZ/Department of Computer Science, SRM Institute of Science and Technology, Chennai, India. et al. - 2020 - Musenet  Music Generation using Abstractive and G.pdf}
}

@misc{dhariwalJukeboxGenerativeModel2020,
  title = {Jukebox: {{A Generative Model}} for {{Music}}},
  shorttitle = {Jukebox},
  author = {Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
  year = {2020},
  month = apr,
  number = {arXiv:2005.00341},
  eprint = {2005.00341},
  primaryclass = {cs, eess, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2005.00341},
  url = {http://arxiv.org/abs/2005.00341},
  urldate = {2023-12-26},
  abstract = {We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples at https://jukebox.openai.com, along with model weights and code at https://github.com/openai/jukebox},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  file = {/home/carlos/Zotero/storage/B294RLLC/Dhariwal et al. - 2020 - Jukebox A Generative Model for Music.pdf;/home/carlos/Zotero/storage/UKDJBJNM/2005.html}
}

@misc{dhuliawalaChainofVerificationReducesHallucination2023,
  title = {Chain-of-{{Verification Reduces Hallucination}} in {{Large Language Models}}},
  author = {Dhuliawala, Shehzaad and Komeili, Mojtaba and Xu, Jing and Raileanu, Roberta and Li, Xian and Celikyilmaz, Asli and Weston, Jason},
  year = {2023},
  month = sep,
  number = {arXiv:2309.11495},
  eprint = {2309.11495},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.11495},
  url = {http://arxiv.org/abs/2309.11495},
  urldate = {2023-10-30},
  abstract = {Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/JLP8R4IU/Dhuliawala et al. - 2023 - Chain-of-Verification Reduces Hallucination in Lar.pdf;/home/carlos/Zotero/storage/5L75C6YB/2309.html}
}

@misc{douglasLargeLanguageModels2023,
  title = {Large {{Language Models}}},
  author = {Douglas, Michael R.},
  year = {2023},
  month = jul,
  number = {arXiv:2307.05782},
  eprint = {2307.05782},
  primaryclass = {hep-th, physics:physics},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.05782},
  url = {http://arxiv.org/abs/2307.05782},
  urldate = {2023-09-27},
  abstract = {Artificial intelligence is making spectacular progress, and one of the best examples is the development of large language models (LLMs) such as OpenAI's GPT series. In these lectures, written for readers with a background in mathematics or physics, we give a brief history and survey of the state of the art, and describe the underlying transformer architecture in detail. We then explore some current ideas on how LLMs work and how models trained to predict the next word in a text are able to perform other tasks displaying intelligence.},
  archiveprefix = {arxiv},
  keywords = {68T01,Computer Science - Computation and Language,High Energy Physics - Theory,I.2.7,Mathematics - History and Overview,Physics - Computational Physics},
  note = {Comment: 46 pages},
  file = {/home/carlos/Zotero/storage/MTB9SGS5/Douglas - 2023 - Large Language Models.pdf;/home/carlos/Zotero/storage/Y46IYSSF/2307.html}
}

@article{duAddressingSyntaxBasedSemantic2022,
  title = {Addressing {{Syntax-Based Semantic Complementation}}: {{Incorporating Entity}} and {{Soft Dependency Constraints}} into {{Metonymy Resolution}}},
  shorttitle = {Addressing {{Syntax-Based Semantic Complementation}}},
  author = {Du, Siyuan and Wang, Hao},
  year = {2022},
  month = mar,
  journal = {Future Internet},
  volume = {14},
  number = {3},
  pages = {85},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1999-5903},
  doi = {10.3390/fi14030085},
  url = {https://www.mdpi.com/1999-5903/14/3/85},
  urldate = {2023-12-25},
  abstract = {State-of-the-art methods for metonymy resolution (MR) consider the sentential context by modeling the entire sentence. However, entity representation, or syntactic structure that are informative may be beneficial for identifying metonymy. Other approaches only using deep neural network fail to capture such information. To leverage both entity and syntax constraints, this paper proposes a robust model EBAGCN for metonymy resolution. First, this work extracts syntactic dependency relations under the guidance of syntactic knowledge. Then the work constructs a neural network to incorporate both entity representation and syntactic structure into better resolution representations. In this way, the proposed model alleviates the impact of noisy information from entire sentences and breaks the limit of performance on the complicated texts. Experiments on the SemEval and ReLocaR dataset show that the proposed model significantly outperforms the state-of-the-art method BERT by more than 4\%. Ablation tests demonstrate that leveraging these two types of constraints benefits fine pre-trained language models in the MR task.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  language = {en},
  keywords = {dependency integration,entity representation,metonymy resolution},
  file = {/home/carlos/Zotero/storage/AP3AK6JY/Du y Wang - 2022 - Addressing Syntax-Based Semantic Complementation .pdf}
}

@misc{erkerImaginationAllYou2023,
  title = {Imagination Is {{All You Need}}! {{Curved Contrastive Learning}} for {{Abstract Sequence Modeling Utilized}} on {{Long Short-Term Dialogue Planning}}},
  author = {Erker, Justus-Jonas and Schaffer, Stefan and Spanakis, Gerasimos},
  year = {2023},
  month = jun,
  number = {arXiv:2211.07591},
  eprint = {2211.07591},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.07591},
  url = {http://arxiv.org/abs/2211.07591},
  urldate = {2024-01-22},
  abstract = {Inspired by the curvature of space-time (Einstein, 1921), we introduce Curved Contrastive Learning (CCL), a novel representation learning technique for learning the relative turn distance between utterance pairs in multi-turn dialogues. The resulting bi-encoder models can guide transformers as a response ranking model towards a goal in a zero-shot fashion by projecting the goal utterance and the corresponding reply candidates into a latent space. Here the cosine similarity indicates the distance/reachability of a candidate utterance toward the corresponding goal. Furthermore, we explore how these forward-entailing language representations can be utilized for assessing the likelihood of sequences by the entailment strength i.e. through the cosine similarity of its individual members (encoded separately) as an emergent property in the curved space. These non-local properties allow us to imagine the likelihood of future patterns in dialogues, specifically by ordering/identifying future goal utterances that are multiple turns away, given a dialogue context. As part of our analysis, we investigate characteristics that make conversations (un)plannable and find strong evidence of planning capability over multiple turns (in 61.56\% over 3 turns) in conversations from the DailyDialog (Li et al., 2017) dataset. Finally, we show how we achieve higher efficiency in sequence modeling tasks compared to previous work thanks to our relativistic approach, where only the last utterance needs to be encoded and computed during inference.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: Accepted in ACL 2023 Findings},
  file = {/home/carlos/Zotero/storage/8M275USC/Erker et al. - 2023 - Imagination is All You Need! Curved Contrastive Learning for Abstract Sequence Modeling Utilized on .pdf;/home/carlos/Zotero/storage/TA4U6C5F/2211.html}
}

@misc{eysenbachDiversityAllYou2018,
  title = {Diversity Is {{All You Need}}: {{Learning Skills}} without a {{Reward Function}}},
  shorttitle = {Diversity Is {{All You Need}}},
  author = {Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
  year = {2018},
  month = oct,
  number = {arXiv:1802.06070},
  eprint = {1802.06070},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1802.06070},
  url = {http://arxiv.org/abs/1802.06070},
  urldate = {2024-01-22},
  abstract = {Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose DIAYN ('Diversity is All You Need'), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  note = {Comment: Videos and code for our experiments are available at: https://sites.google.com/view/diayn},
  file = {/home/carlos/Zotero/storage/FSHXYMD5/Eysenbach et al. - 2018 - Diversity is All You Need Learning Skills without a Reward Function.pdf;/home/carlos/Zotero/storage/HUTK4SCB/1802.html}
}

@misc{FindWaysDeal,
  title = {{Find ways to deal with the scarcity of labeled data}},
  url = {https://www.linkedin.com/pulse/find-ways-deal-scarcity-labeled-data-amit-asawa},
  urldate = {2023-12-21},
  abstract = {Compared to other available choices such as unsupervised and deep learning algorithms: the whole learning process of supervised machine learning (ML) based algorithms is much simpler; the training \& deployment costs are considerably low too; and most importantly supervised models work perfectly for},
  language = {es},
  file = {/home/carlos/Zotero/storage/K3EN3SDY/find-ways-deal-scarcity-labeled-data-amit-asawa.html}
}

@misc{frackiewiczEvolucionIAComposicion2023,
  title = {{La evoluci{\'o}n de la IA en la composici{\'o}n musical}},
  author = {Fr{\k a}ckiewicz, Marcin},
  year = {2023},
  month = jul,
  journal = {TS2 SPACE},
  url = {https://ts2.space/es/la-evolucion-de-la-ia-en-la-composicion-musical/},
  urldate = {2023-11-03},
  abstract = {La evoluci{\'o}n de la IA en la composici{\'o}n musical TS2 SPACE},
  language = {es-ES},
  file = {/home/carlos/Zotero/storage/Z76TKKBM/la-evolucion-de-la-ia-en-la-composicion-musical.html}
}

@article{funkMusicalSuiteComposed2018a,
  title = {A {{Musical Suite Composed}} by an {{Electronic Brain}}: {{Reexamining}} the {{Illiac Suite}} and the {{Legacy}} of {{Lejaren A}}. {{Hiller Jr}}.},
  shorttitle = {A {{Musical Suite Composed}} by an {{Electronic Brain}}},
  author = {Funk, Tiffany},
  year = {2018},
  journal = {Leonardo Music Journal},
  volume = {28},
  pages = {19--24},
  publisher = {{MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info {\dots}}},
  url = {https://direct.mit.edu/lmj/article-abstract/doi/10.1162/lmj_a_01037/69560},
  urldate = {2023-12-28},
  file = {/home/carlos/Zotero/storage/JSP23EKX/Funk - 2018 - A Musical Suite Composed by an Electronic Brain R.pdf}
}

@inproceedings{ganguliPredictabilitySurpriseLarge2022,
  title = {Predictability and {{Surprise}} in {{Large Generative Models}}},
  booktitle = {2022 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Ganguli, Deep and Hernandez, Danny and Lovitt, Liane and DasSarma, Nova and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Kernion, Jackson and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Elhage, Nelson and Showk, Sheer El and Fort, Stanislav and {Hatfield-Dodds}, Zac and Johnston, Scott and Kravec, Shauna and Nanda, Neel and Ndousse, Kamal and Olsson, Catherine and Amodei, Daniela and Amodei, Dario and Brown, Tom and Kaplan, Jared and McCandlish, Sam and Olah, Chris and Clark, Jack},
  year = {2022},
  month = jun,
  eprint = {2202.07785},
  primaryclass = {cs},
  pages = {1747--1764},
  doi = {10.1145/3531146.3533229},
  url = {http://arxiv.org/abs/2202.07785},
  urldate = {2023-06-27},
  abstract = {Large-scale pre-training has recently emerged as a technique for creating capable, general purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have an unusual combination of predictable loss on a broad training distribution (as embodied in their "scaling laws"), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, and academics who want to analyze, critique, and potentially develop large generative models.},
  archiveprefix = {arxiv},
  language = {en},
  keywords = {Computer Science - Computers and Society},
  note = {Comment: Updated to reflect the version submitted (and accepted) to ACM FAccT '22. This update incorporates feedback from peer-review and fixes minor typos. See open access FAccT conference version at: https://dl.acm.org/doi/abs/10.1145/3531146.3533229},
  file = {/home/carlos/Zotero/storage/93WQ2BRW/Ganguli et al. - 2022 - Predictability and Surprise in Large Generative Mo.pdf}
}

@misc{GenerationLLMs,
  title = {Generation with {{LLMs}}},
  url = {https://huggingface.co/docs/transformers/main/en/llm_tutorial},
  urldate = {2023-10-31},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {/home/carlos/Zotero/storage/Q83ZWSQC/llm_tutorial.html}
}

@book{gollapudi2016practical,
  title = {Practical Machine Learning},
  author = {Gollapudi, S.},
  year = {2016},
  publisher = {{Packt Publishing}},
  url = {https://books.google.es/books?id=WmsdDAAAQBAJ},
  isbn = {978-1-78439-401-1}
}

@misc{gonzaloAsomandonosVentanaContextual2023,
  type = {{Billet}},
  title = {{Asom{\'a}ndonos a la ventana contextual de la Inteligencia Artificial: dec{\'a}logo de ayuda para la identificaci{\'o}n del uso de ChatGPT en textos acad{\'e}micos}},
  shorttitle = {{Asom{\'a}ndonos a la ventana contextual de la Inteligencia Artificial}},
  author = {Gonzalo, Jover and Carabantes, David and Gonz{\'a}lez Geraldo, Jos{\'e} L.},
  year = {2023},
  month = jun,
  journal = {Aula Magna 2.0},
  url = {https://cuedespyd.hypotheses.org/13299},
  urldate = {2023-11-08},
  abstract = {Por Gonzalo Jover*, David Carabantes* y Jos{\'e} L. Gonz{\'a}lez Geraldo** *Universidad Complutense de Madrid **Universidad de Castilla La Mancha Palabras clave:~ChatGPT, OpenAI, Inteligencia Artificial ~ Que la Inteligencia Artificial (IA) ha contaminado cada rinc{\'o}n de nuestra sociedad a un ritmo tan espeluznante como atractivo y peligroso es m{\'a}s que evidente a {\dots} Continuar leyendo "Asom{\'a}ndonos a la ventana contextual de la Inteligencia Artificial: dec{\'a}logo de ayuda para la identificaci{\'o}n del uso de ChatGPT en textos acad{\'e}micos"},
  language = {es},
  file = {/home/carlos/Zotero/storage/FZ5IHE5I/13299.html}
}

@book{Goodfellow-et-al-2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {{MIT Press}},
  note = {http://www.deeplearningbook.org}
}

@misc{GPT3RiseFoundation,
  title = {{GPT-3 and the rise of foundation models}},
  url = {https://www.linkedin.com/pulse/gpt-3-rise-foundation-models-joseph-boland},
  urldate = {2023-12-10},
  abstract = {GPT-3 (Generative Pre-Trained Transformer 3) is a large language model with 175 billion parameters, trained using the Common Crawl internet dataset, Wikipedia, and several large digital document collections. Its transformer-based algorithm has demonstrated superior performance in text generation, co},
  language = {es},
  file = {/home/carlos/Zotero/storage/9E6YS6FB/gpt-3-rise-foundation-models-joseph-boland.html}
}

@misc{grahamOneModelAll2022,
  title = {One {{Model}} Is {{All You Need}}: {{Multi-Task Learning Enables Simultaneous Histology Image Segmentation}} and {{Classification}}},
  shorttitle = {One {{Model}} Is {{All You Need}}},
  author = {Graham, Simon and Vu, Quoc Dang and Jahanifar, Mostafa and Raza, Shan E. Ahmed and Minhas, Fayyaz and Snead, David and Rajpoot, Nasir},
  year = {2022},
  month = nov,
  number = {arXiv:2203.00077},
  eprint = {2203.00077},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.00077},
  url = {http://arxiv.org/abs/2203.00077},
  urldate = {2024-01-22},
  abstract = {The recent surge in performance for image analysis of digitised pathology slides can largely be attributed to the advances in deep learning. Deep models can be used to initially localise various structures in the tissue and hence facilitate the extraction of interpretable features for biomarker discovery. However, these models are typically trained for a single task and therefore scale poorly as we wish to adapt the model for an increasing number of different tasks. Also, supervised deep learning models are very data hungry and therefore rely on large amounts of training data to perform well. In this paper, we present a multi-task learning approach for segmentation and classification of nuclei, glands, lumina and different tissue regions that leverages data from multiple independent data sources. While ensuring that our tasks are aligned by the same tissue type and resolution, we enable meaningful simultaneous prediction with a single network. As a result of feature sharing, we also show that the learned representation can be used to improve the performance of additional tasks via transfer learning, including nuclear classification and signet ring cell detection. As part of this work, we train our developed Cerberus model on a huge amount of data, consisting of over 600K objects for segmentation and 440K patches for classification. We use our approach to process 599 colorectal whole-slide images from TCGA, where we localise 377 million, 900K and 2.1 million nuclei, glands and lumina, respectively and make the results available to the community for downstream analysis.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/home/carlos/Zotero/storage/EFDRE7RU/Graham et al. - 2022 - One Model is All You Need Multi-Task Learning Enables Simultaneous Histology Image Segmentation and.pdf;/home/carlos/Zotero/storage/6E4DBE9Y/2203.html}
}

@mastersthesis{guerraparraMesjetiuTFM_Arte_Sonoro_MEMORIA2020,
  title = {Mesjetiu/{{TFM}}\_{{Arte}}\_{{Sonoro}}\_{{MEMORIA}}},
  author = {Guerra Parra, Carlos Arturo},
  year = {2020},
  url = {https://github.com/mesjetiu/TFM_Arte_Sonoro_MEMORIA},
  urldate = {2023-12-09},
  school = {Universidad de Barcelona},
  file = {/home/carlos/Zotero/storage/TRE2DQ8J/TFM_Arte_Sonoro_MEMORIA.html}
}

@misc{gunasekarTextbooksAreAll2023,
  title = {Textbooks {{Are All You Need}}},
  author = {Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and {de Rosa}, Gustavo and Saarikivi, Olli and Salim, Adil and Shah, Shital and Behl, Harkirat Singh and Wang, Xin and Bubeck, S{\'e}bastien and Eldan, Ronen and Kalai, Adam Tauman and Lee, Yin Tat and Li, Yuanzhi},
  year = {2023},
  month = jun,
  number = {arXiv:2306.11644},
  eprint = {2306.11644},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.11644},
  url = {http://arxiv.org/abs/2306.11644},
  urldate = {2023-06-29},
  abstract = {We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6\% on HumanEval and 55.5\% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45\% on HumanEval.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: 26 pages
\par
Comment: 26 pages},
  file = {/home/carlos/Zotero/storage/IMBD7FR8/Gunasekar et al. - 2023 - Textbooks Are All You Need.pdf;/home/carlos/Zotero/storage/PP9DGAW6/2306.html}
}

@misc{hanPreTrainedModelsPresent2021,
  title = {Pre-{{Trained Models}}: {{Past}}, {{Present}} and {{Future}}},
  shorttitle = {Pre-{{Trained Models}}},
  author = {Han, Xu and Zhang, Zhengyan and Ding, Ning and Gu, Yuxian and Liu, Xiao and Huo, Yuqi and Qiu, Jiezhong and Yao, Yuan and Zhang, Ao and Zhang, Liang and Han, Wentao and Huang, Minlie and Jin, Qin and Lan, Yanyan and Liu, Yang and Liu, Zhiyuan and Lu, Zhiwu and Qiu, Xipeng and Song, Ruihua and Tang, Jie and Wen, Ji-Rong and Yuan, Jinhui and Zhao, Wayne Xin and Zhu, Jun},
  year = {2021},
  month = aug,
  number = {arXiv:2106.07139},
  eprint = {2106.07139},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.07139},
  url = {http://arxiv.org/abs/2106.07139},
  urldate = {2024-01-25},
  abstract = {Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success and become a milestone in the field of artificial intelligence (AI). Owing to sophisticated pre-training objectives and huge model parameters, large-scale PTMs can effectively capture knowledge from massive labeled and unlabeled data. By storing knowledge into huge parameters and fine-tuning on specific tasks, the rich knowledge implicitly encoded in huge parameters can benefit a variety of downstream tasks, which has been extensively demonstrated via experimental verification and empirical analysis. It is now the consensus of the AI community to adopt PTMs as backbone for downstream tasks rather than learning models from scratch. In this paper, we take a deep look into the history of pre-training, especially its special relation with transfer learning and self-supervised learning, to reveal the crucial position of PTMs in the AI development spectrum. Further, we comprehensively review the latest breakthroughs of PTMs. These breakthroughs are driven by the surge of computational power and the increasing availability of data, towards four important directions: designing effective architectures, utilizing rich contexts, improving computational efficiency, and conducting interpretation and theoretical analysis. Finally, we discuss a series of open problems and research directions of PTMs, and hope our view can inspire and advance the future study of PTMs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/79LBRCTL/Han et al. - 2021 - Pre-Trained Models Past, Present and Future.pdf;/home/carlos/Zotero/storage/CEQNUL7H/2106.html}
}

@misc{hernandez-olivanSurveyArtificialIntelligence2022,
  title = {A {{Survey}} on {{Artificial Intelligence}} for {{Music Generation}}: {{Agents}}, {{Domains}} and {{Perspectives}}},
  shorttitle = {A {{Survey}} on {{Artificial Intelligence}} for {{Music Generation}}},
  author = {{Hernandez-Olivan}, Carlos and {Hernandez-Olivan}, Javier and Beltran, Jose R.},
  year = {2022},
  month = nov,
  number = {arXiv:2210.13944},
  eprint = {2210.13944},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.13944},
  url = {http://arxiv.org/abs/2210.13944},
  urldate = {2023-09-27},
  abstract = {Music is one of the Gardner's intelligences in his theory of multiple intelligences. How humans perceive and understand music is still being studied and is crucial to develop artificial intelligence models that imitate such processes. Music generation with Artificial Intelligence is an emerging field that is gaining much attention in the recent years. In this paper, we describe how humans compose music and how new AI systems could imitate such process by comparing past and recent advances in the field with music composition techniques. To understand how AI models and algorithms generate music and the potential applications that might appear in the future, we explore, analyze and describe the agents that take part of the music generation process: the datasets, models, interfaces, the users and the generated music. We mention possible applications that might benefit from this field and we also propose new trends and future research directions that could be explored in the future.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  note = {Comment: Under review},
  file = {/home/carlos/Zotero/storage/498MZNDW/Hernandez-Olivan et al. - 2022 - A Survey on Artificial Intelligence for Music Gene.pdf;/home/carlos/Zotero/storage/AMJVURKG/2210.html}
}

@misc{HistoriaInteligenciaArtificial,
  title = {{Historia de la Inteligencia Artificial, el Machine Learning y el Deep Learning}},
  url = {https://www.algotive.ai/es-mx/blog/historia-de-la-inteligencia-artificial-el-machine-learning-y-el-deep-learning},
  urldate = {2023-10-30},
  abstract = {Conoce toda la historia de la Inteligencia Artificial (IA), el Machine Learning (ML) y el Deep Learning (DL), de manera breve, sencilla e ilustrativa.},
  language = {es-mx},
  file = {/home/carlos/Zotero/storage/LU4PW86U/historia-de-la-inteligencia-artificial-el-machine-learning-y-el-deep-learning.html}
}

@article{HochreiterVanishingGradient1998,
  title = {The Vanishing Gradient Problem during Learning Recurrent Neural Nets and Problem Solutions},
  author = {Hochreiter, Sepp},
  year = {1998},
  month = apr,
  journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  volume = {6},
  pages = {107--116},
  doi = {10.1142/S0218488598000094}
}

@misc{holtzmanCuriousCaseNeural2020,
  title = {The {{Curious Case}} of {{Neural Text Degeneration}}},
  author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  year = {2020},
  month = feb,
  number = {arXiv:1904.09751},
  eprint = {1904.09751},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1904.09751},
  url = {http://arxiv.org/abs/1904.09751},
  urldate = {2023-11-07},
  abstract = {Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: Published in ICLR 2020},
  file = {/home/carlos/Zotero/storage/J4H86DF3/Holtzman et al. - 2020 - The Curious Case of Neural Text Degeneration.pdf;/home/carlos/Zotero/storage/6Z9WQDE7/1904.html}
}

@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = {1989},
  month = jan,
  journal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(89)90020-8},
  url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
  urldate = {2023-12-21},
  abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
  keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
  file = {/home/carlos/Zotero/storage/DIC8KESB/0893608089900208.html}
}

@misc{HowGetBetter2023,
  title = {How to {{Get Better Outputs}} from {{Your Large Language Model}}},
  year = {2023},
  month = jun,
  journal = {NVIDIA Technical Blog},
  url = {https://developer.nvidia.com/blog/how-to-get-better-outputs-from-your-large-language-model/},
  urldate = {2024-01-25},
  abstract = {Large language models (LLMs) have generated excitement worldwide due to their ability to understand and process human language at a scale that is unprecedented. It has transformed the way that we{\dots}},
  language = {en-US},
  file = {/home/carlos/Zotero/storage/ZWDH2CVI/how-to-get-better-outputs-from-your-large-language-model.html}
}

@misc{huangAgentCoderMultiAgentbasedCode2023,
  title = {{{AgentCoder}}: {{Multi-Agent-based Code Generation}} with {{Iterative Testing}} and {{Optimisation}}},
  shorttitle = {{{AgentCoder}}},
  author = {Huang, Dong and Bu, Qingwen and Zhang, Jie M. and Luck, Michael and Cui, Heming},
  year = {2023},
  month = dec,
  number = {arXiv:2312.13010},
  eprint = {2312.13010},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2312.13010},
  url = {http://arxiv.org/abs/2312.13010},
  urldate = {2024-01-02},
  abstract = {The advancement of natural language processing (NLP) has been significantly boosted by the development of transformer-based large language models (LLMs). These models have revolutionized NLP tasks, particularly in code generation, aiding developers in creating software with enhanced efficiency. Despite their advancements, challenges in balancing code snippet generation with effective test case generation and execution persist. To address these issues, this paper introduces Multi-Agent Assistant Code Generation (AgentCoder), a novel solution comprising a multi-agent framework with specialized agents: the programmer agent, the test designer agent, and the test executor agent. During the coding procedure, the programmer agent will focus on the code generation and refinement based on the test executor agent's feedback. The test designer agent will generate test cases for the generated code, and the test executor agent will run the code with the test cases and write the feedback to the programmer. This collaborative system ensures robust code generation, surpassing the limitations of single-agent models and traditional methodologies. Our extensive experiments on 9 code generation models and 12 enhancement approaches showcase AgentCoder's superior performance over existing code generation models and prompt engineering techniques across various benchmarks. For example, AgentCoder achieves 77.4\% and 89.1\% pass@1 in HumanEval-ET and MBPP-ET with GPT-3.5, while SOTA baselines obtain only 69.5\% and 63.0\%.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: 21 pages, 12 figures},
  file = {/home/carlos/Zotero/storage/495L4V9P/Huang et al. - 2023 - AgentCoder Multi-Agent-based Code Generation with Iterative Testing and Optimisation.pdf;/home/carlos/Zotero/storage/GBNPDX77/2312.html}
}

@misc{hungTranscriptionAllYou2020,
  title = {Transcription {{Is All You Need}}: {{Learning}} to {{Separate Musical Mixtures}} with {{Score}} as {{Supervision}}},
  shorttitle = {Transcription {{Is All You Need}}},
  author = {Hung, Yun-Ning and Wichern, Gordon and Roux, Jonathan Le},
  year = {2020},
  month = oct,
  number = {arXiv:2010.11904},
  eprint = {2010.11904},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2010.11904},
  url = {http://arxiv.org/abs/2010.11904},
  urldate = {2024-01-22},
  abstract = {Most music source separation systems require large collections of isolated sources for training, which can be difficult to obtain. In this work, we use musical scores, which are comparatively easy to obtain, as a weak label for training a source separation system. In contrast with previous score-informed separation approaches, our system does not require isolated sources, and score is used only as a training target, not required for inference. Our model consists of a separator that outputs a time-frequency mask for each instrument, and a transcriptor that acts as a critic, providing both temporal and frequency supervision to guide the learning of the separator. A harmonic mask constraint is introduced as another way of leveraging score information during training, and we propose two novel adversarial losses for additional fine-tuning of both the transcriptor and the separator. Results demonstrate that using score information outperforms temporal weak-labels, and adversarial structures lead to further improvements in both separation and transcription performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/carlos/Zotero/storage/H5CEA699/Hung et al. - 2020 - Transcription Is All You Need Learning to Separate Musical Mixtures with Score as Supervision.pdf;/home/carlos/Zotero/storage/QYZ9B53T/2010.html}
}

@misc{IntroducingChatGPT,
  title = {Introducing {{ChatGPT}}},
  url = {https://openai.com/blog/chatgpt},
  urldate = {2024-01-28},
  abstract = {We've trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.},
  language = {en-US},
  file = {/home/carlos/Zotero/storage/DJM6NATU/chatgpt.html}
}

@misc{IntroducingClaude,
  title = {Introducing {{Claude}}},
  url = {https://www.anthropic.com/news/introducing-claude},
  urldate = {2024-01-28},
  abstract = {Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.},
  language = {en},
  file = {/home/carlos/Zotero/storage/XLPRF8QQ/introducing-claude.html}
}

@article{InviernoIA2023,
  title = {{Invierno IA}},
  year = {2023},
  month = sep,
  journal = {Wikipedia, la enciclopedia libre},
  url = {https://es.wikipedia.org/w/index.php?title=Invierno_IA&oldid=154109150},
  urldate = {2024-01-28},
  abstract = {En la historia de la inteligencia artificial, un Invierno IA es un per{\'i}odo de reducci{\'o}n de fondos e inter{\'e}s en la investigaci{\'o}n de inteligencia artificial.[1]\hspace{0pt} El t{\'e}rmino fue acu{\~n}ado por analog{\'i}a a la idea del invierno nuclear.[2]\hspace{0pt} El t{\'e}rmino apareci{\'o} por primera vez en 1984 como el tema central de un debate p{\'u}blico en la conferencia anual de la AAAI. Es una reacci{\'o}n en cadena que comienza con el pesimismo de la comunidad de IA, seguido por el pesimismo en la prensa, seguido de un severo recorte en la financiaci{\'o}n, seguido por el final de la investigaci{\'o}n seria.[2]\hspace{0pt} En la conferencia, Roger Schank y Marvin Minsky-dos de los principales investigadores de la IA que hab{\'i}an sobrevivido el "invierno" de la d{\'e}cada de 1970, advirtieron a la comunidad de negocios que el entusiasmo por la IA hab{\'i}a crecido de forma descontrolada en la d{\'e}cada de 1980 y que, sin duda, la decepci{\'o}n ciertamente seguir{\'i}a. Tres a{\~n}os m{\'a}s tarde, la industria de la IA mil millones de d{\'o}lares comenz{\'o} a derrumbarse.[2]\hspace{0pt} El furor es com{\'u}n en diversas tecnolog{\'i}as emergentes, como lo fue la Mania del Ferrocarril o la Burbuja puntocom. El Invierno IA fue un resultado de ese furor, debido a promesas poco realistas por parte de los desarrolladores, expectativas altas de los usuarios finales y una amplia promoci{\'o}n en los medios.[3]\hspace{0pt}  A pesar de la subida y la ca{\'i}da de la reputaci{\'o}n de la IA, se ha continuado desarrollando nuevas tecnolog{\'i}as y exitosas tecnolog{\'i}as. El investigador Rodney Brooks se quejar{\'i}a en 2002 de que "existe este est{\'u}pido mito de que la IA ha fallado, pero la IA esta a su alrededor cada segundo del d{\'i}a." [4]\hspace{0pt} En el 2005, Ray Kurzweil estaba de acuerdo: "Muchos observadores siguen pensando que el invierno IA fue el final de la historia y que nada desde entonces ha venido del campo IA. Sin embargo, hoy en d{\'i}a miles de aplicaciones de la IA est{\'a}n profundamente arraigados en la infraestructura de todas las industrias."[5]\hspace{0pt} EL entusiasmo y optimismo sobre la IA ha aumentado gradualmente desde su punto m{\'a}s bajo en 1990. A partir de la d{\'e}cada del 2010 la Inteligencia artificial (y especialmente el subcampo del Aprendizaje autom{\'a}tico) empez{\'o} a ganar inter{\'e}s por parte de la comunidad de investigaci{\'o}n, lo que llev{\'o} a un auge dram{\'a}tico en el financiamiento y la inversi{\'o}n del sector.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  language = {es},
  annotation = {Page Version ID: 154109150},
  file = {/home/carlos/Zotero/storage/Z4HAAIN6/Invierno_IA.html}
}

@misc{jiangMistral7B2023,
  title = {Mistral {{7B}}},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, L{\'e}lio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timoth{\'e}e and Sayed, William El},
  year = {2023},
  month = oct,
  number = {arXiv:2310.06825},
  eprint = {2310.06825},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.06825},
  url = {http://arxiv.org/abs/2310.06825},
  urldate = {2024-01-28},
  abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: Models and code are available at https://mistral.ai/news/announcing-mistral-7b/},
  file = {/home/carlos/Zotero/storage/HZWHGN5Z/Jiang et al. - 2023 - Mistral 7B.pdf;/home/carlos/Zotero/storage/WIY6DKZE/2310.html}
}

@misc{jonesDoesGPT4Pass2023,
  title = {Does {{GPT-4 Pass}} the {{Turing Test}}?},
  author = {Jones, Cameron and Bergen, Benjamin},
  year = {2023},
  month = oct,
  number = {arXiv:2310.20216},
  eprint = {2310.20216},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.20216},
  url = {http://arxiv.org/abs/2310.20216},
  urldate = {2024-01-27},
  abstract = {We evaluated GPT-4 in a public online Turing Test. The best-performing GPT-4 prompt passed in 41\% of games, outperforming baselines set by ELIZA (27\%) and GPT-3.5 (14\%), but falling short of chance and the baseline set by human participants (63\%). Participants' decisions were based mainly on linguistic style (35\%) and socio-emotional traits (27\%), supporting the idea that intelligence is not sufficient to pass the Turing Test. Participants' demographics, including education and familiarity with LLMs, did not predict detection rate, suggesting that even those who understand systems deeply and interact with them frequently may be susceptible to deception. Despite known limitations as a test of intelligence, we argue that the Turing Test continues to be relevant as an assessment of naturalistic communication and deception. AI models with the ability to masquerade as humans could have widespread societal consequences, and we analyse the effectiveness of different strategies and criteria for judging humanlikeness.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  note = {Comment: 25 pages, 21 figures},
  file = {/home/carlos/Zotero/storage/BQ9SQ2XE/Jones y Bergen - 2023 - Does GPT-4 Pass the Turing Test.pdf;/home/carlos/Zotero/storage/6CFU2PUC/2310.html}
}

@misc{jzh2074,
  title = {{{AI}} Types. {{Tipos}} Inteligencia {{Artificial}}.Svg},
  author = {{Jzh2074}},
  year = {2022},
  url = {https://commons.wikimedia.org/wiki/File:AI_Types._Tipos_Inteligencia_Artificial.svg}
}

@misc{kaddourChallengesApplicationsLarge2023,
  title = {Challenges and {{Applications}} of {{Large Language Models}}},
  author = {Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and Bradley, Herbie and Raileanu, Roberta and McHardy, Robert},
  year = {2023},
  month = jul,
  number = {arXiv:2307.10169},
  eprint = {2307.10169},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.10169},
  url = {http://arxiv.org/abs/2307.10169},
  urldate = {2023-09-27},
  abstract = {Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field's current state more quickly and become productive.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: 72 pages. v01. Work in progress. Feedback and comments are highly appreciated!},
  file = {/home/carlos/Zotero/storage/N4TI6Q5V/Kaddour et al. - 2023 - Challenges and Applications of Large Language Mode.pdf;/home/carlos/Zotero/storage/EWGBTNGU/2307.html}
}

@misc{kainatIntroductionGenerativeAI2023,
  title = {Introduction to {{Generative AI}}},
  author = {Kainat},
  year = {2023},
  month = aug,
  journal = {Medium},
  url = {https://medium.com/@kitkat73275/introduction-to-generative-ai-833c9c467dfa},
  urldate = {2024-01-23},
  abstract = {AI (Artificial Intelligence) is the broad field of creating intelligent machines that can perform tasks that typically require human{\dots}},
  language = {en},
  file = {/home/carlos/Zotero/storage/HANH4MKW/introduction-to-generative-ai-833c9c467dfa.html}
}

@misc{kirkbrideQirkyFoxDot2023,
  title = {Qirky/{{FoxDot}}},
  author = {Kirkbride, Ryan},
  year = {2023},
  month = dec,
  url = {https://github.com/Qirky/FoxDot},
  urldate = {2023-12-09},
  abstract = {Python driven environment for Live Coding}
}

@misc{lewisRetrievalAugmentedGenerationKnowledgeIntensive2021,
  title = {Retrieval-{{Augmented Generation}} for {{Knowledge-Intensive NLP Tasks}}},
  author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
  year = {2021},
  month = apr,
  number = {arXiv:2005.11401},
  eprint = {2005.11401},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2005.11401},
  url = {http://arxiv.org/abs/2005.11401},
  urldate = {2023-11-07},
  abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: Accepted at NeurIPS 2020},
  file = {/home/carlos/Zotero/storage/WVAFYFB3/Lewis et al. - 2021 - Retrieval-Augmented Generation for Knowledge-Inten.pdf;/home/carlos/Zotero/storage/C495Y3UY/2005.html}
}

@misc{liSelfAlignmentInstructionBacktranslation2023,
  title = {Self-{{Alignment}} with {{Instruction Backtranslation}}},
  author = {Li, Xian and Yu, Ping and Zhou, Chunting and Schick, Timo and Zettlemoyer, Luke and Levy, Omer and Weston, Jason and Lewis, Mike},
  year = {2023},
  month = aug,
  number = {arXiv:2308.06259},
  eprint = {2308.06259},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.06259},
  url = {http://arxiv.org/abs/2308.06259},
  urldate = {2023-09-27},
  abstract = {We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/JADHWQUA/Li et al. - 2023 - Self-Alignment with Instruction Backtranslation.pdf;/home/carlos/Zotero/storage/4SRRSDIW/2308.html}
}

@article{ListAudioProgramming2023,
  title = {List of Audio Programming Languages},
  year = {2023},
  month = jul,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=List_of_audio_programming_languages&oldid=1167204231},
  urldate = {2023-12-11},
  abstract = {This is a list of notable programming languages optimized for sound production, algorithmic composition, and sound synthesis. ABC notation, a language for notating music using the ASCII character set Bol Processor, a model of formal grammars enriched with polymetric expressions for the representation of time structures ChucK, strongly timed, concurrent, and on-the-fly audio programming language Real-time Cmix, a MUSIC-N synthesis language somewhat similar to Csound Cmajor, a high-performance JIT-compiled C-style language for DSP Common Lisp Music (CLM), a music synthesis and signal processing package in the Music V family Csound, a MUSIC-N synthesis language released under the LGPL with many available unit generators Extempore, a live-coding environment that borrows a core foundation from the Impromptu environment FAUST, Functional Audio Stream, a functional compiled language for efficient real-time audio signal processing GLICOL, a graph-oriented live coding language written in Rust Hierarchical Music Specification Language (HMSL), optimized more for music than synthesis, developed in the 1980s in Forth Impromptu, a Scheme language environment for Mac OS X capable of sound and video synthesis, algorithmic composition, and 2D and 3D graphics programming Ixi lang, a programming language for live coding musical expression. JFugue, a Java and JVM library for programming music that outputs to MIDI and has the ability to convert to formats including ABC Notation, Lilypond, and MusicXML jMusic JSyn Keykit, programming language and portable graphical environment for MIDI music composition Kyma (sound design language) LilyPond, a computer program and file format for music engraving. Max/MSP, a proprietary, modular visual programming language aimed at sound synthesis for music Music Macro Language (MML), often used to produce chiptune music in Japan MUSIC-N, includes versions I, II, III, IV, IV-B, IV-BF, V, 11, and 360 Nyquist OpenMusic Orca (music programming language) Pure Data, a modular visual programming language for signal processing aimed at music creation Reaktor Sonic Pi Structured Audio Orchestra Language (SAOL), part of the MPEG-4 Structured Audio standard SuperCollider SynthEdit, a modular visual programming language for signal processing aimed at creating audio plug-ins},
  copyright = {Creative Commons Attribution-ShareAlike License},
  language = {en},
  annotation = {Page Version ID: 1167204231},
  file = {/home/carlos/Zotero/storage/GFR76MYV/List_of_audio_programming_languages.html}
}

@misc{liStructuredChainofThoughtPrompting2023,
  title = {Structured {{Chain-of-Thought Prompting}} for {{Code Generation}}},
  author = {Li, Jia and Li, Ge and Li, Yongmin and Jin, Zhi},
  year = {2023},
  month = sep,
  number = {arXiv:2305.06599},
  eprint = {2305.06599},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2305.06599},
  urldate = {2023-10-23},
  abstract = {Large Language Models (LLMs) (e.g., ChatGPT) have shown impressive performance in code generation. LLMs take prompts as inputs, and Chain-of-Thought (CoT) prompting is the state-of-theart prompting technique. CoT prompting asks LLMs first to generate CoTs (i.e., intermediate natural language reasoning steps) and then output the code. However, CoT prompting is designed for natural language generation and has low accuracy in code generation.},
  archiveprefix = {arxiv},
  language = {en},
  keywords = {Computer Science - Computation and Language,Computer Science - Software Engineering},
  note = {Comment: arXiv admin note: text overlap with arXiv:2303.17780},
  file = {/home/carlos/Zotero/storage/KQ5L5V5X/Li et al. - 2023 - Structured Chain-of-Thought Prompting for Code Gen.pdf}
}

@misc{liSyntheticDataGeneration2023,
  title = {Synthetic {{Data Generation}} with {{Large Language Models}} for {{Text Classification}}: {{Potential}} and {{Limitations}}},
  shorttitle = {Synthetic {{Data Generation}} with {{Large Language Models}} for {{Text Classification}}},
  author = {Li, Zhuoyan and Zhu, Hangxiao and Lu, Zhuoran and Yin, Ming},
  year = {2023},
  month = oct,
  number = {arXiv:2310.07849},
  eprint = {2310.07849},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2310.07849},
  urldate = {2024-01-28},
  abstract = {The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. To better understand factors that moderate the effectiveness of the LLMgenerated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data. We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM for synthetic data generation1.},
  archiveprefix = {arxiv},
  language = {en},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  note = {Comment: EMNLP 2023},
  file = {/home/carlos/Zotero/storage/PRD9A6NF/Li et al. - 2023 - Synthetic Data Generation with Large Language Models for Text Classification Potential and Limitati.pdf}
}

@misc{liuLostMiddleHow2023,
  title = {Lost in the {{Middle}}: {{How Language Models Use Long Contexts}}},
  shorttitle = {Lost in the {{Middle}}},
  author = {Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  year = {2023},
  month = nov,
  number = {arXiv:2307.03172},
  eprint = {2307.03172},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2307.03172},
  urldate = {2023-12-12},
  abstract = {While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.},
  archiveprefix = {arxiv},
  language = {en},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: 18 pages, 16 figures. Accepted for publication in Transactions of the Association for Computational Linguistics (TACL), 2023},
  file = {/home/carlos/Zotero/storage/45RW8HRL/Liu et al. - 2023 - Lost in the Middle How Language Models Use Long C.pdf}
}

@misc{liuWavJourneyCompositionalAudio2023,
  title = {{{WavJourney}}: {{Compositional Audio Creation}} with {{Large Language Models}}},
  shorttitle = {{{WavJourney}}},
  author = {Liu, Xubo and Zhu, Zhongkai and Liu, Haohe and Yuan, Yi and Cui, Meng and Huang, Qiushi and Liang, Jinhua and Cao, Yin and Kong, Qiuqiang and Plumbley, Mark D. and Wang, Wenwu},
  year = {2023},
  month = nov,
  number = {arXiv:2307.14335},
  eprint = {2307.14335},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2307.14335},
  urldate = {2024-01-04},
  abstract = {Despite breakthroughs in audio generation models, their capabilities are often confined to domain-specific conditions such as speech transcriptions and audio captions. However, real-world audio creation aims to generate harmonious audio containing various elements such as speech, music, and sound effects with controllable conditions, which is challenging to address using existing audio generation systems. We present WavJourney, a novel framework that leverages Large Language Models (LLMs) to connect various audio models for audio creation. WavJourney allows users to create storytelling audio content with diverse audio elements simply from textual descriptions. Specifically, given a text instruction, WavJourney first prompts LLMs to generate an audio script that serves as a structured semantic representation of audio elements. The audio script is then converted into a computer program, where each line of the program calls a task-specific audio generation model or computational operation function. The computer program is then executed to obtain a compositional and interpretable solution for audio creation. Experimental results suggest that WavJourney is capable of synthesizing realistic audio aligned with textually-described semantic, spatial and temporal conditions, achieving state-of-the-art results on text-toaudio generation benchmarks. Additionally, we introduce a new multi-genre story benchmark. Subjective evaluations demonstrate the potential of WavJourney in crafting engaging storytelling audio content from text. We further demonstrate that WavJourney can facilitate human-machine co-creation in multi-round dialogues. To foster future research, the code and synthesized audio are available at: https://audio-agi.github.io/WavJourney\_demopage/.},
  archiveprefix = {arxiv},
  language = {en},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multimedia,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  note = {Comment: GitHub: https://github.com/Audio-AGI/WavJourney},
  file = {/home/carlos/Zotero/storage/FT53GGQ9/Liu et al. - 2023 - WavJourney Compositional Audio Creation with Large Language Models.pdf}
}

@misc{LiveCodeTidal,
  title = {Live Code with {{Tidal Cycles}} {\textbar} {{Tidal Cycles}}},
  url = {https://tidalcycles.org/},
  urldate = {2023-12-09},
  abstract = {Live coding environment for making algorithmic patterns},
  language = {en},
  file = {/home/carlos/Zotero/storage/6QLETVM5/tidalcycles.org.html}
}

@misc{LLMPromptingGuide,
  title = {{{LLM}} Prompting Guide},
  url = {https://huggingface.co/docs/transformers/main/en/tasks/prompting},
  urldate = {2023-10-30},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  language = {English},
  file = {/home/carlos/Zotero/storage/MLZ8EYCS/prompting.html}
}

@misc{LLMTanSuoGPTLeiMoXingDeJiGeChangYongCanShuTopk,
  title = {{LLM探索：GPT类模型的几个常用参数 Top-k, Top-p, Temperature}},
  journal = {知乎专栏},
  url = {https://zhuanlan.zhihu.com/p/631786282},
  urldate = {2024-01-30},
  abstract = {前言上一篇文章介绍了几个开源LLM的环境搭建和本地部署，在使用ChatGPT接口或者自己本地部署的LLM大模型的时候，经常会遇到这几个参数，本文简单介绍一下{\textasciitilde} temperaturetop\_ptop\_k关于LLM上一篇也有介绍过，这次看{\dots}},
  language = {zh},
  file = {/home/carlos/Zotero/storage/M942PT2V/631786282.html}
}

@misc{luMuseCocoGeneratingSymbolic2023,
  title = {{{MuseCoco}}: {{Generating Symbolic Music}} from {{Text}}},
  shorttitle = {{{MuseCoco}}},
  author = {Lu, Peiling and Xu, Xin and Kang, Chenfei and Yu, Botao and Xing, Chengyi and Tan, Xu and Bian, Jiang},
  year = {2023},
  month = may,
  number = {arXiv:2306.00110},
  eprint = {2306.00110},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2306.00110},
  urldate = {2024-01-04},
  abstract = {Generating music from text descriptions is a user-friendly mode since the text is a relatively easy interface for user engagement. While some approaches utilize texts to control music audio generation, editing musical elements in generated audio is challenging for users. In contrast, symbolic music offers ease of editing, making it more accessible for users to manipulate specific musical elements. In this paper, we propose MuseCoco, which generates symbolic music from text descriptions with musical attributes as the bridge to break down the task into text-to-attribute understanding and attribute-to-music generation stages. MuseCoCo stands for Music Composition Copilot that empowers musicians to generate music directly from given text descriptions, offering a significant improvement in efficiency compared to creating music entirely from scratch. The system has two main advantages: Firstly, it is data efficient. In the attribute-to-music generation stage, the attributes can be directly extracted from music sequences, making the model training self-supervised. In the text-to-attribute understanding stage, the text is synthesized and refined by ChatGPT based on the defined attribute templates. Secondly, the system can achieve precise control with specific attributes in text descriptions and offers multiple control options through attribute-conditioned or text-conditioned approaches. MuseCoco outperforms baseline systems in terms of musicality, controllability, and overall score by at least 1.27, 1.08, and 1.32 respectively. Besides, there is a notable enhancement of about 20\% in objective control accuracy. In addition, we have developed a robust large-scale model with 1.2 billion parameters, showcasing exceptional controllability and musicality. Music samples generated by MuseCoco are available via this link 1, and the code is available at this link 2.},
  archiveprefix = {arxiv},
  language = {en},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Multimedia,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/carlos/Zotero/storage/PW28ZPDB/Lu et al. - 2023 - MuseCoco Generating Symbolic Music from Text.pdf}
}

@misc{malachAutoRegressiveNextTokenPredictors2023,
  title = {Auto-{{Regressive Next-Token Predictors}} Are {{Universal Learners}}},
  author = {Malach, Eran},
  year = {2023},
  month = sep,
  number = {arXiv:2309.06979},
  eprint = {2309.06979},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2309.06979},
  urldate = {2023-09-27},
  abstract = {Large language models display remarkable capabilities in logical and mathematical reasoning, allowing them to solve complex tasks. Interestingly, these abilities emerge in networks trained on the simple task of next-token prediction. In this work, we present a theoretical framework for studying auto-regressive next-token predictors. We demonstrate that even simple models such as linear next-token predictors, trained on Chain-ofThought (CoT) data, can approximate any function efficiently computed by a Turing machine. We introduce a new complexity measure{\textemdash}length complexity{\textemdash}which measures the number of intermediate tokens in a CoT sequence required to approximate some target function, and analyze the interplay between length complexity and other notions of complexity. Finally, we show experimentally that simple next-token predictors, such as linear networks and shallow Multi-Layer Perceptrons (MLPs), display non-trivial performance on text generation and arithmetic tasks. Our results demonstrate that the power of language models can be attributed, to a great extent, to the auto-regressive next-token training scheme, and not necessarily to a particular choice of architecture.},
  archiveprefix = {arxiv},
  language = {en},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/carlos/Zotero/storage/3B7X3RYX/Malach - 2023 - Auto-Regressive Next-Token Predictors are Universa.pdf}
}

@article{manickamResearchStudyApplications2017,
  title = {Research Study on Applications of Artificial Neural Networks and E-Learning Personalization},
  author = {Manickam, M.R.M. and Mohanapriya, M. and Kale, Sandip and Uday, M. and Kulkarni, Prashant and Khandagale, Y. and Patil, S.P.},
  year = {2017},
  month = aug,
  journal = {International Journal of Civil Engineering and Technology},
  volume = {8},
  pages = {1422--1432},
  abstract = {The artificial neural network may likely be the complete solution over the most recent decades which have been broadly utilized as a part of a huge variety of applications. This article focuses on vast artificial neural network applications and importance of e-Learning application. To assess the impact of personalized learning in neural network applications. It is a need to adapt in new smart eLearning system for individual learners personalization. Artificial Neural Networks methodology to the development of new neural network model with an appropriate way of problems formulation is presented in this paper. Student's performance prediction using neural system its impact is presented to understand the necessity of neural network in smart e-learning model. The outcome focused on the importance of using neural networks in possible applications and its influence on learner's progress with personalization system.},
  file = {/home/carlos/Zotero/storage/WR3XIC7Q/Manickam et al. - 2017 - Research study on applications of artificial neural networks and e-learning personalization.pdf}
}

@misc{mastropaoloRobustnessCodeGeneration2023,
  title = {On the {{Robustness}} of {{Code Generation Techniques}}: {{An Empirical Study}} on {{GitHub Copilot}}},
  shorttitle = {On the {{Robustness}} of {{Code Generation Techniques}}},
  author = {Mastropaolo, Antonio and Pascarella, Luca and Guglielmi, Emanuela and Ciniselli, Matteo and Scalabrino, Simone and Oliveto, Rocco and Bavota, Gabriele},
  year = {2023},
  month = feb,
  number = {arXiv:2302.00438},
  eprint = {2302.00438},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2302.00438},
  urldate = {2023-12-26},
  abstract = {Software engineering research has always being concerned with the improvement of code completion approaches, which suggest the next tokens a developer will likely type while coding. The release of GitHub Copilot constitutes a big step forward, also because of its unprecedented ability to automatically generate even entire functions from their natural language description. While the usefulness of Copilot is evident, it is still unclear to what extent it is robust. Specifically, we do not know the extent to which semantic-preserving changes in the natural language description provided to the model have an effect on the generated code function. In this paper we present an empirical study in which we aim at understanding whether different but semantically equivalent natural language descriptions result in the same recommended function. A negative answer would pose questions on the robustness of deep learning (DL)-based code generators since it would imply that developers using different wordings to describe the same code would obtain different recommendations. We asked Copilot to automatically generate 892 Java methods starting from their original Javadoc description. Then, we generated different semantically equivalent descriptions for each method both manually and automatically, and we analyzed the extent to which predictions generated by Copilot changed. Our results show that modifying the description results in different code recommendations in {\textasciitilde}46\% of cases. Also, differences in the semantically equivalent descriptions might impact the correctness of the generated code {\textasciitilde}28\%.},
  archiveprefix = {arxiv},
  language = {en},
  keywords = {Computer Science - Software Engineering},
  file = {/home/carlos/Zotero/storage/HYGTSJJX/Mastropaolo et al. - 2023 - On the Robustness of Code Generation Techniques A.pdf}
}

@misc{mehdiAnnouncingMicrosoftCopilot2023,
  title = {Announcing {{Microsoft Copilot}}, Your Everyday {{AI}} Companion},
  author = {Mehdi, Yusuf},
  year = {2023},
  month = sep,
  journal = {The Official Microsoft Blog},
  url = {https://blogs.microsoft.com/blog/2023/09/21/announcing-microsoft-copilot-your-everyday-ai-companion/},
  urldate = {2024-01-28},
  abstract = {Updated November 15, 2023: To simplify the user experience and make Copilot more accessible to everyone, Bing Chat and Bing Chat Enterprise will now simply become Microsoft Copilot. For more information: https://aka.ms/BingIgnite We are entering a new era of AI, one that is fundamentally changing how we relate to and benefit from technology. With the...},
  language = {en-US},
  file = {/home/carlos/Zotero/storage/ED7X9C4H/announcing-microsoft-copilot-your-everyday-ai-companion.html}
}

@misc{michaelisBroadDatasetAll2022,
  title = {A {{Broad Dataset}} Is {{All You Need}} for {{One-Shot Object Detection}}},
  author = {Michaelis, Claudio and Bethge, Matthias and Ecker, Alexander S.},
  year = {2022},
  month = oct,
  number = {arXiv:2011.04267},
  eprint = {2011.04267},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2011.04267},
  url = {http://arxiv.org/abs/2011.04267},
  urldate = {2024-01-22},
  abstract = {Is it possible to detect arbitrary objects from a single example? A central problem of all existing attempts at one-shot object detection is the generalization gap: Object categories used during training are detected much more reliably than novel ones. We here show that this generalization gap can be nearly closed by increasing the number of object categories used during training. Doing so allows us to improve generalization from seen to unseen classes from 45\% to 89\% and improve the state-of-the-art on COCO by 5.4 \%AP50 (from 22.0 to 27.5). We verify that the effect is caused by the number of categories and not the number of training samples, and that it holds for different models, backbones and datasets. This result suggests that the key to strong few-shot detection models may not lie in sophisticated metric learning approaches, but instead simply in scaling the number of categories. We hope that our findings will help to better understand the challenges of few-shot learning and encourage future data annotation efforts to focus on wider datasets with a broader set of categories rather than gathering more samples per category.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/carlos/Zotero/storage/TTGXP52I/Michaelis et al. - 2022 - A Broad Dataset is All You Need for One-Shot Object Detection.pdf;/home/carlos/Zotero/storage/ZSWF3IMF/2011.html}
}

@book{minsky1969perceptrons,
  title = {Perceptrons: An Introduction to Computational Geometry},
  author = {Minsky, M. and Papert, S.},
  year = {1969},
  publisher = {{MIT Press}},
  url = {https://books.google.es/books?id=Ow1OAQAAIAAJ},
  isbn = {978-0-262-63022-1},
  lccn = {69014379}
}

@book{minskyPerceptronsIntroductionComputational2017,
  title = {Perceptrons: {{An Introduction}} to {{Computational Geometry}}},
  shorttitle = {Perceptrons},
  author = {Minsky, Marvin and Papert, Seymour A.},
  year = {2017},
  month = sep,
  publisher = {{The MIT Press}},
  doi = {10.7551/mitpress/11301.001.0001},
  url = {https://direct.mit.edu/books/book/3132/PerceptronsAn-Introduction-to-Computational},
  urldate = {2023-12-21},
  abstract = {The first systematic study of parallelism in computation by two pioneers in the field.Reissue of the 1988 Expanded Edition with a new foreword by L{\'e}on BottouIn},
  isbn = {978-0-262-34393-0},
  language = {en},
  file = {/home/carlos/Zotero/storage/VUWTY5QW/PerceptronsAn-Introduction-to-Computational.html}
}

@misc{mishkinAllYouNeed2016,
  title = {All You Need Is a Good Init},
  author = {Mishkin, Dmytro and Matas, Jiri},
  year = {2016},
  month = feb,
  number = {arXiv:1511.06422},
  eprint = {1511.06422},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1511.06422},
  url = {http://arxiv.org/abs/1511.06422},
  urldate = {2024-01-22},
  abstract = {Layer-sequential unit-variance (LSUV) initialization - a simple method for weight initialization for deep net learning - is proposed. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one. Experiment with different activation functions (maxout, ReLU-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)). Performance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  note = {Comment: Published as a conference paper at ICLR 2016},
  file = {/home/carlos/Zotero/storage/Z8PT92DE/Mishkin y Matas - 2016 - All you need is a good init.pdf;/home/carlos/Zotero/storage/FE846CPB/1511.html}
}

@article{ModelacionLenguaje2024,
  title = {{Modelaci{\'o}n del lenguaje}},
  year = {2024},
  month = jan,
  journal = {Wikipedia, la enciclopedia libre},
  url = {https://es.wikipedia.org/w/index.php?title=Modelaci%C3%B3n_del_lenguaje&oldid=157157282},
  urldate = {2024-01-24},
  abstract = {Un modelo estad{\'i}stico de lenguaje asigna una probabilidad a una secuencia de m palabras                         P         (                    w                        1                             ,         {\dots}         ,                    w                        m                             )                 \{{\textbackslash}displaystyle P(w\_\{1\},{\textbackslash}ldots ,w\_\{m\})\}    mediante una distribuci{\'o}n de probabilidad. Contar con una forma de estimar la verosimilitud de diversas frases resulta sumamente {\'u}til en una variedad de aplicaciones dentro del procesamiento del lenguaje natural. Los modelos de lenguaje se emplean en el reconocimiento de voz, la traducci{\'o}n autom{\'a}tica, el etiquetado de discurso, el an{\'a}lisis de texto, el reconocimiento de escritura, la recuperaci{\'o}n de informaci{\'o}n y otras muchas aplicaciones.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  language = {es},
  annotation = {Page Version ID: 157157282},
  file = {/home/carlos/Zotero/storage/LTFQTHQH/Modelación_del_lenguaje.html}
}

@misc{mohanStructureReinforcementLearning2023,
  title = {Structure in {{Reinforcement Learning}}: {{A Survey}} and {{Open Problems}}},
  shorttitle = {Structure in {{Reinforcement Learning}}},
  author = {Mohan, Aditya and Zhang, Amy and Lindauer, Marius},
  year = {2023},
  month = aug,
  number = {arXiv:2306.16021},
  eprint = {2306.16021},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.16021},
  url = {http://arxiv.org/abs/2306.16021},
  urldate = {2024-01-28},
  abstract = {Reinforcement Learning (RL), bolstered by the expressive capabilities of Deep Neural Networks (DNNs) for function approximation, has demonstrated considerable success in numerous applications. However, its practicality in addressing various real-world scenarios, characterized by diverse and unpredictable dynamics, noisy signals, and large state and action spaces, remains limited. This limitation stems from issues such as poor data efficiency, limited generalization capabilities, a lack of safety guarantees, and the absence of interpretability, among other factors. To overcome these challenges and improve performance across these crucial metrics, one promising avenue is to incorporate additional structural information about the problem into the RL learning process. Various sub-fields of RL have proposed methods for incorporating such inductive biases. We amalgamate these diverse methodologies under a unified framework, shedding light on the role of structure in the learning problem, and classify these methods into distinct patterns of incorporating structure. By leveraging this comprehensive framework, we provide valuable insights into the challenges of structured RL and lay the groundwork for a design pattern perspective on RL research. This novel perspective paves the way for future advancements and aids in developing more effective and efficient RL algorithms that can potentially handle real-world scenarios better.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/carlos/Zotero/storage/GJR7LZLG/Mohan et al. - 2023 - Structure in Reinforcement Learning A Survey and Open Problems.pdf;/home/carlos/Zotero/storage/MR6TEXIV/2306.html}
}

@article{moradidakhelGitHubCopilotAI2023,
  title = {{{GitHub Copilot AI}} Pair Programmer: {{Asset}} or {{Liability}}?},
  shorttitle = {{{GitHub Copilot AI}} Pair Programmer},
  author = {Moradi Dakhel, Arghavan and Majdinasab, Vahid and Nikanjam, Amin and Khomh, Foutse and Desmarais, Michel C. and Jiang, Zhen Ming (Jack)},
  year = {2023},
  month = sep,
  journal = {Journal of Systems and Software},
  volume = {203},
  pages = {111734},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2023.111734},
  url = {https://www.sciencedirect.com/science/article/pii/S0164121223001292},
  urldate = {2023-12-26},
  abstract = {Automatic program synthesis is a long-lasting dream in software engineering. Recently, a promising Deep Learning (DL) based solution, called Copilot, has been proposed by OpenAI and Microsoft as an industrial product. Although some studies evaluate the correctness of Copilot solutions and report its issues, more empirical evaluations are necessary to understand how developers can benefit from it effectively. In this paper, we study the capabilities of Copilot in two different programming tasks: (i) generating (and reproducing) correct and efficient solutions for fundamental algorithmic problems, and (ii) comparing Copilot's proposed solutions with those of human programmers on a set of programming tasks. For the former, we assess the performance and functionality of Copilot in solving selected fundamental problems in computer science, like sorting and implementing data structures. In the latter, a dataset of programming problems with human-provided solutions is used. The results show that Copilot is capable of providing solutions for almost all fundamental algorithmic problems, however, some solutions are buggy and non-reproducible. Moreover, Copilot has some difficulties in combining multiple methods to generate a solution. Comparing Copilot to humans, our results show that the correct ratio of humans' solutions is greater than Copilot's suggestions, while the buggy solutions generated by Copilot require less effort to be repaired. Based on our findings, if Copilot is used by expert developers in software projects, it can become an asset since its suggestions could be comparable to humans' contributions in terms of quality. However, Copilot can become a liability if it is used by novice developers who may fail to filter its buggy or non-optimal solutions due to a lack of expertise.},
  keywords = {Code completion,GitHub copilot,Language model,Testing},
  file = {/home/carlos/Zotero/storage/UZ645QAQ/Moradi Dakhel et al. - 2023 - GitHub Copilot AI pair programmer Asset or Liabil.pdf;/home/carlos/Zotero/storage/U3TB2FHT/S0164121223001292.html}
}

@article{mukherjeeOrcaProgressiveLearning2023,
  title = {Orca: {{Progressive Learning}} from {{Complex Explanation Traces}} of {{GPT-4}}},
  shorttitle = {Orca},
  author = {Mukherjee, Subhabrata (Subho) and Mitra, Arindam and Jawahar, Ganesh and Agarwal, Sahaj and Palangi, Hamid and Awadallah, Ahmed H.},
  year = {2023},
  month = jun,
  url = {https://www.microsoft.com/en-us/research/publication/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4/},
  urldate = {2023-06-25},
  abstract = {Recent research has focused on enhancing the capability of smaller models through imitation~learning, drawing on the outputs generated by large foundation models (LFMs). A number of issues impact the quality of these models, ranging from~limited imitation signals~from shallow LFM outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting in [{\dots}]},
  language = {en-US},
  file = {/home/carlos/Zotero/storage/ANQ2XMEH/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4.html}
}

@misc{MusicLM,
  title = {{{MusicLM}}},
  url = {https://google-research.github.io/seanet/musiclm/examples/},
  urldate = {2024-02-05}
}

@article{nielsenNeuralNetworksDeep2015,
  title = {Neural {{Networks}} and {{Deep Learning}}},
  author = {Nielsen, Michael A.},
  year = {2015},
  publisher = {{Determination Press}},
  url = {http://neuralnetworksanddeeplearning.com},
  urldate = {2023-05-24},
  language = {en},
  file = {/home/carlos/Zotero/storage/TZXGVB5T/index.html}
}

@misc{nishiKentoNishiAwesomeallyouneedpapers2024,
  title = {{{KentoNishi}}/Awesome-All-You-Need-Papers},
  author = {Nishi, Kento},
  year = {2024},
  month = jan,
  url = {https://github.com/KentoNishi/awesome-all-you-need-papers},
  urldate = {2024-01-27},
  abstract = {A list of all "all you need" papers. Updated daily using the arXiv API.},
  keywords = {arxiv,awesome-list,machine-learning,papers}
}

@misc{noeverTuringDeception2022,
  title = {The {{Turing Deception}}},
  author = {Noever, David and Ciolino, Matt},
  year = {2022},
  month = dec,
  number = {arXiv:2212.06721},
  eprint = {2212.06721},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.06721},
  url = {http://arxiv.org/abs/2212.06721},
  urldate = {2024-01-27},
  abstract = {This research revisits the classic Turing test and compares recent large language models such as ChatGPT for their abilities to reproduce human-level comprehension and compelling text generation. Two task challenges -- summarization, and question answering -- prompt ChatGPT to produce original content (98-99\%) from a single text entry and also sequential questions originally posed by Turing in 1950. We score the original and generated content against the OpenAI GPT-2 Output Detector from 2019, and establish multiple cases where the generated content proves original and undetectable (98\%). The question of a machine fooling a human judge recedes in this work relative to the question of "how would one prove it?" The original contribution of the work presents a metric and simple grammatical set for understanding the writing mechanics of chatbots in evaluating their readability and statistical clarity, engagement, delivery, and overall quality. While Turing's original prose scores at least 14\% below the machine-generated output, the question of whether an algorithm displays hints of Turing's truly original thoughts (the "Lovelace 2.0" test) remains unanswered and potentially unanswerable for now.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/carlos/Zotero/storage/MTI49WUJ/Noever y Ciolino - 2022 - The Turing Deception.pdf;/home/carlos/Zotero/storage/JKLFGCTV/2212.html}
}

@phdthesis{nordgrenPROMPTENGINEERINGITS2023,
  title = {{{PROMPT ENGINEERING AND ITS USABILITY TO IMPROVE MODERN PSYCHOLOGY CHATBOTS}}},
  author = {Nordgren, Isak J and Svensson, L Gustaf E},
  year = {2023},
  abstract = {As advancements in chatbots and Large Language Models (LLMs) such as GPT-3.5 and GPT-4 continue, their applications in diverse fields, including psychology, expand. This study investigates the effectiveness of LLMs optimized through prompt engineering, aiming to enhance their performance in psychological applications. To this end, two distinct versions of a GPT-3.5-based chatbot were developed: a version similar to the base model, and a version equipped with a more extensive system prompt detailing expected behavior.},
  language = {en},
  file = {/home/carlos/Zotero/storage/PXI4SHRK/Nordgren y Svensson - PROMPT ENGINEERING AND ITS USABILITY TO IMPROVE MODERN PSYCHOLOGY CHATBOTS.pdf}
}

@article{NurArtificialNeural2014,
  title = {Artificial Neural Network Weight Optimization: {{A}} Review},
  author = {Nur, Abdirashid and Radzi, Nor and Osman, Ashraf},
  year = {2014},
  month = sep,
  journal = {TELKOMNIKA},
  volume = {12},
  doi = {10.11591/telkomnika.v12i9.6264}
}

@misc{osheaIntroductionConvolutionalNeural2015,
  title = {An {{Introduction}} to {{Convolutional Neural Networks}}},
  author = {O'Shea, Keiron and Nash, Ryan},
  year = {2015},
  month = dec,
  number = {arXiv:1511.08458},
  eprint = {1511.08458},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1511.08458},
  url = {http://arxiv.org/abs/1511.08458},
  urldate = {2024-01-28},
  abstract = {The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs. This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  note = {Comment: 10 pages, 5 figures},
  file = {/home/carlos/Zotero/storage/DVHFDFAY/O'Shea y Nash - 2015 - An Introduction to Convolutional Neural Networks.pdf;/home/carlos/Zotero/storage/6A74TP2C/1511.html}
}

@misc{OvertoneCollaborativeProgrammable,
  title = {Overtone - {{Collaborative Programmable Music}}},
  url = {https://overtone.github.io/},
  urldate = {2023-12-09},
  file = {/home/carlos/Zotero/storage/LXF2GLUX/overtone.github.io.html}
}

@misc{PapersCodeAudioLM,
  title = {Papers with {{Code}} - {{AudioLM}}: A {{Language Modeling Approach}} to {{Audio Generation}}},
  shorttitle = {Papers with {{Code}} - {{AudioLM}}},
  url = {https://paperswithcode.com/paper/audiolm-a-language-modeling-approach-to-audio},
  urldate = {2024-01-04},
  abstract = {Implemented in 4 code libraries.},
  language = {en},
  file = {/home/carlos/Zotero/storage/U33C57MP/audiolm-a-language-modeling-approach-to-audio.html}
}

@misc{PapersCodeMuseCoco,
  title = {Papers with {{Code}} - {{MuseCoco}}: {{Generating Symbolic Music}} from {{Text}}},
  shorttitle = {Papers with {{Code}} - {{MuseCoco}}},
  url = {https://paperswithcode.com/paper/musecoco-generating-symbolic-music-from-text},
  urldate = {2024-01-04},
  abstract = {Implemented in one code library.},
  language = {en},
  file = {/home/carlos/Zotero/storage/WCM7YULJ/musecoco-generating-symbolic-music-from-text.html}
}

@misc{PapersCodeRobustness,
  title = {Papers with {{Code}} - {{On}} the {{Robustness}} of {{Code Generation Techniques}}: {{An Empirical Study}} on {{GitHub Copilot}}},
  shorttitle = {Papers with {{Code}} - {{On}} the {{Robustness}} of {{Code Generation Techniques}}},
  url = {https://cs.paperswithcode.com/paper/on-the-robustness-of-code-generation},
  urldate = {2023-12-26},
  abstract = {Implemented in one code library.},
  language = {en},
  file = {/home/carlos/Zotero/storage/T33YVX3N/on-the-robustness-of-code-generation.html}
}

@misc{PapersCodeWavJourney,
  title = {Papers with {{Code}} - {{WavJourney}}: {{Compositional Audio Creation}} with {{Large Language Models}}},
  shorttitle = {Papers with {{Code}} - {{WavJourney}}},
  url = {https://paperswithcode.com/paper/wavjourney-compositional-audio-creation-with},
  urldate = {2024-01-04},
  abstract = {Implemented in one code library.},
  language = {en},
  file = {/home/carlos/Zotero/storage/WIJE3S3G/wavjourney-compositional-audio-creation-with.html}
}

@misc{pascanuDifficultyTrainingRecurrent2013,
  title = {On the Difficulty of Training {{Recurrent Neural Networks}}},
  author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  year = {2013},
  month = feb,
  number = {arXiv:1211.5063},
  eprint = {1211.5063},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1211.5063},
  urldate = {2024-01-28},
  abstract = {There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
  archiveprefix = {arxiv},
  language = {en},
  keywords = {Computer Science - Machine Learning},
  note = {Comment: Improved description of the exploding gradient problem and description and analysis of the vanishing gradient problem},
  file = {/home/carlos/Zotero/storage/QGYUZ4WR/Pascanu et al. - 2013 - On the difficulty of training Recurrent Neural Networks.pdf}
}

@misc{pengImpactAIDeveloper2023a,
  title = {The {{Impact}} of {{AI}} on {{Developer Productivity}}: {{Evidence}} from {{GitHub Copilot}}},
  shorttitle = {The {{Impact}} of {{AI}} on {{Developer Productivity}}},
  author = {Peng, Sida and Kalliamvakou, Eirini and Cihon, Peter and Demirer, Mert},
  year = {2023},
  month = feb,
  number = {arXiv:2302.06590},
  eprint = {2302.06590},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2302.06590},
  urldate = {2023-12-27},
  abstract = {Generative AI tools hold promise to increase human productivity. This paper presents results from a controlled experiment with GitHub Copilot, an AI pair programmer. Recruited software developers were asked to implement an HTTP server in JavaScript as quickly as possible. The treatment group, with access to the AI pair programmer, completed the task 55.8\% faster than the control group. Observed heterogenous effects show promise for AI pair programmers to help people transition into software development careers.},
  archiveprefix = {arxiv},
  language = {en},
  keywords = {Computer Science - Software Engineering},
  file = {/home/carlos/Zotero/storage/UEPW89PF/Peng et al. - 2023 - The Impact of AI on Developer Productivity Eviden.pdf}
}

@book{penroseNuevaMenteEmperador2015,
  title = {{La nueva mente del emperador}},
  author = {Penrose, Roger},
  year = {2015},
  month = jul,
  publisher = {{Penguin Random House Grupo Editorial Espa{\~n}a}},
  abstract = {Un apasionante paseo por la matem{\'a}tica y la f{\'i}sica, y por los hallazgos del pensamiento humano de la mano de Roger Penrose, Premio Nobel de F{\'i}sica 2020. Durante d{\'e}cadas, los defensores de la inteligencia artificial han mantenido que los ordenadores har{\'a}n pronto todo lo que la mente humana puede hacer. En su favor, se puede utilizar, por ejemplo, el que ya hay m{\'a}quinas que juegan al ajedrez como los grandes maestros. Ahora bien, {\textquestiondown}comprenden el juego como lo hacemos nosotros?En este libro, Roger Penrose, probablemente el especialista en la teor{\'i}a general de la relatividad m{\'a}s prestigioso del mundo y una de las mentes anal{\'i}ticas m{\'a}s originales de la actualidad, sostiene que existen facetas del pensamiento humano que nunca ser{\'a}n emuladas por un ordenador. Para defender esa tesis, Penrose recurre a una amplia gama de conocimientos cient{\'i}ficos, que van desde la m{\'a}quina de Turing hasta la estructura del cerebro, pasando por el teorema de G{\"o}del, los agujeros negros y los blancos, la radiaci{\'o}n de Hawking, la entrop{\'i}a o la mec{\'a}nica cu{\'a}ntica. Entre los numerosos estudios existentes dedicados a la relaci{\'o}n entre la mente y el cuerpo, esta ambiciosa obra sobresale tanto por su lucidez y claridad como por su rigor y profundidad. Rese{\~n}a:{\guillemotleft}Un libro audaz, brillante e innovador. Cuando Penrose habla, los cient{\'i}ficos escuchan.{\guillemotright}The New York Times Book Review},
  googlebooks = {sLz3CQAAQBAJ},
  isbn = {978-84-663-3083-1},
  language = {es},
  keywords = {Computers / Artificial Intelligence / General,Science / Essays,Science / General,Science / Life Sciences / Neuroscience}
}

@misc{perezThreadsYaSupera2023,
  title = {{Threads ya supera los 100 millones de usuarios. Ha aplastado el r{\'e}cord que ten{\'i}a ChatGPT}},
  author = {P{\'e}rez, Enrique},
  year = {2023},
  month = jul,
  journal = {Xataka},
  url = {https://www.xataka.com/aplicaciones/threads-supera-100-millones-usuarios-ha-aplastado-record-que-tenia-chatgpt},
  urldate = {2023-12-08},
  abstract = {La tecnolog{\'i}a va a un ritmo vertiginoso. Threads, el rival de Twitter de Instagram, lleg{\'o} oficialmente la semana pasada. Y aunque para usarlo en Europa hay...},
  chapter = {aplicaciones},
  language = {es},
  file = {/home/carlos/Zotero/storage/9CKSGAN7/threads-supera-100-millones-usuarios-ha-aplastado-record-que-tenia-chatgpt.html}
}

@misc{ph.dTrainingTimeFoundation2023,
  title = {The Training Time of the Foundation Models (from Scratch)},
  author = {Ph.D, Micha{\l} Marci{\'n}czuk},
  year = {2023},
  month = nov,
  journal = {CodeNLP},
  url = {https://medium.com/codenlp/the-training-time-of-the-foundation-models-from-scratch-59bbce90cc87},
  urldate = {2024-01-21},
  abstract = {How many GPU hours does it take to train a large language model (LLM)?},
  language = {en},
  file = {/home/carlos/Zotero/storage/WE2QT4G9/the-training-time-of-the-foundation-models-from-scratch-59bbce90cc87.html}
}

@misc{PrimerosMesesChat2023,
  title = {{Primeros 6 meses del Chat GPT, {\textquestiondown}cu{\'a}les son sus logros y desaf{\'i}os?}},
  year = {2023},
  month = may,
  journal = {Noticias Radio Reflejos},
  url = {https://www.noticiasradioreflejos.com.ar/2021/primeros-6-meses-del-chat-gpt-cuales-son-sus-logros-y-desafios/},
  urldate = {2023-12-08},
  abstract = {Buenos Aires, mayo de 2023 {\textendash} Bruno Ruy{\'u}, profesor de la Maestr{\'i}a de Ciencias de Datos de la Universidad Austral, hace un repaso por los principales logros y desaf{\'i}os del ChatGPT. ``El ChatGPT logr{\'o} alcanzar los 100 millones de usuarios en dos meses, un n{\'u}mero impresionante. Mientras Instagram alcanz{\'o} este record en 30 meses, Spotify [{\dots}]},
  language = {es},
  file = {/home/carlos/Zotero/storage/K7W3P7AF/primeros-6-meses-del-chat-gpt-cuales-son-sus-logros-y-desafios.html}
}

@misc{PromptEngineeringGuide2023,
  title = {Prompt {{Engineering Guide}}},
  year = {2023},
  month = dec,
  url = {https://www.promptingguide.ai/},
  urldate = {2024-01-05},
  abstract = {A Comprehensive Overview of Prompt Engineering},
  language = {en},
  file = {/home/carlos/Zotero/storage/866LWP67/www.promptingguide.ai.html}
}

@misc{PureDataPd,
  title = {Pure {{Data}} {\textemdash} {{Pd Community Site}}},
  url = {https://puredata.info/},
  urldate = {2024-01-28},
  file = {/home/carlos/Zotero/storage/YN45KFD2/puredata.info.html}
}

@misc{QueConsistenModelos,
  title = {{{\textquestiondown}En qu{\'e} consisten los modelos autorregresivos?: Explicaci{\'o}n sobre los modelos autorregresivos: AWS}},
  shorttitle = {{{\textquestiondown}En qu{\'e} consisten los modelos autorregresivos?}},
  journal = {Amazon Web Services, Inc.},
  url = {https://aws.amazon.com/es/what-is/autoregressive-models/},
  urldate = {2024-01-26},
  abstract = {En qu{\'e} consisten los modelos autorregresivos, de qu{\'e} manera y por qu{\'e} las empresas recurren a estos y c{\'o}mo se utilizan con AWS.},
  language = {es-ES},
  file = {/home/carlos/Zotero/storage/6AHZ6W4R/autoregressive-models.html}
}

@misc{QueEsAjuste,
  title = {{{\textquestiondown}Qu{\'e} es el ajuste de hiperpar{\'a}metros?}},
  shorttitle = {{{\textquestiondown}Qu{\'e} es el ajuste de hiperpar{\'a}metros?}},
  journal = {Amazon Web Services, Inc.},
  url = {https://aws.amazon.com/es/what-is/hyperparameter-tuning/},
  urldate = {2024-01-26},
  abstract = {Qu{\'e} es el ajuste de hiperpar{\'a}metros, c{\'o}mo y por qu{\'e} lo utiliza las empresas, y c{\'o}mo se utiliza con AWS.},
  language = {es-ES},
  file = {/home/carlos/Zotero/storage/XYHKEV34/hyperparameter-tuning.html}
}

@inproceedings{radfordLanguageModelsAre2019,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, D. and Amodei, Dario and Sutskever, I.},
  year = {2019},
  url = {https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe},
  urldate = {2024-01-27},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  note = {[TLDR] It is demonstrated that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText, suggesting a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  file = {/home/carlos/Zotero/storage/IDI7KPUK/Radford et al. - 2019 - Language Models are Unsupervised Multitask Learners.pdf}
}

@misc{radovanovicEmergingArchitecturesLLM2023,
  title = {Emerging {{Architectures}} for {{LLM Applications}}},
  author = {Radovanovic, Rajko, Matt Bornstein},
  year = {2023},
  month = jun,
  journal = {Andreessen Horowitz},
  url = {https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/},
  urldate = {2023-06-27},
  abstract = {A reference architecture for the LLM app stack. It shows the most common systems, tools, and design patterns used by AI startups and tech companies.},
  language = {en-US},
  file = {/home/carlos/Zotero/storage/PAWYWIEW/emerging-architectures-for-llm-applications.html}
}

@misc{ramsauerHopfieldNetworksAll2021,
  title = {Hopfield {{Networks}} Is {{All You Need}}},
  author = {Ramsauer, Hubert and Sch{\"a}fl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlovi{\'c}, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, G{\"u}nter and Brandstetter, Johannes and Hochreiter, Sepp},
  year = {2021},
  month = apr,
  number = {arXiv:2008.02217},
  eprint = {2008.02217},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2008.02217},
  url = {http://arxiv.org/abs/2008.02217},
  urldate = {2024-01-22},
  abstract = {We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: https://github.com/ml-jku/hopfield-layers},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  note = {Comment: 10 pages (+ appendix); 12 figures; Blog: https://ml-jku.github.io/hopfield-layers/; GitHub: https://github.com/ml-jku/hopfield-layers},
  file = {/home/carlos/Zotero/storage/36MASUK3/Ramsauer et al. - 2021 - Hopfield Networks is All You Need.pdf;/home/carlos/Zotero/storage/BGMBA4V5/2008.html}
}

@misc{RegressionVsClassification2021,
  title = {Regression vs {{Classification}}, {{Explained}} - {{Sharp Sight}}},
  shorttitle = {Regression vs {{Classification}}},
  year = {2021},
  month = apr,
  url = {https://www.sharpsightlabs.com/blog/regression-vs-classification/},
  urldate = {2024-01-26},
  abstract = {This article explains the difference between regression vs classification in machine learning. For ML tutorials, sign up for our email list.},
  chapter = {data science},
  language = {en-US},
  file = {/home/carlos/Zotero/storage/286HSH4E/regression-vs-classification.html}
}

@misc{RegressionVsClassification2021a,
  title = {Regression vs. {{Classification}} in {{Machine Learning}}: {{What}}'s the {{Difference}}?},
  shorttitle = {Regression vs. {{Classification}} in {{Machine Learning}}},
  year = {2021},
  month = oct,
  journal = {Springboard Blog},
  url = {https://www.springboard.com/blog/data-science/regression-vs-classification/},
  urldate = {2024-01-26},
  abstract = {Comparing regression vs classification in machine learning can sometimes confuse even the most seasoned data scientists. This can eventually make it difficult},
  chapter = {Data Science},
  language = {en},
  file = {/home/carlos/Zotero/storage/7CPDH77Z/regression-vs-classification.html}
}

@misc{RevistaBitsCiencia,
  title = {{Revista Bits de Ciencia N{$^\circ$} 21 - 2021}},
  url = {https://revistasdex.uchile.cl/files/full/BitsdeCiencia_(21)_2021/index.html},
  urldate = {2023-10-30},
  chapter = {Article Section},
  language = {es},
  file = {/home/carlos/Zotero/storage/6QXLYRRD/index.html}
}

@book{roadsComputerMusicTutorial1996,
  title = {The {{Computer Music Tutorial}}},
  author = {Roads, Curtis and Strawn, John and Abbott, Curtis and Gordon, John and Philip, Greenspun},
  year = {1996},
  publisher = {{The MIT Press}},
  address = {{Cambride}}
}

@article{rosenblattPerceptronProbabilisticModel1958,
  title = {The Perceptron: {{A}} Probabilistic Model for Information Storage and Organization in the Brain.},
  shorttitle = {The Perceptron},
  author = {Rosenblatt, F.},
  year = {1958},
  journal = {Psychological Review},
  volume = {65},
  number = {6},
  pages = {386--408},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/h0042519},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0042519},
  urldate = {2023-12-21},
  abstract = {The first of these questions is in the province of sensory physiology, and is the only one for which appreciable understanding has been achieved. This article will be concerned primarily with the second and third questions, which are still subject to a vast amount of speculation, and where the few relevant facts currently supplied by neurophysiology have not yet been integrated into an acceptable theory. With regard to the second question, two alternative positions have been maintained. The first suggests that storage of sensory information is in the form of coded representations or images, with some sort of one-to-one mapping between the sensory stimulus},
  language = {en},
  note = {[TLDR] This article will be concerned primarily with the second and third questions, which are still subject to a vast amount of speculation, and where the few relevant facts currently supplied by neurophysiology have not yet been integrated into an acceptable theory.},
  file = {/home/carlos/Zotero/storage/9Z6ZPSWA/Rosenblatt - 1958 - The perceptron A probabilistic model for informat.pdf}
}

@book{rothmanTransformersNaturalLanguage2021,
  title = {Transformers for {{Natural Language Processing}}: {{Build}} Innovative Deep Neural Network Architectures for {{NLP}} with {{Python}}, {{PyTorch}}, {{TensorFlow}}, {{BERT}}, {{RoBERTa}}, and More},
  shorttitle = {Transformers for {{Natural Language Processing}}},
  author = {Rothman, Denis},
  year = {2021},
  publisher = {{Packt Publishing}},
  address = {{Birmingham}},
  isbn = {978-1-80056-579-1},
  language = {eng}
}

@article{roziereCodeLlamaOpena,
  title = {Code {{Llama}}: {{Open Foundation Models}} for {{Code}}},
  author = {Rozi{\`e}re, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Ellen, Xiaoqing and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and Kozhevnikov, Artyom and Evtimov, Ivan and Bitton, Joanna and Bhatt, Manish and Ferrer, Cristian Canton and Grattafiori, Aaron and Xiong, Wenhan and D{\'e}fossez, Alexandre and Copet, Jade and Azhar, Faisal and Touvron, Hugo and Martin, Louis and Usunier, Nicolas and Scialom, Thomas and Synnaeve, Gabriel},
  language = {en},
  file = {/home/carlos/Zotero/storage/FEFJVJDY/Rozière et al. - Code Llama Open Foundation Models for Code.pdf}
}

@misc{RunTextGeneration2022,
  title = {Run Text Generation with {{Bloom}} and {{GPT}} Models on {{Amazon SageMaker JumpStart}} {\textbar} {{AWS Machine Learning Blog}}},
  year = {2022},
  month = nov,
  url = {https://aws.amazon.com/blogs/machine-learning/run-text-generation-with-gpt-and-bloom-models-on-amazon-sagemaker-jumpstart/},
  urldate = {2023-10-31},
  chapter = {Amazon SageMaker},
  language = {en-US},
  file = {/home/carlos/Zotero/storage/FTBUAU2Q/run-text-generation-with-gpt-and-bloom-models-on-amazon-sagemaker-jumpstart.html}
}

@book{RussellStuartJ2021AI:A,
  title = {Artificial Intelligence : {{A}} Modern Approach},
  author = {Russell, Stuart J},
  year = {2021},
  edition = {4th Edition.; Global edition.},
  publisher = {{Pearson}},
  address = {{Harlow}},
  isbn = {978-0-13-461099-3},
  language = {eng},
  keywords = {Inteligencia artificial,Inteligencia artificial Robotica,Transferencia del conocimiento}
}

@misc{ruviaroGentleIntroductionSuperCollider2015,
  title = {A {{Gentle Introduction}} to {{SuperCollider}}},
  author = {Ruviaro, Bruno},
  year = {2015},
  url = {https://github.com/brunoruviaro/A_Gentle_Introduction_To_SuperCollider/blob/master/A_Gentle_Introduction_To_SuperCollider_BOOK_9x7_Landscape.tex},
  urldate = {2024-01-30},
  abstract = {A step-by-step tutorial for total beginners.},
  language = {en},
  file = {/home/carlos/Zotero/storage/CN3KBP2D/A_Gentle_Introduction_To_SuperCollider_BOOK_9x7_Landscape.html}
}

@misc{schaefferAreEmergentAbilities2023,
  title = {Are {{Emergent Abilities}} of {{Large Language Models}} a {{Mirage}}?},
  author = {Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  year = {2023},
  month = may,
  number = {arXiv:2304.15004},
  eprint = {2304.15004},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2304.15004},
  urldate = {2023-10-23},
  abstract = {Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, emergent abilities appear due the researcher's choice of metric rather than due to fundamental changes in model behavior with scale. Specifically, nonlinear or discontinuous metrics produce apparent emergent abilities, whereas linear or continuous metrics produce smooth, continuous, predictable changes in model performance. We present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities, (2) make, test and confirm two predictions about metric choices in a metaanalysis of emergent abilities on BIG-Bench; and (3) show how to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks. Via all three analyses, we provide evidence that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models.},
  archiveprefix = {arxiv},
  language = {en},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/carlos/Zotero/storage/IKAF8WT5/Schaeffer et al. - 2023 - Are Emergent Abilities of Large Language Models a .pdf}
}

@misc{schmidtRecurrentNeuralNetworks2019,
  title = {Recurrent {{Neural Networks}} ({{RNNs}}): {{A}} Gentle {{Introduction}} and {{Overview}}},
  shorttitle = {Recurrent {{Neural Networks}} ({{RNNs}})},
  author = {Schmidt, Robin M.},
  year = {2019},
  month = nov,
  number = {arXiv:1912.05911},
  eprint = {1912.05911},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1912.05911},
  url = {http://arxiv.org/abs/1912.05911},
  urldate = {2024-01-28},
  abstract = {State-of-the-art solutions in the areas of "Language Modelling \& Generating Text", "Speech Recognition", "Generating Image Descriptions" or "Video Tagging" have been using Recurrent Neural Networks as the foundation for their approaches. Understanding the underlying concepts is therefore of tremendous importance if we want to keep up with recent or upcoming publications in those areas. In this work we give a short overview over some of the most important concepts in the realm of Recurrent Neural Networks which enables readers to easily understand the fundamentals such as but not limited to "Backpropagation through Time" or "Long Short-Term Memory Units" as well as some of the more recent advances like the "Attention Mechanism" or "Pointer Networks". We also give recommendations for further reading regarding more complex topics where it is necessary.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/carlos/Zotero/storage/A4V23CJF/Schmidt - 2019 - Recurrent Neural Networks (RNNs) A gentle Introduction and Overview.pdf;/home/carlos/Zotero/storage/L4ULHTME/1912.html}
}

@book{searleMentesCerebrosCiencia1985,
  title = {{Mentes, cerebros y ciencia}},
  author = {Searle, John R.},
  year = {1985},
  month = dec,
  publisher = {{C{\'a}tedra}},
  abstract = {Mientras que el sentido comun nos presenta como seres racionales, conscientes y libres, la ciencia nos informa que el universo en el que operamos, y que constituimos, es un agregado de particulas inconscienles. Que sabemos del cerebro, punto de union entre nuestra mente y el mundo fisico? Muy poco. En los ultimos tiempos se abre paso una atrevida analogia que relaciona el cerebro con un ordenador digital. Algunos cientificos postulan que las maquinas pueden --o podran -- pensar. Resolvera la inteligencia Artificial los problemas planteados por la relacion mente/cerebro?},
  googlebooks = {jfFUVl03PYUC},
  isbn = {978-84-376-0569-2},
  language = {es}
}

@misc{selAlgorithmThoughtsEnhancing2023,
  title = {Algorithm of {{Thoughts}}: {{Enhancing Exploration}} of {{Ideas}} in {{Large Language Models}}},
  shorttitle = {Algorithm of {{Thoughts}}},
  author = {Sel, Bilgehan and {Al-Tawaha}, Ahmad and Khattar, Vanshaj and Wang, Lu and Jia, Ruoxi and Jin, Ming},
  year = {2023},
  month = aug,
  number = {arXiv:2308.10379},
  eprint = {2308.10379},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.10379},
  url = {http://arxiv.org/abs/2308.10379},
  urldate = {2023-09-27},
  abstract = {Current literature, aiming to surpass the "Chain-of-Thought" approach, often resorts to an external modus operandi involving halting, modifying, and then resuming the generation process to boost Large Language Models' (LLMs) reasoning capacities. This mode escalates the number of query requests, leading to increased costs, memory, and computational overheads. Addressing this, we propose the Algorithm of Thoughts -- a novel strategy that propels LLMs through algorithmic reasoning pathways, pioneering a new mode of in-context learning. By employing algorithmic examples, we exploit the innate recurrence dynamics of LLMs, expanding their idea exploration with merely one or a few queries. Our technique outperforms earlier single-query methods and stands on par with a recent multi-query strategy that employs an extensive tree search algorithm. Intriguingly, our results suggest that instructing an LLM using an algorithm can lead to performance surpassing that of the algorithm itself, hinting at LLM's inherent ability to weave its intuition into optimized searches. We probe into the underpinnings of our method's efficacy and its nuances in application.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/RUC6VMXG/Sel et al. - 2023 - Algorithm of Thoughts Enhancing Exploration of Id.pdf;/home/carlos/Zotero/storage/DMUQY952/2308.html}
}

@misc{seydeBangBangControlAll2021,
  title = {Is {{Bang-Bang Control All You Need}}? {{Solving Continuous Control}} with {{Bernoulli Policies}}},
  shorttitle = {Is {{Bang-Bang Control All You Need}}?},
  author = {Seyde, Tim and Gilitschenski, Igor and Schwarting, Wilko and Stellato, Bartolomeo and Riedmiller, Martin and Wulfmeier, Markus and Rus, Daniela},
  year = {2021},
  month = nov,
  number = {arXiv:2111.02552},
  eprint = {2111.02552},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2111.02552},
  url = {http://arxiv.org/abs/2111.02552},
  urldate = {2024-01-22},
  abstract = {Reinforcement learning (RL) for continuous control typically employs distributions whose support covers the entire action space. In this work, we investigate the colloquially known phenomenon that trained agents often prefer actions at the boundaries of that space. We draw theoretical connections to the emergence of bang-bang behavior in optimal control, and provide extensive empirical evaluation across a variety of recent RL algorithms. We replace the normal Gaussian by a Bernoulli distribution that solely considers the extremes along each action dimension - a bang-bang controller. Surprisingly, this achieves state-of-the-art performance on several continuous control benchmarks - in contrast to robotic hardware, where energy and maintenance cost affect controller choices. Since exploration, learning,and the final solution are entangled in RL, we provide additional imitation learning experiments to reduce the impact of exploration on our analysis. Finally, we show that our observations generalize to environments that aim to model real-world challenges and evaluate factors to mitigate the emergence of bang-bang solutions. Our findings emphasize challenges for benchmarking continuous control algorithms, particularly in light of potential real-world applications.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/carlos/Zotero/storage/CKZ7I9RM/Seyde et al. - 2021 - Is Bang-Bang Control All You Need Solving Continuous Control with Bernoulli Policies.pdf;/home/carlos/Zotero/storage/UQUDFT7H/2111.html}
}

@article{shannon1951prediction,
  title = {Prediction and Entropy of Printed English},
  author = {Shannon, Claude Elwood},
  year = {1951},
  month = jan,
  journal = {Bell System Technical Journal},
  volume = {30},
  pages = {50--64},
  url = {http://languagelog.ldc.upenn.edu/myl/Shannon1950.pdf},
  added-at = {2013-02-27T08:26:04.000+0100},
  interhash = {daabc21c7f6e71f6e78a10c8d3492927},
  intrahash = {2e79cf0f6022645a632b13e081b0b035},
  keywords = {communication english entropy information prediction shannon toread},
  timestamp = {2014-07-28T15:57:31.000+0200}
}

@misc{shevlaneModelEvaluationExtreme2023,
  title = {Model Evaluation for Extreme Risks},
  author = {Shevlane, Toby and Farquhar, Sebastian and Garfinkel, Ben and Phuong, Mary and Whittlestone, Jess and Leung, Jade and Kokotajlo, Daniel and Marchal, Nahema and Anderljung, Markus and Kolt, Noam and Ho, Lewis and Siddarth, Divya and Avin, Shahar and Hawkins, Will and Kim, Been and Gabriel, Iason and Bolina, Vijay and Clark, Jack and Bengio, Yoshua and Christiano, Paul and Dafoe, Allan},
  year = {2023},
  month = may,
  number = {arXiv:2305.15324},
  eprint = {2305.15324},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2305.15324},
  urldate = {2023-06-27},
  abstract = {Current approaches to building general-purpose AI systems tend to produce systems with both beneficial and harmful capabilities. Further progress in AI development could lead to capabilities that pose extreme risks, such as offensive cyber capabilities or strong manipulation skills. We explain why model evaluation is critical for addressing extreme risks. Developers must be able to identify dangerous capabilities (through "dangerous capability evaluations") and the propensity of models to apply their capabilities for harm (through "alignment evaluations"). These evaluations will become critical for keeping policymakers and other stakeholders informed, and for making responsible decisions about model training, deployment, and security.},
  archiveprefix = {arxiv},
  language = {en},
  keywords = {Computer Science - Artificial Intelligence,K.4.1},
  file = {/home/carlos/Zotero/storage/BDHEV472/Shevlane et al. - 2023 - Model evaluation for extreme risks.pdf}
}

@misc{SonicPiLive,
  title = {Sonic {{Pi}} - {{The Live Coding Music Synth}} for {{Everyone}}},
  url = {https://sonic-pi.net/},
  urldate = {2024-01-28},
  file = {/home/carlos/Zotero/storage/RBEGXTK5/sonic-pi.net.html}
}

@misc{StableAudioFast,
  title = {Stable {{Audio}}: {{Fast Timing-Conditioned Latent Audio Diffusion}}},
  shorttitle = {Stable {{Audio}}},
  journal = {Stability AI},
  url = {https://stability.ai/research/stable-audio-efficient-timing-latent-diffusion},
  urldate = {2023-09-27},
  abstract = {Stable Audio represents the cutting-edge audio generation research by Stability AI's generative audio research lab, Harmonai. We continue to improve our model architectures, datasets, and training procedures to improve output quality, controllability, inference speed, and output length.},
  language = {en-GB},
  file = {/home/carlos/Zotero/storage/HE8PHSK2/stable-audio-efficient-timing-latent-diffusion.html}
}

@misc{stokesGuideLanguageModel2023,
  title = {A Guide to Language Model Sampling in {{AllenNLP}}},
  author = {Stokes, Jackson},
  year = {2023},
  month = sep,
  journal = {Medium},
  url = {https://blog.allenai.org/a-guide-to-language-model-sampling-in-allennlp-3b1239274bc3},
  urldate = {2024-01-30},
  abstract = {How randomness can be used to make language models more creative},
  language = {en},
  file = {/home/carlos/Zotero/storage/BGKPIPNS/a-guide-to-language-model-sampling-in-allennlp-3b1239274bc3.html}
}

@misc{StrudelREPL,
  title = {Strudel {{REPL}}},
  url = {https://strudel.cc/},
  urldate = {2023-12-09},
  file = {/home/carlos/Zotero/storage/6HPXIMRG/strudel.cc.html}
}

@misc{SunoAI,
  title = {Suno {{AI}}},
  url = {https://www.suno.ai/},
  urldate = {2023-12-26},
  abstract = {We are building a future where anyone can make great music. No instrument needed, just imagination. From your mind to music.},
  language = {en},
  file = {/home/carlos/Zotero/storage/6922LJJL/www.suno.ai.html}
}

@misc{SuperCollider12Help,
  title = {{{SuperCollider}} 3.12.2 {{Help}}},
  url = {https://doc.sccode.org/Help.html},
  urldate = {2024-01-30},
  file = {/home/carlos/Zotero/storage/98KDEH49/Help.html}
}

@misc{SupercolliderSupercollider2024,
  title = {Supercollider/Supercollider},
  year = {2024},
  month = jan,
  url = {https://github.com/supercollider/supercollider},
  urldate = {2024-01-28},
  abstract = {An audio server, programming language, and IDE for sound synthesis and algorithmic composition.},
  copyright = {GPL-3.0},
  howpublished = {SuperCollider},
  keywords = {algorithmic-composition,audio,c-plus-plus,computer-music,electronic-music,livecoding,music,programming-language,sclang,scsynth,sonification,sound,supercollider,synthesis}
}

@book{supperMusicaElectronicaMusica2012,
  title = {M{\'u}sica Electr{\'o}nica y M{\'u}sica Con Ordenador. {{Historia}}, Est{\'e}tica, M{\'e}todos, Sistemas},
  author = {Supper, Martin},
  year = {2012},
  publisher = {{Alianza M{\'u}sica}},
  address = {{Madrid}}
}

@misc{SyntheticDataGeneration,
  title = {Synthetic {{Data Generation}} with {{Large Language Models}} for {{Text Classification}}: {{Potential}} and {{Limitations}}},
  shorttitle = {Papers with {{Code}} - {{Synthetic Data Generation}} with {{Large Language Models}} for {{Text Classification}}},
  url = {https://paperswithcode.com/paper/synthetic-data-generation-with-large-language},
  urldate = {2024-01-28},
  abstract = {No code available yet.},
  language = {en},
  file = {/home/carlos/Zotero/storage/BEFLSYPQ/synthetic-data-generation-with-large-language.html}
}

@misc{teamChucKStronglyTimedMusic,
  title = {{{ChucK}}: {{A Strongly-Timed Music Programming Language}}},
  shorttitle = {{{ChucK}}},
  author = {Team, ChucK},
  url = {https://ccrma.stanford.edu/software/chuck/},
  urldate = {2023-12-09},
  abstract = {ChucK is a strongly-timed programming language for interactive sound synthesis and music creation.},
  language = {en},
  file = {/home/carlos/Zotero/storage/Q7Y4LBTF/chuck.stanford.edu.html}
}

@misc{TestTuring,
  title = {El {{Test}} de {{Turing}}},
  url = {https://edea.juntadeandalucia.es/bancorecursos/file/d1bb5f09-682f-4e93-a799-e90b1c3364ed/2/COM_3ESO_REA_01_V02.zip/411_el_test_de_turing.html},
  urldate = {2024-01-27},
  file = {/home/carlos/Zotero/storage/D9EYC4EE/411_el_test_de_turing.html}
}

@misc{TextGenerationStrategies,
  title = {Text Generation Strategies},
  url = {https://huggingface.co/docs/transformers/main/en/generation_strategies},
  urldate = {2023-10-31},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {/home/carlos/Zotero/storage/4ENA6UIN/generation_strategies.html}
}

@misc{tianFinetuningLanguageModels2023,
  title = {Fine-Tuning {{Language Models}} for {{Factuality}}},
  author = {Tian, Katherine and Mitchell, Eric and Yao, Huaxiu and Manning, Christopher D. and Finn, Chelsea},
  year = {2023},
  month = nov,
  number = {arXiv:2311.08401},
  eprint = {2311.08401},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2311.08401},
  url = {http://arxiv.org/abs/2311.08401},
  urldate = {2024-01-25},
  abstract = {The fluency and creativity of large pre-trained language models (LLMs) have led to their widespread use, sometimes even as a replacement for traditional search engines. Yet language models are prone to making convincing but factually inaccurate claims, often referred to as 'hallucinations.' These errors can inadvertently spread misinformation or harmfully perpetuate misconceptions. Further, manual fact-checking of model responses is a time-consuming process, making human factuality labels expensive to acquire. In this work, we fine-tune language models to be more factual, without human labeling and targeting more open-ended generation settings than past work. We leverage two key recent innovations in NLP to do so. First, several recent works have proposed methods for judging the factuality of open-ended text by measuring consistency with an external knowledge base or simply a large model's confidence scores. Second, the direct preference optimization algorithm enables straightforward fine-tuning of language models on objectives other than supervised imitation, using a preference ranking over possible model responses. We show that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or our novel retrieval-free approach, significantly improves the factuality (percent of generated claims that are correct) of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality. At 7B scale, compared to Llama-2-chat, we observe 58\% and 40\% reduction in factual error rate when generating biographies and answering medical questions, respectively.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/carlos/Zotero/storage/G7PVTXL8/Tian et al. - 2023 - Fine-tuning Language Models for Factuality.pdf;/home/carlos/Zotero/storage/CJWHU2CJ/2311.html}
}

@book{torresivinalsPythonDeepLearning2020,
  title = {Python {{Deep Learning}}. {{Introducci{\'o}n}} Pr{\'a}ctica Con {{Keras}} y {{TensorFlow}} 2},
  author = {{Torres i Vi{\~n}als}, Jordi},
  year = {2020},
  edition = {1ª},
  publisher = {{Marcombo}},
  isbn = {978-84-267-2921-7}
}

@misc{touvronLLaMAOpenEfficient2023,
  title = {{{LLaMA}}: {{Open}} and {{Efficient Foundation Language Models}}},
  shorttitle = {{{LLaMA}}},
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  year = {2023},
  month = feb,
  number = {arXiv:2302.13971},
  eprint = {2302.13971},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.13971},
  url = {http://arxiv.org/abs/2302.13971},
  urldate = {2024-01-28},
  abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/TSWEP6YH/Touvron et al. - 2023 - LLaMA Open and Efficient Foundation Language Models.pdf;/home/carlos/Zotero/storage/4249Y8EG/2302.html}
}

@misc{UnderstandingLearningDemonstrations,
  title = {An {{Understanding}} of {{Learning}} from {{Demonstrations}} for {{Neural Text Generation}} {$\cdot$} {{The ICLR Blog Track}}},
  url = {https://iclr-blog-track.github.io/2022/03/25/text-gen-via-lfd/},
  urldate = {2024-01-26},
  file = {/home/carlos/Zotero/storage/ZP65IJW5/text-gen-via-lfd.html}
}

@misc{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2023-05-23},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: 15 pages, 5 figures},
  file = {/home/carlos/Zotero/storage/ZUQMSYXF/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/home/carlos/Zotero/storage/E7YRG247/1706.html}
}

@article{wang2021promising,
  title = {A Promising and Challenging Approach: {{Radiologists}}' Perspective on Deep Learning and Artificial Intelligence for Fighting {{COVID-19}}},
  author = {Wang, Tianming and Chen, Zhu and Shang, Quanliang and Ma, Cong and Chen, Xiangyu and Xiao, Enhua},
  year = {2021},
  journal = {Diagnostics},
  volume = {11},
  number = {10},
  pages = {1924},
  publisher = {{Multidisciplinary Digital Publishing Institute}}
}

@misc{wangAllYouNeed2019,
  title = {All {{You Need Is Boundary}}: {{Toward Arbitrary-Shaped Text Spotting}}},
  shorttitle = {All {{You Need Is Boundary}}},
  author = {Wang, Hao and Lu, Pu and Zhang, Hui and Yang, Mingkun and Bai, Xiang and Xu, Yongchao and He, Mengchao and Wang, Yongpan and Liu, Wenyu},
  year = {2019},
  month = nov,
  number = {arXiv:1911.09550},
  eprint = {1911.09550},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1911.09550},
  url = {http://arxiv.org/abs/1911.09550},
  urldate = {2024-01-22},
  abstract = {Recently, end-to-end text spotting that aims to detect and recognize text from cluttered images simultaneously has received particularly growing interest in computer vision. Different from the existing approaches that formulate text detection as bounding box extraction or instance segmentation, we localize a set of points on the boundary of each text instance. With the representation of such boundary points, we establish a simple yet effective scheme for end-to-end text spotting, which can read the text of arbitrary shapes. Experiments on three challenging datasets, including ICDAR2015, TotalText and COCO-Text demonstrate that the proposed method consistently surpasses the state-of-the-art in both scene text detection and end-to-end text recognition tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Accepted to AAAI2020},
  file = {/home/carlos/Zotero/storage/7KT9R5CE/Wang et al. - 2019 - All You Need Is Boundary Toward Arbitrary-Shaped Text Spotting.pdf;/home/carlos/Zotero/storage/5SXPIZ7P/1911.html}
}

@misc{wangCostEffectiveHyperparameterOptimization2023,
  title = {Cost-{{Effective Hyperparameter Optimization}} for {{Large Language Model Generation Inference}}},
  author = {Wang, Chi and Liu, Susan Xueqing and Awadallah, Ahmed H.},
  year = {2023},
  month = aug,
  number = {arXiv:2303.04673},
  eprint = {2303.04673},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2303.04673},
  urldate = {2023-11-07},
  abstract = {Large Language Models (LLMs) have sparked significant interest in their generative capabilities, leading to the development of various commercial applications. The high cost of using the models drives application builders to maximize the value of generation under a limited inference budget. This paper presents a study of optimizing inference hyperparameters such as the number of responses, temperature and max tokens, which significantly affects the utility/cost of text generation. We design a framework named EcoOptiGen which leverages economical hyperparameter optimization and cost-based pruning. Experiments with the GPT-3.5/GPT-4 models on a variety of tasks verify its effectiveness. EcoOptiGen is implemented in the `autogen' package of the FLAML library: https: //aka.ms/autogen.},
  archiveprefix = {arxiv},
  language = {en},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/carlos/Zotero/storage/25YJQ69L/Wang et al. - 2023 - Cost-Effective Hyperparameter Optimization for Lar.pdf}
}

@article{wangHyperparameterOptimizationAlgorithm2022,
  title = {A {{Hyperparameter Optimization Algorithm}} for the {{LSTM Temperature Prediction Model}} in {{Data Center}}},
  author = {Wang, Simin and Ma, Chunmiao and Xu, Yixuan and Wang, Jinyu and Wu, Weiguo},
  year = {2022},
  month = dec,
  journal = {Scientific Programming},
  volume = {2022},
  pages = {e6519909},
  publisher = {{Hindawi}},
  issn = {1058-9244},
  doi = {10.1155/2022/6519909},
  url = {https://www.hindawi.com/journals/sp/2022/6519909/},
  urldate = {2023-11-07},
  abstract = {As the main tool to realize data mining and efficient knowledge acquisition in the era of big data, machine learning is widely used in data center energy-saving research. The temperature prediction model based on machine learning predicts the state of the data center according to the upcoming tasks. It can adjust the refrigeration equipment in advance to avoid temperature regulation lag and set the air conditioning temperature according to the actual demand to avoid excessive refrigeration. Task scheduling and migration algorithm based on temperature prediction can effectively avoid hot spots. However, the choice of hyperparameter of machine learning model has a great impact on its performance. In this study, a hyperparameter optimization algorithm based on MLP is proposed. On the basis of trying certain hyperparameters, the MLP model is used to predict the value of all hyperparameters' space, and then, a certain number of high-quality hyperparameters are selected to train the model repeatedly. In each iteration, the amount of training data decreases gradually, while the accuracy of the model improves rapidly, and finally, the appropriate hyperparameter are obtained. We use the idea of mutation in the genetic algorithm to improve the probability of high-quality solutions and the loss function weighting method to select the solution with the best stability. Experiments are carried out on two representative machine learning models, LSTM and Random Forest, and compared with the standard Gaussian Bayes and Random Search method. The results show that the method proposed in this study can obtain high-precision and high-stability hyperparameter through one run and can greatly improve the operation efficiency. This algorithm is not only effective for LSTM but also suitable for other machine learning models.},
  language = {en},
  file = {/home/carlos/Zotero/storage/CSFU4NGN/Wang et al. - 2022 - A Hyperparameter Optimization Algorithm for the LS.pdf}
}

@misc{wangSelfConsistencyImprovesChain2023,
  title = {Self-{{Consistency Improves Chain}} of {{Thought Reasoning}} in {{Language Models}}},
  author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  year = {2023},
  month = mar,
  number = {arXiv:2203.11171},
  eprint = {2203.11171},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.11171},
  url = {http://arxiv.org/abs/2203.11171},
  urldate = {2024-01-20},
  abstract = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  note = {Comment: Published at ICLR 2023. V2: added PaLM results; V3: added UL2 results; V4: camera ready version at ICLR 2023},
  file = {/home/carlos/Zotero/storage/937N86FR/Wang et al. - 2023 - Self-Consistency Improves Chain of Thought Reasoning in Language Models.pdf;/home/carlos/Zotero/storage/DV8TJ9QK/2203.html}
}

@misc{weiChainofThoughtPromptingElicits2023,
  title = {Chain-of-{{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  year = {2023},
  month = jan,
  number = {arXiv:2201.11903},
  eprint = {2201.11903},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2201.11903},
  urldate = {2023-10-23},
  abstract = {We explore how generating a chain of thought{\textemdash}a series of intermediate reasoning steps{\textemdash}significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-ofthought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  archiveprefix = {arxiv},
  language = {en},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/carlos/Zotero/storage/AZPLR9LR/Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf}
}

@misc{weiEmergentAbilitiesLarge2022,
  title = {Emergent {{Abilities}} of {{Large Language Models}}},
  author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  year = {2022},
  month = oct,
  number = {arXiv:2206.07682},
  eprint = {2206.07682},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.07682},
  url = {http://arxiv.org/abs/2206.07682},
  urldate = {2023-06-27},
  abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: Transactions on Machine Learning Research (TMLR), 2022
\par
Comment: Transactions on Machine Learning Research (TMLR), 2022},
  file = {/home/carlos/Zotero/storage/C3F58HUQ/Wei et al. - 2022 - Emergent Abilities of Large Language Models.pdf;/home/carlos/Zotero/storage/SK77QAMT/2206.html}
}

@misc{WhatRetrievalAugmented,
  title = {{What is Retrieval Augmented Generation?}},
  url = {https://www.linkedin.com/pulse/what-retrieval-augmented-generation-grow-right},
  urldate = {2024-01-05},
  abstract = {Retrieval Augmented Generation (RAG) is an approach that combines the power of large-scale neural language models with external retrieval or search mechanisms. Here's a breakdown of RAG in a semi-formal, business tone: Core Concept: At the heart of RAG is the idea of leveraging external knowledge so},
  language = {es},
  file = {/home/carlos/Zotero/storage/A9NJFVUJ/what-retrieval-augmented-generation-grow-right.html}
}

@misc{WhatRetrievalaugmentedGeneration2021,
  title = {What Is Retrieval-Augmented Generation?},
  year = {2021},
  month = feb,
  journal = {IBM Research Blog},
  url = {https://research.ibm.com/blog/retrieval-augmented-generation-RAG},
  urldate = {2023-11-07},
  abstract = {RAG is an AI framework for retrieving facts to ground LLMs on the most accurate information and to give users insight into AI's decisionmaking process.},
  copyright = {{\copyright} Copyright IBM Corp. 2021},
  language = {en-US},
  file = {/home/carlos/Zotero/storage/S4AMA7JD/retrieval-augmented-generation-RAG.html}
}

@book{wilsonSuperColliderBook2011a,
  title = {The {{SuperCollider Book}}},
  author = {Wilson, Scott and Cottle, David and Collins, Nick},
  year = {2011},
  month = mar,
  publisher = {{The MIT Press}},
  abstract = {SuperCollider is one of the most important domain-specific audio programming languages, with potential applications that include real-time interaction, installations, electroacoustic pieces, generative music, and audiovisuals. The SuperCollider Book is the essential reference to this powerful and flexible language, offering students and professionals a collection of tutorials, essays, and projects. With contributions from top academics, artists, and technologists that cover topics at levels from the introductory to the specialized, it will be a valuable sourcebook both for beginners and for advanced users. SuperCollider, first developed by James McCartney, is an accessible blend of Smalltalk, C, and further ideas from a number of programming languages. Free, open-source, cross-platform, and with a diverse and supportive developer community, it is often the first programming language sound artists and computer musicians learn. The SuperCollider Book is the long-awaited guide to the design, syntax, and use of the SuperCollider language. The first chapters offer an introduction to the basics, including a friendly tutorial for absolute beginners, providing the reader with skills that can serve as a foundation for further learning. Later chapters cover more advanced topics and particular topics in computer music, including programming, sonification, spatialization, microsound, GUIs, machine listening, alternative tunings, and non-real-time synthesis; practical applications and philosophical insigh"s from the composer's and artist's perspectives; and "under the hood," developer's-eye views of SuperCollider's inner workings. A Web site accompanying the book offers code, links to the application itself and its source code, and a variety of third-party extras, extensions, libraries, and examples.},
  isbn = {978-0-262-23269-2}
}

@misc{xieAllYouNeed2017,
  title = {All {{You Need}} Is {{Beyond}} a {{Good Init}}: {{Exploring Better Solution}} for {{Training Extremely Deep Convolutional Neural Networks}} with {{Orthonormality}} and {{Modulation}}},
  shorttitle = {All {{You Need}} Is {{Beyond}} a {{Good Init}}},
  author = {Xie, Di and Xiong, Jiang and Pu, Shiliang},
  year = {2017},
  month = apr,
  number = {arXiv:1703.01827},
  eprint = {1703.01827},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1703.01827},
  url = {http://arxiv.org/abs/1703.01827},
  urldate = {2024-01-22},
  abstract = {Deep neural network is difficult to train and this predicament becomes worse as the depth increases. The essence of this problem exists in the magnitude of backpropagated errors that will result in gradient vanishing or exploding phenomenon. We show that a variant of regularizer which utilizes orthonormality among different filter banks can alleviate this problem. Moreover, we design a backward error modulation mechanism based on the quasi-isometry assumption between two consecutive parametric layers. Equipped with these two ingredients, we propose several novel optimization solutions that can be utilized for training a specific-structured (repetitively triple modules of Conv-BNReLU) extremely deep convolutional neural network (CNN) WITHOUT any shortcuts/ identity mappings from scratch. Experiments show that our proposed solutions can achieve distinct improvements for a 44-layer and a 110-layer plain networks on both the CIFAR-10 and ImageNet datasets. Moreover, we can successfully train plain CNNs to match the performance of the residual counterparts. Besides, we propose new principles for designing network structure from the insights evoked by orthonormality. Combined with residual structure, we achieve comparative performance on the ImageNet dataset.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  note = {Comment: Updating experiments; CVPR2017},
  file = {/home/carlos/Zotero/storage/AXI3SGH8/Xie et al. - 2017 - All You Need is Beyond a Good Init Exploring Better Solution for Training Extremely Deep Convolutio.pdf;/home/carlos/Zotero/storage/CZIYPAEK/1703.html}
}

@misc{yangLargeLanguageModels2023,
  title = {Large {{Language Models}} as {{Optimizers}}},
  author = {Yang, Chengrun and Wang, Xuezhi and Lu, Yifeng and Liu, Hanxiao and Le, Quoc V. and Zhou, Denny and Chen, Xinyun},
  year = {2023},
  month = sep,
  number = {arXiv:2309.03409},
  eprint = {2309.03409},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.03409},
  url = {http://arxiv.org/abs/2309.03409},
  urldate = {2023-10-30},
  abstract = {Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8\% on GSM8K, and by up to 50\% on Big-Bench Hard tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/carlos/Zotero/storage/SLLUN5RF/Yang et al. - 2023 - Large Language Models as Optimizers.pdf;/home/carlos/Zotero/storage/HAGLRUNJ/2309.html}
}

@misc{zengMusicBERTSymbolicMusic2021,
  title = {{{MusicBERT}}: {{Symbolic Music Understanding}} with {{Large-Scale Pre-Training}}},
  shorttitle = {{{MusicBERT}}},
  author = {Zeng, Mingliang and Tan, Xu and Wang, Rui and Ju, Zeqian and Qin, Tao and Liu, Tie-Yan},
  year = {2021},
  month = jun,
  number = {arXiv:2106.05630},
  eprint = {2106.05630},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.05630},
  url = {http://arxiv.org/abs/2106.05630},
  urldate = {2023-05-27},
  abstract = {Symbolic music understanding, which refers to the understanding of music from the symbolic data (e.g., MIDI format, but not audio), covers many music applications such as genre classification, emotion classification, and music pieces matching. While good music representations are beneficial for these applications, the lack of training data hinders representation learning. Inspired by the success of pre-training models in natural language processing, in this paper, we develop MusicBERT, a large-scale pre-trained model for music understanding. To this end, we construct a large-scale symbolic music corpus that contains more than 1 million music songs. Since symbolic music contains more structural (e.g., bar, position) and diverse information (e.g., tempo, instrument, and pitch), simply adopting the pre-training techniques from NLP to symbolic music only brings marginal gains. Therefore, we design several mechanisms, including OctupleMIDI encoding and bar-level masking strategy, to enhance pre-training with symbolic music data. Experiments demonstrate the advantages of MusicBERT on four music understanding tasks, including melody completion, accompaniment suggestion, genre classification, and style classification. Ablation studies also verify the effectiveness of our designs of OctupleMIDI encoding and bar-level masking strategy in MusicBERT.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Multimedia,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  note = {Comment: Accepted by ACL 2021 Findings},
  file = {/home/carlos/Zotero/storage/JL4NSPDK/Zeng et al. - 2021 - MusicBERT Symbolic Music Understanding with Large.pdf;/home/carlos/Zotero/storage/VIRCCY7D/2106.html}
}

@book{zhang2023dive,
  title = {Dive into Deep Learning},
  author = {Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
  year = {2023},
  publisher = {{Cambridge University Press}},
  note = {\href{https://D2L.ai}{https://D2L.ai}}
}

@misc{zhouLeasttoMostPromptingEnables2023,
  title = {Least-to-{{Most Prompting Enables Complex Reasoning}} in {{Large Language Models}}},
  author = {Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and Chi, Ed},
  year = {2023},
  month = apr,
  number = {arXiv:2205.10625},
  eprint = {2205.10625},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.10625},
  url = {http://arxiv.org/abs/2205.10625},
  urldate = {2023-06-25},
  abstract = {Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99\% using just 14 exemplars, compared to only 16\% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  note = {Comment: ICLR 2023},
  file = {/home/carlos/Zotero/storage/7MQPZXWI/Zhou et al. - 2023 - Least-to-Most Prompting Enables Complex Reasoning .pdf;/home/carlos/Zotero/storage/S48WGUZE/2205.html}
}

@misc{zhuangComprehensiveSurveyTransfer2020,
  title = {A {{Comprehensive Survey}} on {{Transfer Learning}}},
  author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  year = {2020},
  month = jun,
  number = {arXiv:1911.02685},
  eprint = {1911.02685},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1911.02685},
  url = {http://arxiv.org/abs/1911.02685},
  urldate = {2024-01-25},
  abstract = {Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning researches, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey paper reviews more than forty representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-21578, and Office-31. And the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: 31 pages, 7 figures},
  file = {/home/carlos/Zotero/storage/N58MIQLS/Zhuang et al. - 2020 - A Comprehensive Survey on Transfer Learning.pdf;/home/carlos/Zotero/storage/23KDLTRQ/1911.html}
}

@misc{zraraPORTFOLIOOPTIMIZATIONUSING2020,
  title = {{{PORTFOLIO OPTIMIZATION USING DEEP LEARNING FOR THE MOROCCAN MARKET}}},
  author = {Zrara, Lamiaa},
  year = {2020},
  url = {http://www.aui.ma/sse-capstone-repository/pdf/fall-2020/PORTFOLIO%20OPTIMIZATION%20USING%20DEEP%20LEARNING%20FOR%20THE%20MOROCCAN%20MARKET.pdf},
  urldate = {2024-01-27}
}
