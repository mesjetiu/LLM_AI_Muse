\chapter{Marco teórico}

\section{Inteligencia artificial, aprendizaje automático y aprendizaje profundo}

Comencemos por definir las diferentes disciplinas que intervienen en el estudio y desarrollo de sistemas de inteligencia artificial. A menudo se encuentran términos como \textit{inteligencia artificial} (\textit{Artificial Intelligence}), \textit{aprendizaje automático} (\textit{Machine Learning}) y \textit{aprendizaje profundo} (\textit{Deep Learning}) usados de forma intercambiable. Sin embargo, cada uno tiene un significado específico, y es crucial diferenciarlos para comprender el estado actual de la investigación en el área. Estos conceptos se organizan jerárquicamente \citep{torresivinalsPythonDeepLearning2020}, donde la inteligencia artificial es el término más general, que abarca el \textit{aprendizaje automático}, y este último incluye al \textit{aprendizaje profundo} (ver Figura \ref{fig:ai_ml_dl}).

\begin{figure}[]
    \caption{Relación AI, ML y DL}
    \centering
    \includegraphics[width=0.8\textwidth]{./figuras/AI_ML_DL.png}
    \source{Jzh2074 \protect\citeyear{jzh2074}, Wikimedia Commons; Wang et al. \protect\citeyear{wang2021promising}.}
    \label{fig:ai_ml_dl}
\end{figure}

% He añadido una imagen de relación AI, ML, DL y AI generativa. Sería interesante modificar lo anterior para hablar también de la IA generativa, que al fin y al cabo es la que nos interesa. La imagen la he sacado de aquí: https://medium.com/@kitkat73275/introduction-to-generative-ai-833c9c467dfa (añadirla a bibliografía y citarla)


La inteligencia artificial es el campo de estudio más antiguo entre los tres, y no se limita únicamente al ámbito computacional. En su sentido más amplio, la inteligencia artificial ha sido abordada por la filosofía. Los racionalismos y estructuralismos, fundamentales en los sistemas de pensamiento occidentales, forman la base conceptual de la inteligencia artificial. Sin embargo, no es hasta el siglo XX cuando se empieza a considerar la posibilidad matemática de un sistema que genere inteligencia. En 1950, Alan Turing publicó su artículo \textit{Computing Machinery and Intelligence} \citep{alan1950a}, en el que propuso un test para determinar si una máquina puede pensar. Este test, conocido como \textit{Test de Turing}, consiste en la interacción de un humano con una entidad artificial usando solo un terminal de texto como interfaz. La máquina aprueba el test si el humano no puede discernir si está interactuando con una entidad artificial o humana. A pesar de sus limitaciones y su enfoque antropocéntrico sobre la inteligencia, este test sigue siendo una referencia común para evaluar sistemas modernos de IA.

No obstante, el concepto de inteligencia artificial trasciende la mera imitación humana, aunque no lo descarta. Si consideramos la \textit{racionalidad} como un conjunto de estructuras lógicas que incluyen el pensamiento y la racionalidad humanos, la inteligencia artificial no necesita limitar su objetivo a superar el test de Turing. Las definiciones de inteligencia artificial a lo largo del tiempo han variado dependiendo de si se enfocan en imitar el pensamiento o acción \textit{humanos} o el pensamiento y acción \textit{racional} \citep{RussellStuartJ2021AI:A}.

Debemos a matemáticos como Alan Turing y Curt Gödel los fundamentos matemáticos de los procesos de pensamiento, considerados sistemas computacionales capaces de generar \textit{outputs} racionales a partir de \textit{inputs} arbitrarios. La idea de que el cerebro humano es una de las posibles \textit{máquinas de Turing} \citep{penroseNuevaMenteEmperador2015} ha sido un estímulo para la investigación en IA computacional. Además, investigaciones en \textit{procesamiento del lenguaje natural}, en las que destaca el concepto de \textit{entropía} de Shannon \citep{shannon1951prediction} y que han acompañado el desarrollo de lenguajes de programación, son fundamentales para los sistemas actuales de IA.


\section{Machine Learning}

El \textit{Machine Learning}, o \textit{aprendizaje automático}, es una subdisciplina de la inteligencia artificial que investiga algoritmos y modelos matemáticos que habilitan a un sistema computacional a aprender a partir de datos sin ser explícitamente programado. Estos sistemas identifican patrones y toman decisiones con poca o sin intervención humana. En el \textit{Machine Learning}, el modelo se autoconfigura basándose en los datos. Las únicas intervenciones humanas son el diseño de la arquitectura y la provisión de los datos de entrenamiento, aunque incluso estas tareas podrían delegarse a otro sistema de \textit{Machine Learning} en determinadas circunstancias.

Aunque el \textit{Machine Learning} ha sido una área de interés desde los inicios de la computación, es esencial reconocer que es solo una faceta de la inteligencia artificial. No todos los sistemas inteligentes son sistemas de \textit{Machine Learning}. Por ejemplo, el \textit{Deep Blue} de IBM, que venció al campeón mundial de ajedrez Garry Kasparov en 1997, no se basaba en \textit{Machine Learning} \citep{campbellDeepBlue2002}. En lugar de ello, \textit{Deep Blue} utilizaba una vasta base de datos de jugadas de ajedrez y algoritmos de búsqueda para decidir el mejor movimiento en cada situación. Otros sistemas, como los \emph{sistemas expertos}, utilizan reglas predefinidas para tomar decisiones y se aplican en áreas como medicina, ingeniería y gestión empresarial.

En términos generales, el objetivo del \textit{Machine Learning} es encontrar una función matemática que describa un conjunto de datos de entrenamiento y que, posteriormente, pueda prever datos desconocidos con precisión. Esta capacidad predictiva se conoce como \textit{inferencia}. El proceso busca que la función determinada durante el entrenamiento aproxime la función real que describe los datos, permitiendo al sistema generalizar de forma análoga al cerebro humano.

Se utiliza el término \textit{modelo} para referirse al sistema una vez que ha sido entrenado y posee capacidad predictiva. Dependiendo de su aplicación, un modelo puede ser empleado para predecir datos desconocidos o clasificarlos. Si produce un valor numérico, se habla de \textit{regresión}; si categoriza datos, de \textit{clasificación}. La clasificación puede ser binaria o multiclase, y la regresión unidimensional o multidimensional.

El aprendizaje en \textit{Machine Learning} se produce a través de un proceso de entrenamiento. Según la naturaleza de los datos y el método de validación, existen tres tipos principales de aprendizaje automático \citep[p. ~38]{torresivinalsPythonDeepLearning2020}:

\begin{itemize}
    \item \textbf{Aprendizaje supervisado:} Aquí, los datos se etiquetan con la respuesta esperada, como imágenes de animales etiquetadas como \textit{gato} o \textit{perro}. Tras el entrenamiento, se espera que el sistema identifique imágenes no etiquetadas correctamente.
    
    \item \textbf{Aprendizaje no supervisado:} En este caso, los datos no están etiquetados. El sistema busca patrones y agrupa datos en categorías por sí mismo.
    
    \item \textbf{Aprendizaje por refuerzo:} El sistema aprende interactuando con un entorno. No recibe etiquetas explícitas, sino recompensas por decisiones correctas y penalizaciones por errores.
\end{itemize}


\section{Deep Learning}

qué es el Deep Learning, y por qué ha evolucionado tanto en los últimos años, factores: poder computacional, cantidad de datos disponibles, democratización de la computación, mejoras en las arquitecturas y modelos de Deep Learning.

A partir de ahí se explican detalles desde las redes neuronales hasta Transformer, que es el estado del arte actual de deep learning, incluyendo los modelos de lenguaje que nos atañen.


\subsection{Tipos de redes neuronales}
\subsection{Arquitecturas de redes neuronales}
\subsection{Datos y entrenamiento de modelos de \textit{Deep Learning}}

\subsection{La arquitectura Transformer}

En la Figura \ref{fig:transformer_architecture} se puede ver un esquema de la arquitectura \textit{Transformer}.

\begin{figure}[]
    \caption[Aquitectura de Transformer]{Aquitectura de Transformer}
    \centering
    \includegraphics[width=0.4\textwidth]{./figuras/Transformer_architecture.png}
    \source{\cite{vaswaniAttentionAllYou2017}}
    \label{fig:transformer_architecture}
\end{figure}

\section{Modelos de lenguaje}

% Nos centramos en los modelos de lenguaje, para entender su funcionamiento. Importantísimo para comprender el papel de estos en la generación de código.

Un \textit{modelo de lenguaje} no es más que un modelo estadístico que asigna una probabilidad a una secuencia de palabras dada como input. En ese sentido, se reduce a una función matemática \textit{modelada} para imitar la forma en la que se escribe en lenguaje natural. Una forma sencilla de entender qué hace un \textit{modelo de lenguaje} es pensar en un teclado predictivo. Cuando escribimos un mensaje en nuestro teléfono móvil, el teclado predictivo nos sugiere las palabras que más probablemente podrían seguir a las que hemos escrito. Este comportamiento aparentemente inocuo y trivial es el que está detrás de los \textit{modelos de lenguaje}, incluyendo los chatbots más avanzados del momento.

Más técnicamente hablando, un \textit{modelo de lenguaje} devuelve como output la distribución de pobabilidad del siguiente \textit{token}, dada una lista de \textit{tokens} como entrada \citep{GenerationLLMs}. Un \textit{token} es la unidad mínima de información que recibe el \textit{modelo de lenguaje} y equivale, grosso modo, a una palabra\footnote{Un token puede ser una palabra, pero también un signo de puntuación, un número, o cualquier otra unidad mínima de información. En el caso de los modelos de lenguaje, los tokens suelen ser palabras, pero en el caso de los modelos de \textit{Machine Learning} en general, los tokens pueden ser cualquier unidad mínima de información, como por ejemplo los píxeles de una imagen.}. En la Figura \ref{fig:llm_generation} se puede ver un ejemplo de un \textit{modelo de lenguaje} que recibe como input una lista de palabras y devuelve como output la distribución de probabilidad de la siguiente, tras su entreamiento con grandes cantidades de textos en inglés.

\begin{figure}[]
    \caption[Inferencia de \textit{token} de un LLM]{Inferencia de \textit{token} de un LLM}
    \centering
    \includegraphics[width=0.9\textwidth]{./figuras/LLM_predice_token.png}
    \source{\cite{GenerationLLMs}}
    \label{fig:llm_generation}
\end{figure}

Los \textit{modelos de lenguaje} no están asociados a una única arquitectura de \textit{Machine Learning}, sino que pueden ser implementados por diversos tipos de redes neuronales, como las redes neuronales recurrentes, las redes neuronales convolucionales, o las redes neuronales Transformer, de las cuales se ha hablado más arriba. En todo caso, el punto de inflexión que ha permitido avances sin precedentes en el mundo del \textit{Machine Learning} ha sido, sin duda alguna, el desarrollo de la arquitectura \textit{Transformer}. 

\subsection{Grandes modelos de lenguaje}

Un \textit{gran modelo de lenguaje} (LLM) es un modelo de lenguaje con un número de parámetros del orden del billón. El primer gran modelo de lenguaje fue \textit{GPT-2}, desarrollado y entrenado por OpenAI en 2019 \citep{radfordLanguageModelsAre2019a}. \textit{GPT-2} fue entrenado con 40 GB de texto de Internet, y alcanzó un tamaño de 1.5 billones de parámetros. \textit{GPT-2} fue entrenado con el objetivo de predecir la siguiente palabra de una secuencia de texto dada, sorprendiendo a la comunidad científica por la calidad de los textos generados, sin precedentes en la historia del \textit{Deep Learning}. A pesar de ello, OpenAI decidió publicar un modelo reducido, por miedo a los posibles usos irresponsables de esta nueva tecnología. El modelo puesto al público constaba de 117 millones de parámetros, el que utilizó para generar los textos de prueba que se pueden ver en la Figura \ref{fig:gpt2_text_generation}. 

\begin{figure}[]
    \caption{Generación de textos por \textit{GPT-2}}
    \centering
    \includegraphics[width=0.9\textwidth]{./figuras/GPT2_text_generation.png}
    \source{\cite{RunTextGeneration2022}}
    \label{fig:gpt2_text_generation}
\end{figure}

Los \textit{grandes modelos de lenguaje} se basan en la arquitectura \textit{Transformer} \citep{vaswaniAttentionAllYou2017}, la cual ha permitido una eficiencia computacional y una escalabilidad de los modelos sin ningún precedente. Desde su aparición en 2017 la escalada por el tamaño de los modelos ha sido exponencial. En la Figura \ref{fig:llm_sizes} se puede comparar el tamaño de diversos LLM a lo largo del tiempo.

\begin{figure}[]
    \caption{Gráfico comparativo de tamaños de LLM}
    \centering
    \includegraphics[width=0.9\textwidth]{./figuras/LLMs_sizes.png}
    \source{\cite{ChallengesAssociatedBuilding}}
    \label{fig:llm_sizes}
\end{figure}

\subsection{Modelos prentrenados}
\subsection{Ajuste fino de los modelos}
\subsection{Habilidades emergentes de los modelos de lenguaje}
\subsection{Prompting engineering}
Exponer aquí esta disciplina nueva, y un esquema general de todas las aproximaciones a ello. \citep{LLMPromptingGuide}

\section{SuperCollider}

