\section{\emph{Prompting engineering}}
\label{sec:llm_tecnicas_prompting}

El concepto de \emph{prompting engineering} se refiere al estudio de diferentes métodos mediante los cuales se solicita a un modelo generativo que produzca una respuesta \citep{LLMPromptingGuide}. Específicamente, en el contexto de un \gls{llm}, esto implica generar texto basado en un \emph{prompt} o entrada de texto en lenguaje natural proporcionada por el usuario. Actualmente, el \emph{prompting} representa un área de investigación dinámica que ha originado una amplia variedad de estudios y publicaciones\footnote{Sin embargo, el término \emph{prompting engineering} es objeto de debate y no cuenta con un consenso universal dentro de la comunidad científica. A pesar de la controversia y de que algunos cuestionan su validez, existen numerosas investigaciones científicas que emplean esta terminología.}.

\subsection{Técnicas más importantes de \emph{prompting engineering}}

En función del propósito de la interacción con sistemas basados en \gls{llm}, existen diferentes técnicas {prompting} (véase Figura \ref{fig:prompting_engineering}). La forma más sencilla de \emph{prompting} es la denominada \emph{zero-shot}\footnote{\emph{Zero-shot} se traduce como \emph{sin ejemplos previos}.}, en la que el usuario presenta al \gls{llm} un \emph{prompt} y el \gls{llm} genera un texto de salida. En este caso, el \gls{llm} no ha sido entrenado para realizar una tarea específica, sino que ha sido entrenado con textos en lenguaje natural de propósito general. Sin embargo, el \gls{llm} puede generar un texto de salida que se corresponda con la tarea que el usuario quiere realizar. Por ejemplo, si el usuario presenta al \gls{llm} el {prompt} <<\emph{¿Cuál es la capital de Francia}>>, el \gls{llm} puede generar un texto de salida como <<\emph{La capital de Francia es París}>>. En este caso, el \gls{llm} no ha sido entrenado como experto en geografía, pero ha sido entrenado con textos en lenguaje natural de propósito general, y es capaz de generar un texto de salida que se corresponde con la petición del usuario. La limitación de esta técnica es la del propio entrenamiento del \gls{llm}. Este no podrá generar en ningún caso nada en lo que no haya sido previamente entrenado.


\subsubsection{\emph{Few-shot prompting}}
\label{sec:few_shot_prompting}

Una primera técnica aplicable a un modelo de lenguaje es la de \emph{few-shot}\footnote{\emph{Few-shot} se traduce como \emph{con pocos ejemplos.}}, en la que el modelo recibe como input una serie de pares petición-respuesta más una petición a la cual ha de responder de forma análoga al contexto. Se presenta muy útil para tareas que requieren de cierto automatismo, como la generación de respuestas con un formato concreto, ya que en el prompt se muestra al \gls{llm} qué tipo de respuesta se requiere (véase Figura \ref{fig:few_shot_prompting}). De hecho, esta es la técnica que subyace tras las interfaces tipo \emph{chat} que se pueden encontrar en la mayoría de los \gls{llm} conversacionales. Si bien, fueron prentrenados para completar textos, pueden ser guiados por el \emph{few-shot prompting} a generar respuestas en un formato y personalidad concreta, ya que el modelo tenderá a repetir el estilo de los ejemplos pasados por contexto.

% \begin{figure}[H]
%     \caption[Técnica de \emph{few-shot prompting}]{Técnica de \emph{few-shot prompting}. El LLM recibe como input una serie de pares <<petición-respuesta>> más una petición a la cual ha de responder de forma análoga al contexto. En este caso, el LLM, con papel de <<asistente>>, ha de devolver el input del <<usuario>> con las letras invertidas. Sin embargo, nunca se le pide explícitamente que invierta las letras, sino que lo ha de inferir a partir de los ejemplos.}
%     \centering
%     \includegraphics[width=0.2\textwidth]{./figuras/few_shot_prompting.png}
%     \source{\propio}
%     \label{fig:few_shot_prompting}
% \end{figure}


\begin{figure}[H]
    \caption[Técnica de {few-shot prompting}]{Técnica de {few-shot prompting}.}
    \centering
    \begin{subfigure}{.48\textwidth}
      \centering
      \setstretch{1}
      \fontsize{9.5pt}{11pt}\selectfont
      \begin{mdframed}
        Usuario: Casa
        \vspace{0.1cm}

        Asistente: Asac
        \vspace{0.1cm}

        Usuario: Ciclo
        \vspace{0.1cm}

        Asistente: Oleic
        \vspace{0.1cm}

        Usuario: Ropa
        \vspace{0.2cm}
      \end{mdframed}
    %   \caption{Csound}
    \end{subfigure}

    \vspace{0.2cm}
    \centering
    \begin{subfigure}{.48\textwidth}
        \setstretch{1}
        \fontsize{9.5pt}{11pt}\selectfont
        \begin{mdframed}
        Asistente: Apor
        \end{mdframed}
      \end{subfigure}
      \note{El LLM recibe como input una serie de pares \emph{petición-respuesta} más una petición a la cual ha de responder de forma análoga al contexto. En este caso, el LLM, con papel de \emph{asistente}, ha de devolver el input del \emph{usuario} con las letras invertidas. Sin embargo, nunca se le pide explícitamente que invierta las letras, sino que lo infiere a partir de los ejemplos dados.}
      \source{\propio}
      \label{fig:few_shot_prompting}
\end{figure}

\subsubsection{\emph{Chain of thoughts}}

Una técnica más compleja, recientemente elaborada, es la denominada \gls{cot}\footnote{Se puede traducir como \emph{cadena de pensamientos}.}, que consiste en presentar al \gls{llm} un problema y pedirle que explique su razonamiento antes de dar la respuesta. Esta técnica se ha mostrado muy efectiva en especial en tareas de resolución de problemas aritméticos, ya que obliga al \gls{llm} a razonar antes de dar una respuesta, lo que lleva a una respuesta correcta en la mayoría de los casos \citep{weiChainofThoughtPromptingElicits2023}. Esta técnica pone en evidencia el hecho de que los \gls{llm} no razonan necesariamente su respuesta, a pesar de que esta pueda resultar convincente y pasar por correcta (véase \ref{sec:limitaciones_llm}). Por otra parte, esta técnica ha mostrado que el \emph{prompting} tiene un gran poder sobre el resultado de la inferencia en los \gls{lm}. 

La Figura \ref{fig:chain_of_thoughts} muestra un ejemplo de \gls{cot} aplicado a un problema aritmético. Obsérvese cómo el \gls{llm} llega a la respuesta correcta solo cuando se le incita a dar una explicación de su razonamiento. En este caso no se le ha pedido esta explicación explícitamente, sino a través de la técnica de \emph{few-shot prompting}. 

% Las técnicas que se presentan a continuación pueden entenderse como derivaciones del principio de \gls{cot}, en los cuales se fuerza un razonamiento explícito por parte del \gls{llm} antes de dar una respuesta.

\begin{figure}[H]
    \caption[Chain of thoughts]{\emph{Chain of thoughts}.}
    \centering
    \begin{subfigure}{.48\textwidth}
      \centering
      \begin{mdframed}
        \setstretch{1}
        \fontsize{9.5pt}{11pt}\selectfont
        P: Roger tiene 5 pelotas de tenis. Compra 2 latas más de pelotas de tenis. Cada lata tiene 3 pelotas de tenis. ¿Cuántas pelotas de tenis tiene ahora?
        \vspace{0.2cm}

        R: La respuesta es 11.
        \vspace{0.2cm}

        P: La cafetería tenía 23 manzanas. Si usaron 20 para hacer el almuerzo y compraron 6 más, ¿cuántas manzanas tienen?
        \vspace{0.2cm}
      \end{mdframed}
    %   \caption{Csound}
    \end{subfigure}\hfill
    \begin{subfigure}{.48\textwidth}
      \centering
      \begin{mdframed}
        \setstretch{1}
        \fontsize{9.5pt}{11pt}\selectfont
        P: Roger tiene 5 pelotas de tenis. Compra 2 latas más de pelotas de tenis. Cada lata tiene 3 pelotas de tenis. ¿Cuántas pelotas de tenis tiene ahora?
        \vspace{0.2cm}

        R: \highlight{Roger comenzó con 5 pelotas. 2 latas de 3 pelotas de tenis cada una hacen 6 pelotas de tenis. 5 + 6 = 11.} La respuesta es 11.
        \vspace{0.2cm}

        P: La cafetería tenía 23 manzanas. Si usaron 20 para hacer el almuerzo y compraron 6 más, ¿cuántas manzanas tienen?
        \vspace{0.2cm}
      \end{mdframed}
    %   \caption{SuperCollider}
    \end{subfigure}

    \vspace{0.2cm}

    \begin{subfigure}{.48\textwidth}
        \centering
        \begin{mdframed}
          \setstretch{1}
          \fontsize{9.5pt}{11pt}\selectfont
        R: La respuesta es 29.
        \end{mdframed}
        \caption{\emph{Few-shot} para forzar una resolución directa de un problema aritmético. La respuesta es incorrecta.}
      \end{subfigure}\hfill
    \begin{subfigure}{.48\textwidth}
      \centering
      \begin{mdframed}
        \setstretch{1}
        \fontsize{9.5pt}{11pt}\selectfont
        R: \highlight{Comenzaron con 23 manzanas. Usaron 20, por lo que quedaron 23-20 = 3 manzanas. Compraron 6 más, por lo que ahora tienen 3+6 = 9 manzanas.} La respuesta es 9.
      \end{mdframed}
      \caption{\emph{Few-shot} con \emph{chain of thoughts} para provocar un razonamiento previo a la respuesta. La respuesta es correcta.}
    \end{subfigure}
    \note{En ambos ejemplos se plantea un problema aritmético a GPT-3. En (\textbf{a}), por medio de \emph{few-shot prompting}, se le pide una respuesta directa al problema. En (\textbf{b}), aplicando, además, la técnica de \emph{chain of thoughts}, se le pide que explique su razonamiento, lo cual lleva a una respuesta correcta. Se ha resaltado el texto correspondiente a los razonamientos.}
    \source{\cite{weiChainofThoughtPromptingElicits2023}. Traducción propia.}
    \label{fig:chain_of_thoughts}
\end{figure}

\subsubsection{\emph{Self consistency of chain of thoughts}}

La técnica de \gls{cot_sc}\footnote{Se puede traducir como \emph{autoconsistencia de la cadena de pensamientos}.} mejora la metodología de \gls{cot} al requerir que el \gls{llm} genere múltiples razonamientos para un problema antes de decidir la respuesta más coherente. Este enfoque no solo estimula un análisis profundo, sino que también permite evaluar diferentes perspectivas para hallar la solución más sólida. \citeauthor{wangSelfConsistencyImprovesChain2023} (\citeyear{wangSelfConsistencyImprovesChain2023}) muestran cómo esta técnica mejora a la de \gls{cot} tanto en el terreno aritmético como en el del razonamiento por sentido común. La Figura \ref{fig:cot_sc} ilustra con un ejemplo esta técnica aplicada a un problema aritmético.

\begin{figure}[H]
    \caption[Esquema de los pasos en los que se divide la técnica {Self Consistency of Chain of Thoughts}]{Esquema de los pasos en los que se divide la técnica {Self Consistency of Chain of Thoughts}.}
    \centering
    \includegraphics[width=0.9\textwidth]{./figuras/cot_sc.png}
    \source{\cite{wangSelfConsistencyImprovesChain2023}}
    \label{fig:cot_sc}
\end{figure}

\subsubsection{\emph{Tree of thoughts}}

Esta técnica \citep{LLMPromptingGuide} implementa un árbol de decisión. Se solicita al \gls{llm} que haga una búsqueda en profundidad y anchura en un árbol de pensamientos, pudiendo ir hacia atrás en la estructura si no encuentra una respuesta satisfactoria. La Figura \ref{fig:prompting_engineering} muestra un esquema de esta técnica comparada con la de \gls{cot} y \gls{cot_sc}.

\begin{figure}[H]
  \caption[Técnicas de \emph{prompting engineering}]{Técnicas de \emph{prompting engineering} aplicadas a problemas aritméticos.}
  \centering
  \includegraphics[width=0.9\textwidth]{./figuras/prompt_engineering_techniques.png}
  \source{\cite{bhavsarPromptEngineeringArithmetic2023}}
  \label{fig:prompting_engineering}
\end{figure}


% El buen desempeño de los \gls{llm} en tareas de código de programación lleva a estudios como el de \cite{liStructuredChainofThoughtPrompting2023}, que propone un sistema de prompting que pide al \gls{llm} una reflexión explícita sobre la implementación a realizar, en una especie de pseudocódigo, que en un segundo paso será traducido al código final de programación. Este sistema de prompting se basa en la idea de que el \gls{llm} generará código de mayor calidad si se le permite \emph{pensar} la respuesta antes de generarla. En este sentido, existen sobradas publicaciones que ponen de relieve la sensibilidad de los \gls{llm} a la forma en la que se le pide que procese la información en el prompt y la correlación en la calidad de sus resultados \citep{zhouLeasttoMostPromptingEnables2023,weiChainofThoughtPromptingElicits2023,LLMPromptingGuide}.
 
\subsection{\emph{Retrieval-augmented generation}}
\label{sec:rag}

Los \gls{lm} están prentrenados con grandes cantidades de texto de propósito general, lo que los hace capaces y hábiles en múltiples campos sin necesidad de un entrenamiento específico (véase Figura \ref{fig:fundation_models_habilities}). Es posible, sin embargo, rentrenarlos con datos más concretos y precisos para una tarea específica (por ejemplo, con datos médicos, ingenieriles, con datos privados de una empresa, etc.). A este proceso, como se vio en la sección \ref{prentrenamiento_fine_tuning}, se le conoce como \emph{fine-tuning} o ajuste fino. Sin embargo, el proceso de \emph{fine-tuning} de un \gls{llm} requiere de grandes cantidades de datos y tiempo de entrenamiento, algo que en este momento no está al alcance del usuario medio. Por ello, se están desarrollando técnicas que permitan a los usuarios aprovechar los modelos prentrenados para tareas específicas sin necesidad de \emph{fine-tuning}. Una de estas técnicas es \gls{rag} \citep{WhatRetrievalaugmentedGeneration2021}, que podemos traducir por \emph{generación mejorada por recuperación}, que combina la recuperación de información con la generación de texto. Esta es una técnica que ofrece resultados muy notables en términos de eficacia y eficiencia frente al \emph{fine-tuning}. \gls{rag} consiste en pasar como contexto al \gls{llm} junto a la petición del usuario, una serie de fragmentos de texto que se corresponden semánticamente con la petición, y que se obtienen de una base de datos de documentos relacionados con la tarea a realizar. Esta base de datos puede ser de cualquier tipo, desde una base de datos de texto en lenguaje natural, hasta una base de datos de código de programación, pasando por una base de datos de partituras musicales, o una base de datos de audio, y representan la base de conocimiento extra (\emph{knowledge}) que el \gls{llm} manejará en sus interacciones con el usuario. Este sistema de \emph{prompting} está en la base actual de muchos productos relacionados con \gls{llm}, como Assistant o GPTs de OpenAI. La Figura \ref{fig:rag} muestra el esquema de funcionamiento de \gls{rag}. La base de datos del conocimiento del \gls{llm} es dividido en una primera fase en fragmentos más pequeños y analizados por el \gls{llm} para obtener una representación vectorial de cada uno de ellos. En una segunda fase, el \gls{llm} recibe el {prompt} del usuario y lo analiza para obtener una representación vectorial. En una tercera fase, el \gls{llm} busca en la base de datos de fragmentos aquellos que tienen similitud vectorial (y, por ende, semántica) con el {prompt} del usuario. En una cuarta fase, el \gls{llm} genera el texto de salida a partir del {prompt} del usuario y de los fragmentos de texto recuperados de la base de datos. 

\begin{figure}[H]
  \caption[Habilidades emergentes de los \emph{Foundation Models} de OpenAI]{Habilidades de los \emph{Foundation Models} de OpenAI.}
  \centering
  \includegraphics[width=0.6\textwidth]{./figuras/fundation_models_habilities.png}
  \note{Los LLM tienen habilidades emergentes que surgen, entre otras cosas, por su gran tamaño. No es posible predecir qué habilidades tendrá un LLM mayor que el estado del arte.}
  \source{\cite{GPT3RiseFoundation}}
  \label{fig:fundation_models_habilities}
\end{figure}

\begin{figure}[H]
    \caption[Esquema de funcionamiento de RAG]{Esquema de funcionamiento de RAG.}
    \centering
    \includegraphics[width=0.7\textwidth]{./figuras/rag.png}
    \note{El LLM recibe como input el {prompt} del usuario y, como contexto, una serie de fragmentos de texto de la base de datos de conocimiento, que tienen similitud vectorial con el {prompt} del usuario.}
    \source{\cite{WhatRetrievalAugmented}}
    \label{fig:rag}
\end{figure}


\subsubsection{\emph{Fine-tuning} versus \emph{retrieval-augmented generation}}

Las diversas técnicas de {prompting} ofrecen una serie de ventajas frente al \emph{fine-tuning}, como la posibilidad de utilizar modelos prentrenados de propósito general, tarea más eficiente que reentrenar modelos para una tarea específica, o que usar bases de datos de conocimiento de gran tamaño (técnica de \gls{rag}), que son más fáciles de obtener que los grandes conjuntos de datos necesarios para el \emph{fine-tuning}. Además, la base de datos de conocimiento puede ser actualizada de forma independiente al modelo, lo que permite una mayor flexibilidad en el uso de los modelos. Por contra, en la técnica de \gls{rag} el modelo no tiene conocimiento global de los contenidos de la base de datos, puesto que no ha sido reentrenado con ellos, por lo que no puede realizar inferencias globales sobre ella, y la calidad de los resultados depende en gran medida de cómo se ha dividido su contenido en fragmentos y de la calidad de la representación vectorial de cada uno de ellos. Se ha probado un mayor rendimiento, en tareas específicas, de técnicas combinadas de \emph{fine-tuning} y \gls{rag} \citep{lewisRetrievalAugmentedGenerationKnowledgeIntensive2021}.



% \subsection{Habilidades emergentes de los modelos de lenguaje}
% \todo{Por hacer: habilidades emergentes de los modelos de lenguaje}


