
\section{Inteligencia artificial, machine learning, deep learning e inteligencia artificial generativa}

Comencemos delimitando las diferentes áreas en las que se inscribe el estudio y desarrollo de sistemas de \glsdisp{ia}{inteligencia artificial}. A menudo se encuentran términos como \glsdisp{ia}{inteligencia artificial}, \gls{ml}, \gls{dl}, e, incluso, \gls{iag}, usados de forma intercambiable. Sin embargo, cada uno tiene un significado específico, y es crucial diferenciarlos para comprender el estado actual de la investigación. Estos conceptos se organizan jerárquicamente \citep{torresivinalsPythonDeepLearning2020}, donde la \gls{ia} es el término más general, que abarca el \gls{ml}, y este último incluye al \gls{dl} y la \gls{iag} (véase Figura \ref{fig:ai_ml_dl_gai}).

\begin{figure}[H]
    \caption{Relación entre inteligencia artificial, {machine learning}, {deep learning} e inteligencia artificial generativa.}
    \centering
    \includegraphics[width=0.4\textwidth]{./figuras/ai_ml_dl_gai.png}
    \source{\cite{kainatIntroductionGenerativeAI2023}}
    \label{fig:ai_ml_dl_gai}
\end{figure}



\subsection{La inteligencia artificial y sus precursores}

La \gls{ia}, como concepto genérico, es el campo de estudio más antiguo entre todos, y no se limita únicamente al ámbito computacional. En su sentido más amplio, la \gls{ia} ha sido abordada por la filosofía. 
% Los racionalismos y estructuralismos, fundamentales en los sistemas de pensamiento occidentales, forman la base conceptual de la \gls{ia}. 
Sin embargo, no es hasta el siglo XX cuando se empieza a considerar la posibilidad matemática de que un sistema pueda ser inteligente. En 1950, Alan Turing publicó su artículo \emph{Computing Machinery and Intelligence} \citep{alan1950a}, en el que propuso un test para determinar si una máquina puede pensar. Este test, conocido como \emph{test de Turing} (véase Figura~\ref{fig:test_turing}), tiene por objetivo determinar si una máquina puede exhibir un comportamiento inteligente indistinguible del de un ser humano. Su método consiste en la interacción de un humano-evaluador con una entidad artificial únicamente por medio de un terminal de texto como interfaz. La máquina pasará el test si el evaluador no puede discernir si está interactuando con una entidad artificial o humana. A pesar de sus limitaciones y su enfoque antropocéntrico sobre la inteligencia, este test sigue siendo una referencia común para evaluar sistemas modernos de \gls{ia}. Estudios recientes, sin embargo, muestran que no es únicamente la inteligencia cognitiva aquello que un humano evalúa en el test, sino que existen otros factores como el estilo lingüístico y los rasgos emocionales \citep{jonesDoesGPT4Pass2023}, o la creatividad y la originalidad \citep{noeverTuringDeception2022}.

\begin{figure}[H]
    \caption[Test de Turing]{Test de Turing. El humano-evaluador interactúa con un interlocutor desconocido a través de un terminal de texto. Si este no puede discernir si está interactuando con una entidad artificial o humana, la máquina pasa el test.}
    \centering
    \includegraphics[width=0.5\textwidth]{./figuras/test_turing.png}
    \source{\cite{TestTuring}}
    \label{fig:test_turing}
\end{figure}

No obstante, el concepto de \gls{ia} trasciende a la mera imitación de lo humano. Si consideramos la racionalidad como un conjunto de estructuras lógicas que incluyen el pensamiento y el entendimiento humanos, la \gls{ia} no necesita limitar su objetivo a superar el test de Turing. Las definiciones de \gls{ia} a lo largo del tiempo han variado dependiendo de si se enfocan en imitar el pensamiento o acción humanos o el pensamiento y acción racional \citep{RussellStuartJ2021AI:A}.

% Además de a Alan Turing, debemos a Curt Gödel los fundamentos matemáticos de los procesos de pensamiento, considerados sistemas computacionales capaces de generar \emph{outputs} racionales a partir de \emph{inputs} arbitrarios. 

A Turing debemos también el concepto de \emph{máquina universal}, que es la base de la computación moderna y de la inteligencia artificial computacional. Sobre este asunto debate sobradamente Roger Penrose en \cite{penroseNuevaMenteEmperador2015}. En esta misma línea, investigaciones en \emph{procesamiento del lenguaje natural}, en las que destaca el concepto pionero de \emph{entropía} de Claude E. Shannon \citep{shannon1951prediction} y que han acompañado el desarrollo de lenguajes de programación, son fundamentales para los sistemas actuales de \gls{ia}. La entropía de Shannon mide cuán probable es cualquier combinación de letras en un idioma concreto. Sus estudios en esta área han resultado fundamentales para la teorización de los \emph{modelos de lenguaje}, o \gls{lm}, que son la base de los sistemas de \gls{ia} actuales.


\subsection{\emph{Machine learning}}

El \glsdisp{ml}{\emph{machine learning}}, o \emph{aprendizaje automático}, es una rama de la \gls{ia} que investiga algoritmos y modelos matemáticos que habilitan a un sistema computacional a aprender a partir de datos sin ser explícitamente programado. Estos sistemas identifican patrones a partir de un conjunto de datos de entrenamiento, con poca o ninguna intervención humana, para después, en la fase de inferencia, predecir o clasificar datos a los que no tuvo acceso en la fase de entrenamiento \citep{gollapudi2016practical}. En el \gls{ml}, el sistema se autoconfigura basándose en los datos con los que se entrena. Las únicas intervenciones humanas en el proceso son el diseño de la arquitectura y la provisión de los datos de entrenamiento, aunque incluso estas tareas podrían delegarse a otro sistema de \gls{ml} en determinadas circunstancias\footnote{P. ej., muchos de los \emph{datasets} utilizados en el entrenamiento de sistemas de \gls{ia} actuales han sido sintetizados por medio de otros sistemas previos en producción \citep{liSyntheticDataGeneration2023}, lo cual reduce considerablemente sus costes de entrenamiento.}.

Aunque el \gls{ml} ha sido una área de interés desde los inicios de la computación, es esencial reconocer que no todos los sistemas inteligentes utilizan \gls{ml}. Como ejemplo, el software de ajedrez \emph{Deep Blue} de IBM, que venció al campeón mundial Garry Kasparov en 1997, no se basaba en \gls{ml} \citep{campbellDeepBlue2002}. En lugar de ello, {Deep Blue} utilizaba una vasta base de datos de jugadas de ajedrez y algoritmos de búsqueda heurísticos para decidir el mejor movimiento en cada situación. Otros sistemas no basados en \gls{ml}, como los \emph{sistemas expertos}, utilizan reglas predefinidas para tomar decisiones y se aplican ampliamente en áreas tan diversas como medicina, ingeniería o gestión empresarial.

% En términos generales, el objetivo del \gls{ml} es encontrar, de forma automática, funciones matemáticas que describan un conjunto de datos de entrenamiento y que, posteriormente, puedan ser utilizadas para prever datos desconocidos con precisión. Esta capacidad predictiva se conoce como \emph{inferencia}. El proceso busca que la función determinada durante el entrenamiento se aproxime lo más fielmente posible a la función real que describe los datos. Modelos complejos y grandes, como los actuales \gls{llm}, pueden llegar a generalizar de forma análoga al cerebro humano. 


Se utiliza el término \emph{modelo} para referirse al sistema una vez que ha sido entrenado y posee capacidad predictiva. Dependiendo de su aplicación, un modelo puede ser empleado para predecir datos desconocidos o clasificarlos. Si produce un valor numérico, se habla de \emph{regresión}; si el modelo categoriza datos, de \emph{clasificación} \citep{zhang2023dive}. La clasificación es binaria cuando el modelo solo devuelve dos posibles respuestas, o multiclase cuando clasifica en más de dos categorías. La Figura \ref{fig:ml_classification_regression} muestra de forma intuitiva la diferencia entre clasificación y regresión.

\begin{figure}[H]
    \caption[Problema de clasificación versus regresión]{Problema de clasificación versus regresión. Los modelos de machine learning aprenden de forma automática a categorizar datos nuevos (clasificación) o a predecir nuevos datos de una serie (regresión).}
    \centering
  
    \begin{subfigure}{.49\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{./figuras/regression-vs-classification_simple-comparison-image.png}
        \caption{Representación gráfica de un modelo de clasificación y otro de regresión. La clasificación consiste en categorizar un conjunto de datos en dos o más clases. La regresión permite predecir el siguiente dato a partir de una serie temporal o espacial.}
      \end{subfigure}\hfill
    \begin{subfigure}{.49\textwidth}
      \centering
      \includegraphics[width=.8\textwidth]{./figuras/regression-vs-classification-easy-example.png}
      \caption{Diagrama intuitivo para comprender la regresión y la clasificación.}
    \end{subfigure}

    \source{\textit{Regression vs. Classification} (\citeyear{RegressionVsClassification2021})}
    \label{fig:ml_classification_regression}
\end{figure}


El aprendizaje de sistemas de \gls{ml} se produce a través de un proceso de \emph{entrenamiento}. Según la naturaleza de los datos y el método de validación, existen tres tipos principales de aprendizaje automático \citep[p. ~38]{torresivinalsPythonDeepLearning2020}:

\begin{enumerate}
    \item \emph{Aprendizaje supervisado:} Aquí, los datos se etiquetan con la respuesta esperada, como, p. ej., imágenes de animales etiquetadas como \emph{gato} o \emph{perro}. Tras el entrenamiento, se espera que el sistema clasifique imágenes no etiquetadas en la categoría correcta. Este procedimiento de aprendizaje e inferencia queda reflejado en la Figura \ref{fig:labeled_data_training}. Una de las aplicaciones más comunes del aprendizaje supervisado es la clasificación de imágenes. Sin embargo, su debilidad consiste precisamente en la necesidad de etiquetado de los datos de entrenamiento, que puede ser un proceso costoso y laborioso si este solo se puede hacer manualmente.
    
    \item \emph{Aprendizaje no supervisado:} En este caso, los datos no están etiquetados. El sistema busca patrones y agrupa datos en categorías por sí mismo. Precisamente este es el tipo de aprendizaje utilizado en el entrenamiento de \gls{lm}, donde los datos de entrenamiento son textos sin etiquetar \citep{radfordLanguageModelsAre2019}.
    
    \item \emph{Aprendizaje por refuerzo:} El sistema aprende interactuando con un entorno. No recibe etiquetas explícitas, sino recompensas por decisiones correctas y penalizaciones por errores. Este es el modo de aprendizaje más parecido al humano, ya que se basa en la experiencia. Sin embargo, a pesar de ser uno de los métodos más atractivos de aprendizaje automático, es el más complejo de implementar y requiere un entorno de simulación adecuado, que no siempre es posible ni eficiente en términos de computación \citep{mohanStructureReinforcementLearning2023}.
\end{enumerate}

\begin{figure}[H]
    \caption[Esquema de aprendizaje supervisado]{Esquema de aprendizaje supervisado, usando datos de entrenamiento etiquetados. En la inferencia se espera que el modelo pueda clasificar datos no etiquetados en las categorías en las que fue entrenado.}
    \centering
    \includegraphics[width=0.6\textwidth]{./figuras/labeled_data_training.png}
    \source{\cite{FindWaysDeal}}
    \label{fig:labeled_data_training}
\end{figure}


\subsection{\emph{Deep learning}}

Esta sección pretende ser una somera introducción a los conceptos de \glsdisp{dl} y \gls{rna}, que constituyen la base de los \gls{lm}, objeto de nuestro estudio, con la única finalidad de aportar un aparato teórico mínimo en el que contextualizarlo. En los últimos meses la bibliografía sobre este tema está creciendo exponencialmente\footnote{Además de las obras citadas en este trabajo, se puede consultar, a modo de introducción para no especialistas, \cite{BeginnerGuideNeural}.}. El \gls{dl} es una rama del \gls{ml} que utiliza \gls{rna} para aprender de forma automática. Las \gls{rna} son modelos matemáticos cuyos principios imitan el funcionamiento de las neuronas biológicas y hunden sus raíces en la intersección entre biología, matemáticas y ciencias de la computación ya en la primera mitad del siglo XX.

\subsubsection{El concepto de \emph{neurona artificial}}

Para entender en qué consiste una \gls{rna} y, por extensión, un modelo de \gls{dl}, antes es necesario comprender el concepto de neurona artificial. Una neurona artificial no es sino un modelo matemático simplificado de una neurona natural que encontramos en el sistema nervioso de los animales. Análogamente a su homóloga biológica, una neurona artificial recibe una serie de valores de entrada, los procesa y devuelve una salida. La entrada de una neurona artificial es la suma ponderada de las salidas de las neuronas de la capa anterior. Esta suma ponderada se pasa a una función de activación, que determina la salida de la neurona. Sin necesidad de entrar en detalles matemáticos, la función de activación proporciona a la neurona y los sistemas de \gls{rna} una no linealidad, imprescindible para el comportamiento complejo de estos sistemas \citep{torresivinalsPythonDeepLearning2020,zhang2023dive}. La Figura \ref{fig:neurona_artificial_natural} muestra un esquema de una neurona artificial.

\begin{figure}[H]
    \caption[Neurona biológica y neurona artificial]{Neurona biológica y neurona artificial}
    \centering
    \includegraphics[width=0.9\textwidth]{./figuras/perceptron_with_neuron.png}
    \source{\cite{DeepLearningDL}}
    \label{fig:neurona_artificial_natural}
\end{figure}

El concepto de neurona artificial fue expuesto por primera vez por F. Rosenblatt en 1958 \citep{rothmanTransformersNaturalLanguage2021} bajo el nombre de \emph{perceptrón}. El perceptrón no es otra cosa que un modelo de clasificación binaria, es decir, que solo puede clasificar datos en dos categorías. Rosenblatt planteó su modelo para ser implementado en hardware, y, de hecho, se construyó un prototipo en 1959. Sin embargo, el perceptrón tenía ciertas limitaciones matemáticas que no permitían su aplicación a problemas más complejos. En 1969, Minsky y Papert publicaron el libro \emph{Perceptrons: An introduction to computational geometry} \citep{minsky1969perceptrons}, en el que demostraban que el perceptrón no podía resolver problemas linealmente no separables. Este hecho supuso un freno en la investigación en \gls{rna} ---lo que se ha venido a llamar \emph{invierno de la inteligencia artificial} \citep{InviernoIA2023}, que no se retomó hasta la década de los 80, cuando nuevos desarrollos en modelos de redes neuronales artificiales permitieron resolver problemas más complejos que un simple perceptrón. 

Actualmente sabemos que una red neuronal de varias capas con una función de activación no lineal puede aproximar arbitrariamente cualquier función matemática, lo que ha venido a llamarse \emph{teorema de la aproximación universal}, propuesto en 1989 por \citeauthor{hornikMultilayerFeedforwardNetworks1989} en su artículo \emph{Multilayer Feedforward Networks Are Universal Approximators} \citep{hornikMultilayerFeedforwardNetworks1989}. Este teorema tiene grandes implicaciones, no solo científicas, sino también filosóficas, en la medida en que podemos considerar que una \gls{rna} lo suficientemente compleja es capaz de imitar el razonamiento humano, susceptible de reducirse al de una función matemática por compleja que esta sea. En la literatura científico-filosófica del s. XX encontramos interesantes reflexiones sobre las implicaciones de que nuestro cerebro fuera un algoritmo matemático que genera pensamiento, como en en la ya citada obra de \cite{penroseNuevaMenteEmperador2015} o en \cite{searleMentesCerebrosCiencia1985}.


\subsubsection{Redes neuronales artificiales}
Una \gls{rna} es un modelo matemático que se compone de varias capas de neuronas artificiales. La primera capa se denomina \emph{capa de entrada} y recibe los datos de entrada. La última capa se denomina \emph{capa de salida} y devuelve la predicción del modelo. Las capas intermedias se denominan \emph{capas ocultas} y son las que permiten que el modelo pueda aproximar cualquier función matemática. La Figura \ref{fig:deep_neural_network} muestra un esquema de una \gls{rna} con una capa de entrada, dos capas ocultas y una capa de salida.

% La salida de la neurona se pasa a las neuronas de la capa siguiente, y así sucesivamente hasta llegar a la capa de salida. La salida de esta última capa es la predicción del modelo. 

\begin{figure}[H]
    \caption[Estructura de capas de una posible red neuronal artificial]{Estructura de capas de una posible red neuronal artificial, con una capa de entrada, tres capas ocultas y una capa de salida.}
    \centering
    \includegraphics[width=0.6\textwidth]{./figuras/Deep_neural_network.png}
    \source{\cite{zraraPORTFOLIOOPTIMIZATIONUSING2020}}
    \label{fig:deep_neural_network}
\end{figure}

En función de los objetivos de la \gls{rna}, su arquitectura o estructura interna puede variar considerablemente. Una \gls{rna} puede constar de una sola capa, de una capa de entrada y otra de salida y de múltiples capas ocultas\footnote{Precisamente de esta característica de \emph{profundidad} que puede llegar a tener una \gls{rna} con varias capas ocultas, toma el calificativo de \emph{profundo} o \emph{deep} el aprendizaje \emph{profundo} o \emph{deep} learning.}. Cada arquitectura busca resolver un tipo de problema concreto. Una arquitectura especialmente compleja y que ha dado muy buenos resultados en la visión artificial es la \gls{cnn} \citep{osheaIntroductionConvolutionalNeural2015}, inspirada en el funcionamiento y estructura de la corteza visual del cerebro humano. Las \gls{cnn} se utilizan para el reconocimiento de imágenes y vídeos, y han sido la base de los avances en el campo de la visión artificial en los últimos años.

El entrenamiento de una \gls{rna} o modelo de \gls{dl} implica ajustar sus parámetros internos, principalmente los \emph{pesos} de las conexiones neuronales (ver variables $w$ en la Figura \ref{fig:neurona_artificial_natural}), para que la salida generada se aproxime a la esperada, basándose en los datos de entrenamiento \citep{NurArtificialNeural2014}. Este proceso iterativo, ilustrado en la Figura \ref{fig:ann_training}, tiene como objetivo minimizar el margen de error entre la salida de la red y su valor esperado, hasta alcanzar un nivel aceptable. Una vez entrenada, la \gls{rna} puede realizar predicciones o clasificaciones. Inicialmente, un modelo no entrenado produce salidas aleatorias, pero a medida que se ajustan los pesos, la salida se vuelve más precisa. Hay una relación directa entre la cantidad de parámetros de un modelo y su capacidad predictiva, lo que implica también un aumento en el tiempo de entrenamiento y los recursos computacionales necesarios. Este aspecto ha sido crucial en los avances recientes que permiten a las \gls{rna} lograr resultados comparables a los del cerebro humano en tareas de generación de imágenes, video, audio o texto.

\begin{figure}[H]
    \caption[Diagrama de entrenamiento de una red neuronal artificial]{Diagrama de entrenamiento de una red neuronal artificial.}
    \centering
    \includegraphics[width=0.9\textwidth]{./figuras/ann_training.png}
    \source{\propio}
    \label{fig:ann_training}
\end{figure}

\subsubsection{Redes neuronales recurrentes}
Hasta 2017, el estado del arte en el ámbito del procesamiento de lenguaje natural por medio de modelos de \gls{dl} eran las \emph{redes neuronales recurrentes} o \gls{rnn} \citep{schmidtRecurrentNeuralNetworks2019}. Estas se basaban en la idea de que la salida de una neurona se podía retroalimentar a la entrada, de forma que la salida de la neurona en el instante $t$ se podía usar como entrada en el instante $t+1$, lo cual permitía inputs de secuencias donde el elemento temporal es importante, como las temperaturas de un lugar durante un año o una oración lingüística. Esta arquitectura se ilustra en la Figura \ref{fig:red_neuronal_recurrente}. Sin embargo, las RNN presentaban un problema conocido como \emph{vanishing gradient} \citep{pascanuDifficultyTrainingRecurrent2013}, que hacía que el modelo no pudiera retener contextos de secuencias largas, ya que su atención se desvanece en los \emph{tokens} más alejados en el tiempo. Este problema se solucionó en parte con las redes neuronales de memoria a corto y largo plazo \gls{lstm} \citep{HochreiterVanishingGradient1998}, que permitían que la información fluyera a través de la red sin perderse. Sin embargo, las \gls{lstm} no podían trabajar con grandes cantidades de \emph{tokens} de contexto, lo que limitaba su aplicación a tareas de procesamiento de lenguaje natural.

\begin{figure}[H]
    \caption[Arquitectura de una red neuronal recurrente]{Arquitectura de una red neuronal recurrente.}
    \centering
    \includegraphics[width=0.9\textwidth]{./figuras/red_neuronal_recurrente.png}
    \source{\cite{calvoRedNeuronalRecurrente2018}}
    \label{fig:red_neuronal_recurrente}
\end{figure}


\subsubsection{La arquitectura \emph{transformer}}
En junio de 2017, \citeauthor{vaswaniAttentionAllYou2017} publicaron su artículo \emph{Attention Is All You Need} \citep{vaswaniAttentionAllYou2017} en el que proponían una nueva arquitectura de \gls{rna}, denominada \emph{transformer}, que no se basaba en las RNN y que permitía trabajar con grandes cantidades de tokens de contexto. Esta arquitectura, que marcaría un antes y un después en los modelos de \gls{dl} se basaba en el concepto de \emph{atención}, que permite que el modelo pueda predecir nuevos tokens teniendo en cuenta todo el contexto de la cadena previa. En esta arquitectura, no importa la lejanía en la posición de los tokens de la secuencia de entrada, ya que el modelo asignar pesos a cada token en función de su relevancia respecto al resto de tokens para la predicción del siguiente. A diferencia de una \gls{rnn}, un transformer puede procesar todos los tokens de una secuencia de entrada en paralelo, lo que lo hace mucho más eficiente computacionalmente tanto en el entrenamiento como en la inferencia. La Figura \ref{fig:transformer_attention} muestra una matriz de atención de un transformer entrenado con textos en inglés.

% En la figura \ref{fig:transformer_architecture} se puede ver el esquema de la arquitectura \emph{Transformer} tal como se presentó originalmente en \emph{Attention is All You Need} \cite{vaswaniAttentionAllYou2017}.

% \begin{figure}[H]
%     \caption[Aquitectura de Transformer]{Aquitectura de Transformer. Esta imagen se ha convertido en icono de la revolución de la IA generativa.}
%     \centering
%     \includegraphics[width=0.4\textwidth]{./figuras/Transformer_architecture.png}
%     \source{\cite{vaswaniAttentionAllYou2017}}
%     \label{fig:transformer_architecture}
% \end{figure}

\begin{figure}[H]
    \caption[Matriz de \emph{atención} de un \emph{Transformer} entrenado con textos en inglés]{Matriz de \emph{atención} de un \emph{Transformer} entrenado con textos en inglés. En ella queda representada la \emph{atención}, o nivel de relación semántica de cada \emph{token} dentro de la secuencia de entrada <<\emph{He later went to report Malaysia for one year}>> con respecto a los demás.}
    \centering
    \includegraphics[width=0.4\textwidth]{./figuras/Transformer_attention_matrix.png}
    \source{\cite{duAddressingSyntaxBasedSemantic2022}}
    \label{fig:transformer_attention}
\end{figure}

Esta capacidad de generar eficientemente contenido original prestando atención a todos los tokens de un gran contexto, propia de los modelos entrenados con la arquitectura transformer, es lo que ha dado lugar a la explosiva evolución de la \gls{iag}. La \gls{iag} abarca tanto a la generación de texto como de música, imagen, vídeo y cualquier forma de información, por compleja que esta sea, susceptible de ser reducida a modelos estadísticos.