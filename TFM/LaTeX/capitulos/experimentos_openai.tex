% Aquí se expondrá todo lo investigado con OpenAI y los modelos de lenguaje.

\section{Componiendo música con la asistencia de GPT-4}
El estudio llevado acabo en esta investigación se expone a continuación siguiendo el orden cronológico del trabajo exploratorio llevado a cabo con las diversas herramientas que provee OpenAI. Se comienza con la exploración de ChatGPT, el chatbot de OpenAI, para luego pasar al \textit{Playground} y finalmente a la API, que nos posibilita el uso de los modelos en entornos de programación ad hoc que nos permiten la automatización de las tareas de peticiones a la API y procesamiento de los resultados en la generación de música.

\subsection{ChatGPT, como banco de pruebas}
La forma en la que ha dado a conocer al gran público las potencialidades de los modelos de lenguaje ha sido a través de chatbots. Si bien en la actualidad existen muchos chatbots que utilizan modelos de lenguaje, el primero en hacerlo de forma pública fue ChatGPT, de OpenAI, un chatbot que utiliza el modelo GPT-3 y GPT-4, con el cual se pueden mantener conversaciones en lenguaje natural. Si bien en la actualidad cuenta con muchas características que lo convierten en una herramienta dirigida a la productividad en múltiples ámbitos, en un principio, y esencialmente, fue concebido como un chatbot con el que mantener conversaciones.

Como ya se ha señalado, una de sus grandes potencialidades es la de poder ser usado como <<copiloto>> en tareas de programación. En este sentido, se ha usado ChatGPT como banco de pruebas para la una primera exploración de cómo pueden ser usados estos modelos en la creación de música a través del lenguaje del código.

\subsubsection{GPT-4, modelo utilizado en todos los experimentos}

En la sección \ref{sec:llm_asistentes_creacion_codigo_programacion} se ha expuesto el estado del arte de los modelos de lenguaje aplicados a la creación de código, habilidad esta que constituye una de las más notorias. Al mismo tiempo, se señaló que el modelo más avanzado en la actualidad es GPT-4, de OpenAI\index{OpenAI}. No obstante, se realizaron pruebas con otros chatbots, como Bart, de Google, o el mismo Github Copilot, modelo este último que ha recibido un \textit{fine tunning} específico para la creación de código con todos los repositorios de Github. Sin embargo, en GPT-4 se ha encontrado un buen equilibrio entre precisión en respuestas y posibilidades de uso, especialmente gracias a la API y su extensa documentación. La Figura \ref{fig:GPT4_correction_comparation} muestra un ejemplo, realizado al azar, que pone de manifiesto la diferencia entre la corrección de código en SuperCollider de GPT-4 y Bart\index{Bart}, en el que se puede apreciar cómo el código generado por Bart no se puede ejecutar por numeroros errores sintácticos, mientras que el de GPT-4 es correcto. En cuanto al modelo exacto de GPT-4, si bien este ha ido cambiando a lo largo del tiempo en el que se han realizado los experimentos, la versión más utilizada, tanto por coste como por optimización, ha sido $gpt-4-1106-preview$, más conocida como <<GPT-4 turbo>>.


\begin{figure}[h]
    \caption[Respuesta de \textit{Bart} y \textit{ChatGPT} a un mismo prompt]{(a) Respuesta de \textit{Bart} y (b) \textit{ChatGPT} a un mismo prompt: <<Haz un código sencillo en SuperCollider, listo para ejecutarse, que ejemplifique una frecuencia modulada>>. El código de \textit{Bart} cuenta con varios errores graves de sintaxis, mientras el de \textit{ChatGPT} es correcto.}
    \centering
    \begin{subfigure}{.48\textwidth}
    \centering
    \begin{lstlisting}[style=SuperCollider-IDE, basicstyle=\footnotesize\ttfamily, numbers=none]
// Declaramos las variables
float fm = 440;
float fms = 100;
float amp = 0.5; 

// Generamos la señal FM
SinOsc s = SinOsc(fm);
s.freq = 
s.freq * (1 + fms * SinOsc(fms).cos);

// Asignamos la señal a un bus
Out(s);
    \end{lstlisting}
    \caption{Bart (con Palm 2)}
    \end{subfigure}\hfill
    \begin{subfigure}{.48\textwidth}
    \centering
    \begin{lstlisting}[style=SuperCollider-IDE, basicstyle=\footnotesize\ttfamily, numbers=none]
(
SynthDef(\fmSimple, {
    |out = 0, freq = 440, 
    modIndex = 10,
    modFreqRatio = 1|
    var modulator = 
    SinOsc.ar(freq * modFreqRatio) 
    * modIndex;
    var carrier = 
    SinOsc.ar(freq + modulator);
    Out.ar(out, carrier ! 2);
}).add;
)

// Tocar el SynthDef
(
Synth(\fmSimple, [
    \freq, 440,   
    \modIndex, 20, 
    \modFreqRatio, 0.5
]);
)
    \end{lstlisting}
    \caption{ChatGPT (con GPT-4)}
    \end{subfigure}

    \source{Elaboración propia.}
    \label{fig:GPT4_correction_comparation}
\end{figure}


Un segundo LLM disponible en ChatGPT, GPT-3.5, si bien es capaz de generar código mejor que el de Bart, no lo hace con la tasa de corrección de GPT-4. En los experimentos realizados esto se pudo comprobar constantemente: una gran parte del código generado en SuperCollider por GPT-3.5 contenía demasiados errores de sintaxis y alucionaciones, lo cual lo convertía en ineficiente para la creación de código musical al lado de GPT-4.


\subsection{Tareas en las que se ha probado ChatGPT}
Por la propia interfaz de chatbot, ChatGPT solo permite una interacción tipo <<diálogo>>. Esto es, el usuario introduce un texto, y el modelo responde a este texto. Esto determina fuertemente el tipo de tareas que con él podemos realizar a la hora de crear código musical. En primer lugar, no es práctico para la creación de código musical en tiempo real, ya que es el usuario quien debe cortar y pegar constantemente téxto desde el IDE al chat y viceversa, lo cual entorpecería el flujo de trabajo. Sin embargo, se ha visto muy útil para las tareas que enumeramos a continuación:

\begin{itemize}
    \item Crear esbozos generales de una obra.
    \item Crear snippets sencillos de código.
    \item Poner a prueba ciertas técnicas de prompting, como \textit{Chain of Thoughts}.
\end{itemize}

\subsubsection{Creación de esbozos generales de una obra}
Una de las tareas más útiles que se ha encontrado para ChatGPT en relación a la composición musical es la de crear esbozos generales de una obra. Esto es, estructuras generales, como la forma temporal, junto con la estructura del código con las clases y funciones que eventualmente se van a usar. Se trata de una planificación previa a la composición de los detalles. En este punto es tentador pensar que chatGPT podría producir la obra completa, pero esta ilusión cae rápidamente al intentarlo, ya que lo  más probable es contrar multitud de errores en el código generado, los cuales provienen de una mala planificación general y no merece la pena depurar. Sin embargo, si el objetivo de una conversación es la crear una estructura general, el resultado puede ser muy satisfactorio.

Para llevar a cabo una tarea así, se ha encontrado conveniente hacer que nuestro prompt pida al LLM realizar una planificación previa de la obra de forma razonada, siguiendo técnicas de prompting como \textit{Chain of Thoughts} (ver sección \ref{sec:llm_tecnicas_prompting}). En este sentido, se ha encontrado que es muy útil que el prompt contenga una serie de indicaciones que guíen al modelo en la creación de la estructura general de la obra. Por otra parte, la mayor parte de las veces que se le ha pedido este tipo de trabajo, lo ha hecho correctamente solo tras iterar ciertas indicaciones en la conversación.  Por ejemplo, en el prompt de la Figura \ref{fig:ChatGPT_esbozo_estructura} se puede ver cómo se le pide al modelo que cree una estructura general de una obra. Su primera respuesta, a pesar de ser correcta desde el punto de vista del lenguaje, no responde en absoluto a la dimensión temporal pedida, que es la esencia de la estructura. Simplemente devolviéndole una pista de lo que se busca, su segunda respuesta se adecua perfectamente a lo pedido originalmente. 

Ya desde el inicio de los experimentos se ha visto que el <<feedback>> del usuario experto en la tarea pedida es fundamental para no fustrar la tarea inmediatamente. Sin embargo, llama la atención que ChatGPT no comprenda la esencia de la petición primera, en el ejemplo dado, la de crear una estructura temporal esquemática, y que baste una pista para que la respuesta sea correcta. La respuesta radica en que la iteración es la clave para obtener buenos resultados. Los LLM <<razonan>> mejor en conversaciones, donde pueden ir recibiendo feedback del usuario, que en prompts aislados.


\begin{figure}[h!]
    \caption[Conversación con ChatGPT para crear una estructura general de una obra]{Conversación con ChatGPT para crear una estructura general de una obra. El prompt (a) pide al modelo que cree una estructura general de una obra de unos 5 minutos de duración. La respuesta ignora la dimensión temporal pedida, por lo que se le da una pista de lo que se busca. La respuesta al siguiente prompt (b) se adecua perfectamente a lo pedido originalmente.}
    \centering
    \begin{subfigure}{.48\textwidth}
    \centering
    \begin{lstlisting}[style=SuperCollider-IDE, basicstyle=\footnotesize\ttfamily, numbers=none]
// Definición de SynthDef 
para la introducción
SynthDef(\granularSynth, { 
    |out = 0, freq = 440, dur = 1|
    // Código para síntesis granular
}).add;

// Patrón para la sección de introducción
Pbind(
    \instrument, \granularSynth,
    \dur, Pwhite(0.1, 0.3),
    \freq, Pexprand(400, 500)
).play;
    \end{lstlisting}
    \caption{Respuesta a Prompt 1: <<Crea la estructura en SuperCollider de una obra de música electroacústica.  La duración ha de ser de unos 5 minutos. El desarrollo temporal ha de ser la de una modificación constante de la textura sonora. Ayúdate de las técnicas de síntesis sonora que sean necesarias en cada caso. El código ha de ser modular. Los detalles se trabajarán después. Interesa un adecuado uso de las clases y placeholders de SuperCollider. Haz primero una planificación razonada.>> Nota: se han eliminado los comentarios previos al código de las respuestas.}
    \end{subfigure}\hfill
    \begin{subfigure}{.48\textwidth}
    \centering
    \begin{lstlisting}[style=SuperCollider-IDE, basicstyle=\footnotesize\ttfamily, numbers=none]
// Definición de secciones como funciones
~introduccion = {
    // Código para la introducción
    "Iniciando introducción".postln;
    // Duración de la introducción
    60.wait;
};

~desarrollo = {
    "Iniciando desarrollo".postln;
    120.wait;
};

~climax = {
    "Iniciando clímax".postln;
    60.wait;
};

~conclusion = {
    "Iniciando conclusión".postln;
    60.wait;
};

// Secuenciador temporal con Routine
Routine({
    ~introduccion.value;
    ~desarrollo.value;
    ~climax.value;
    ~conclusion.value;
}).play;
    \end{lstlisting}
    \caption{(Respueta a Prompt 2: <<Utiliza algún tipo de rutina para organizar el aspecto temporal.>>}
    \end{subfigure}
    \source{Elaboración propia.}
    \label{fig:ChatGPT_esbozo_estructura}
\end{figure}

Una vez que ChatGPT nos ha provisto de un esqueleto para una obra musical, ¿por qué no continuar la conversación hasta tener todas sus partes completas? Los intentos de crear códigos completos desde cero, paso a paso, han sido casi siempre frustrados en algún punto de la conversación. Llegados a una cierta cantidad de peticiones y respuestas, puede resultar desconcertante el olvido del LLM de ciertos aspectos importantes trabajo realizado hasta el momento y el que queda por completar. 


\begin{figure}[h!]
    \caption[Precisión de GPT-3 en función del tamaño de la ventana de contexto]{Precisión de GPT-3 en función del tamaño de la ventana de contexto. En la gráfica se aprecia cómo la precisión del modelo disminuye hacia el centro de la ventana de contexto. Ello puede se runa limitación en la creación de un proyecto, ya que la parte central de la conversación recordada por el modelo puede resultar minimizada en su importancia.}
    \centering
    \includegraphics[width=0.5\textwidth]{./figuras/Precision_LLM_gran_contexto.png}
    \source{\cite{liuLostMiddleHow2023}}
    \label{fig:Precision_LLM_gran_contexto}
\end{figure}


Esto se debe a varios factores: En primer lugar, la ventana de contexto definitivamente limita la memoria del LLM (el número de \textit{tokens} que recibe como input para la siguiente respuesta) de forma que los primeros prompts, precisamente en los que se le daban las directrices generales para un obra, quedan fuera de su alcance. Su comportamiento en ese momento es impredecible, y tiende a enfocarse en las últimas tareas como objetivo principal. Y debido a que el usuario desconoce cuál es el valor de esta ventana de contexto, puede arrastrar el proyecto en la conversación hacia un punto sin salida. En segundo lugar, independientemente de la ventana de contexto, y especialmente en las ventanas grandes, se ha visto que los LLM no procesan por igual toda la información. Pueden existir lagunas importantes en el centro de la ventana, tal como demuestran recientes estudios \citep{liuLostMiddleHow2023}, lo cual puede llevar al modelo a alucinar. La Figura \ref{fig:Precision_LLM_gran_contexto} muestra una gráfica con este fenómeno. Por tanto, en una conversación larga dentro de la creación de un proyecto de envergadura el LLM puede sufrir de pérdida absoluta de la parte inicial de la conversación (limitación de la ventana de contexto) y, por otra parte, de pérdida parcial de la parte central de la conversación (limitación de la precisión en el centro de la ventana de contexto). Si a esto le sumamos que el usuario desconoce el valor de la ventana de contexto en el caso de chatGPT, el resultado es que el usuario puede arrastrar el proyecto hacia un punto sin salida, en el que el LLM no recuerda las directrices iniciales y puede alucinar en el centro de la ventana de contexto. Véase la Figura \ref{fig:chat_ventana_lost_in_the_middle}, que ilustra gráficamente el problema combinado de conocimiento del contexto por parte de los LLM en conversaciones suficientemente largas.


\begin{figure}[h!]
    \caption[Ventana de contexto y pérdida de precisión en su interior]{Ventana de contexto y pérdida de precisión en su interior. En un proyecto de medianas y grandes dimensiones, chatGPT puede perder la memoria de las directrices iniciales (limitación de la ventana de contexto) y perder precisión o alucinar en el centro de la ventana de contexto (limitación de la precisión en el centro de la ventana de contexto). Las partes sombreadas indican una pérdida total (fuera de la ventana de contexto) o parcial (hacia la mitad de la ventana de contexto) de la atención por parte del LLM.}
    \centering
    \includegraphics[width=0.7\textwidth]{./figuras/chat_ventana_lost_in_the_middle.png}
    \source{Elaboración propia.}
    \label{fig:chat_ventana_lost_in_the_middle}
\end{figure}



\subsubsection{Creación de snippets sencillos de código}

Entendemos aquí por \textit{snippet} a una pieza de código funcional, pero que no constituye un proyecto completo. Por ejemplo, una función, un patrón, etc. Al no constituir un código completo, no se requiere del manejo de una gran información en las respuestas del LLM, por lo que se puede esperar una mayor precisión en las mismas, al tiempo que se evitan los problemas asociados con la ventana de contexto. Del mismo modo, el usuario, a priori, puede controlar mejor la conversación iterando de una forma más controlada y cómoda. 

Esta forma de trabajo, sin embargo, requiere de una buena planificación por parte del usuario, en la medida en la que es este quien dará unidad a los diferentes elementos del trabajo sonoro y musical. En este sentido, es difícil pensar en un buen rendimiento de este flujo de trabajo sin conocimientos sólidos de las herramientas de programación utilizadas. Si bien existen técnicas de uso de LLM que implican la interacción de varios modelos o conversaciones paralelas, al modo de <<agentes>>, en este trabajo no se ha explorado esta posibilidad debido a su complejidad y a la falta de técnicas y plataformas maduras que permitan su uso \footnote{En relación a técnicas de <<agentes>> aplicadas a la generación de código, véase \cite{huangAgentCoderMultiAgentbasedCode2023}. Un repositorio con extensa y actualizada bibliografía sobre el tema puede encontrarse en \cite{AGIEdgerunnersLLMAgentsPapers2024}.}

En general, en nuestros experimentos se ha encontrado que GPT-4 se ha mostrado muy correcto en sus respuestas en el sentido sintáctico, mostrando una alta capacidad de autocorrección cuando se le ha devuelto un código de error, requiriendo poca o nula intervención humana en la corrección de errores. No obstante, como ocurría en el caso de la creación de esbozos generales de una obra, se han encontrado casos de bloqueo en la conversación, en los que el LLM no ha sido capaz de avanzar en la tarea sin la intervención humana. 

El lenguaje utilizado para comunicarse con GPT-4 siempre ha sido, deliberadamente, el lenguaje natural orientado a la creación sonora y musical, para encontrar así los límites de esta herramienta. Incluso en el caso de conversaciones en las que se devuelve código correcto y funcionial, se han encontrado diversos problemas a nivel artístico, que hay que tener muy en cuenta a la hora de utilizar esta herramienta con fines musicales:

\begin{itemize}
    \item Alucinaciones recurrentes.
    \item Abuso de valores <<por defecto>>, que restan interés artístico al código.
    \item Introducción de errores de sintaxis en código previamente correcto.
    \item Ocasionales errores de sintaxis que escapan a la autocorrección.
    \item Gran distancia entre el sonido producido y el sonido esperado.
\end{itemize}

Las <<alucinaciones>> en los LLM son una de las características más conocidas de estos modelos, al tiempo que una de las más difíciles de detectar sin una supervisión humana, y, por ende, uno de los mayores handicups a la hora de delegar tareas de cualquier índole a sistemas de IA. Se entiende por <<alucinación>> a un dato erróneo que el modelo genera con total apariencia de veracidad y corrección. En el mejor de los casos estas alucinaciones se producen en el uso de clases o métodos inexistentes en el lenguaje de programación en uso. Normalmente estos defectos son fáciles de detectar por el compilador o por el intérprete del lenguaje, que suele dar cuenta de dicho problema inmediatamente. Más difíciles de detectar son alucinaciones que pueden pasar desapercibidas por no provocar errores de sintaxis en el código, pero que son eventuales problemas de seguridad. Por ejemplo, en el caso de SuperCollider, un código como el de la Figura \ref{fig:ChatGPT_alucinacion} es correcto desde el punto de vista sintáctico, pero no lo es desde el punto de vista de la semántica del lenguaje, ya que el constructor de la clase <<SinOsc>> no cuenta con el el parámetro $freq$. En este caso, el compilador de SuperCollider no da cuenta de este error, ya que no es un error de sintaxis, sino de semántica. Este tipo de errores son muy difíciles de detectar, ya que requieren de un conocimiento profundo del lenguaje de programación en uso. En este caso, el error se ha detectado por la experiencia del usuario en el uso de SuperCollider, pero en otros casos puede pasar desapercibido, lo cual puede ser considerado un problema de seguridad. Los lenguajes de programación sonora no cuentan con limitaciones de volumen o frecuencia, por lo que un error de este tipo puede provocar daños en el equipo de sonido o en el oído del usuario.

Otro defecto observado en relación a los parámetros de los métodos y clases del código es la propensión a utilizar valores por defecto. Este problema, lejos de comprometer la seguridad, resta interés al código generado desde el punto de vista artístico. Los valores por defecto son aquellos que un elemento del código utiliza cuando el usuario no provee de un valor explícito. Por ejemplo, en el caso de SuperCollider, el método $SinOsc.ar$ cuenta con un parámetro $mul$ que, si no se provee de un valor explícito, toma el valor por defecto de $1$; o $freq$, con el valor por defecto de $440$ Hz. Por ejemplo, en la Figura \ref{fig:ChatGPT_valores_por_defecto} se puede ver cómo el modelo utiliza valores por defecto en el código generado, lo cual no es deseable desde el punto de vista artístico. En este caso, el usuario ha tenido que intervenir para corregir el código generado por el modelo. Junto a la <<alucinación>>, se trata de uno de los problemas que pueden pasar desapercibidos sin un entendimiento profundo del lenguaje por parte del usuario.

Aunque infrecuente, no menos desconcertante es la introducción impredecible de errores de cualquier tipo en código previamente correcto. Si los errores son de sintaxis, el compilador o el intérprete del lenguaje darán cuenta de ellos, pero si se trata de errores de semántica, como en el caso de la alucinación, pueden pasar desapercibidos. De nuevo, el nivel de conocimientos de usuario es fundamental para detectar este tipo de errores.

Una característica interesante de los LLM, cuando las conversaciones no son demasiado extensas, es la de autocorregirse. Esto es, si el usuario devuelve un el mensaje de error dado por el sistema, el modelo es capaz de corregirlo en la siguiente respuesta. Sin embargo, factores que escapan al control, como la introducción de alucinaciones o a simple carencia de información de entrenamiento en el problema actual del código, pueden hacer que el modelo no sea capaz de autocorregirse, pudiendo llevar a un bloqueo en la conversación, especialmente si esta se extiende demasiado en relación a su ventana de contexto, donde, como vimos, se producen lagunas de información. En relación a esto, se han encontrado errores de sintaxis muy simples, como ausencia de paréntesis, que el modelo no ha sido capaz de autocorregir, como se puede ver en la Figura \ref{fig:ChatGPT_autocorreccion}.

El poder de los LLM es del procesar el lenguaje natural y traducir descripciones en código funcional. Si el código requerido es el de una función en \textit{Python} de ha de <<devolver el sumatorio de las entradas>>, el nivel de comprensión requerido al LLM en dicha transcripción es trivial, en tanto en cuanto la propia descripción es prácticamente un pseudocódigo de dicha función. El desnivel semántico no es comparable cuando lo que pedimos al LLM es <<crear un sonido rugoso que evolucione temporalmente de la granulación a un sonido armónico con leves y agradables fluctuaciones de sus componentes parciales>>. En este caso, el LLM ha de comprender la descripción, traducirla a código y, además, generar un sonido que se adecue a la descripción.  En este caso, el desnivel semántico es mucho mayor ya que la descripción responde a criterios estéticos y psicoacústicos, por lo que el resultado puede ser muy distinto al esperado. En la Figura \ref{fig:ChatGPT_desnivel_semantico} se puede ver un ejemplo de este fenómeno. Este problema puede ser mitigado en parte con el uso de técnicas de prompting y una continua iteración en la conversación, pero en muchos casos el resultado final el resultado aceptado por el usuario puede estar lejos el esperado, lo cual constituye un problema a la hora de usar esta herramienta para la creación musical.

A nivel artístico, la utilización de esta herramienta con conocimiento de la misma, así como del campo  en cuestión, en este caso, la creación sonora y musical. La creación iterada de snippets de código sonoro puede producir fragmentos de código eventualmente de interés, en muchos casos como serendipia serendipia, si bien, al men os en el momento de la realización de este trabajo, esta valoración entra ya en la percepción estética y subjetiva del creador humano.


\subsubsection{Poner a prueba ciertas técnicas de prompting, como \textit{Chain of Thoughts}}

A partir de aquí, por realizar...

\subsubsection{Resumen de las características encontradas de la interacción con ChatGPT en la creación musical-sonora}

Se resumen las cuestiones insteresantes, así como las limitaciones, tanto al crear grandes como pequeños proyectos.

\subsection{Trabajando con el Playground de OpenAI}

\subsection{Ampliando el \textit{Knowledge}: RAG con GPTs y Assistants}

\subsection{Programando con la API de OpenAI}
Aquí se explica con detalle los scripts que se han programado, difultades, limitaciones y puntos de interés.




% ----Limitaciones encontradas:

% 1. llegar a puntos sin salida si se delega en el chat toda la creación del código.

% 2. Dificultad en manejar proyectos de tamaño medio o grande (100 líneas...) (esto es independiente de la ventana de contexto)

% 3. Limitación de la ventana de contexto y, por tanto, olvido de las directrices de los primeros prompts. Esto dificulta la escritura de código extenso, incluso teniendo en cuenta su capacidad de crear estructuras generales de piezas.

% 4. Alucinaciones recurrentes (veremos que este problema ocurre siempre). Esto ocurre mucho más en Bart.

% 5. Es una caja negra: No podemos controlar los hiperparámetros básicos, ni siquiera saber sus valores. Esto es un problema de cara a una investigación más cuantitativa.

% 6. No se puede esperar códigos complejos.


% ----Ejemplos sonoros conseguidos a través de ChatGPT.